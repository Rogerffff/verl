{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddc582b",
   "metadata": {},
   "source": [
    "# VeRL Ray API 教程\n",
    "\n",
    "本教程将帮助你理解 verl 中如何使用 Ray 进行分布式计算。\n",
    "\n",
    "**前置知识**：\n",
    "- Python 基础\n",
    "- PyTorch 基础\n",
    "\n",
    "**学习目标**：\n",
    "- 理解 Ray 的基本概念（远程函数、Actor）\n",
    "- 掌握 RayResourcePool 和 RayWorkerGroup 的使用\n",
    "- 了解数据分发（Dispatch）和收集（Collection）机制\n",
    "- 学习 Megatron 并行的集成方式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fe3b94",
   "metadata": {},
   "source": [
    "## 第一章：Ray 基础\n",
    "\n",
    "Ray 是一个用于构建分布式应用的开源框架。它的核心概念包括：\n",
    "\n",
    "1. **远程函数 (Remote Functions)**：可以在集群中任意节点上执行的函数\n",
    "2. **Actor**：有状态的远程对象，可以保持内部状态并响应方法调用\n",
    "3. **Object Store**：分布式内存存储，用于在节点间共享数据\n",
    "\n",
    "在 verl 中，我们主要使用 Ray 的 Actor 模式来管理 Worker（如 Actor Worker、Critic Worker 等）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1347d381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e75b9d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import ray\n",
    "import torch\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e90ae00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 启动本地 Ray 集群\n",
    "# Head 节点和 Worker 节点都在本机上\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a127e4e4",
   "metadata": {},
   "source": [
    "### 1.1 Ray Actor 示例\n",
    "\n",
    "下面实现一个简单的累加器类。\n",
    "\n",
    "**关键点**：\n",
    "- `@ray.remote` 装饰器将普通类转换为 Ray Actor\n",
    "- Actor 是一个独立的进程，可以保持状态\n",
    "- 调用 Actor 方法使用 `.remote()` 后缀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "20e7b9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class Accumulator:\n",
    "    def __init__(self):\n",
    "        self.value = 0\n",
    "\n",
    "    def add(self, x):\n",
    "        self.value += x\n",
    "\n",
    "    def get_value(self):\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b80098c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化一个累加器\n",
    "# Accumulator 可以看作是一个进程，充当 RPC 服务\n",
    "accumulator = Accumulator.remote()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14b1009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 查看当前值\n",
    "# 注意：.remote() 调用会立即返回，不会等待远程执行完成\n",
    "# 返回的是一个 ObjectRef（对象引用）\n",
    "value_ref = accumulator.get_value.remote()\n",
    "\n",
    "# 使用 ray.get() 获取实际值（这会阻塞直到远程执行完成）\n",
    "value = ray.get(value_ref)\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a84b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 执行累加操作，然后查看结果\n",
    "# 同样，这里的 add 也会立即返回\n",
    "accumulator.add.remote(10)\n",
    "\n",
    "# 获取新值\n",
    "new_value = ray.get(accumulator.get_value.remote())\n",
    "print(new_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c332fe0",
   "metadata": {},
   "source": [
    "## 第二章：资源池（Resource Pool）和 RayWorkerGroup\n",
    "\n",
    "在上一个例子中，我们使用了简单的单进程 Worker。\n",
    "\n",
    "在实际的 RL 训练中，我们需要：\n",
    "1. **多 GPU 并行**：每个 Worker 绑定一个 GPU\n",
    "2. **Worker 分组**：将多个 Worker 组织成一个 WorkerGroup 进行协同工作\n",
    "3. **资源管理**：灵活分配和复用 GPU 资源\n",
    "\n",
    "verl 提供了 `RayResourcePool` 和 `RayWorkerGroup` 来满足这些需求。\n",
    "\n",
    "### 核心概念\n",
    "\n",
    "- **RayResourcePool**：GPU 资源池，定义可用的 GPU 资源\n",
    "- **RayWorkerGroup**：Worker 组，将多个 Worker 映射到资源池上\n",
    "- **Worker**：verl 中的基础 Worker 类，继承后实现具体功能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "04229afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from verl.single_controller.base import Worker\n",
    "from verl.single_controller.ray.base import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup, merge_resource_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0d0dbd58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource_pool = RayResourcePool([4], use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f6838a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class GPUAccumulator(Worker):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # The initial value of each rank is the same as the rank\n",
    "        # 每个 rank 的初始值与其 rank 相同\n",
    "        self.value = torch.zeros(size=(1,), device=\"cuda\") + self.rank\n",
    "\n",
    "    def add(self, x):\n",
    "        self.value += x\n",
    "        print(f\"rank {self.rank}, value: {self.value}\")\n",
    "        return self.value.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aad8fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([1.]), tensor([2.]), tensor([3.]), tensor([4.])]\n"
     ]
    }
   ],
   "source": [
    "# Each worker's initial value is its rank, and then each rank's value is incremented by 1, so the values obtained on each rank are [1, 2, 3, 4]\n",
    "# 每个 worker 的初始值是它的 rank，然后每个 rank 的值加 1，所以每个 rank 上得到的值是 [1, 2, 3, 4]\n",
    "class_with_args = RayClassWithInitArgs(cls=GPUAccumulator)\n",
    "worker_group = RayWorkerGroup(resource_pool, class_with_args)\n",
    "print(worker_group.execute_all_sync(\"add\", x=[1, 1, 1, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6705284",
   "metadata": {},
   "source": [
    "The principle of parameter passing: The input parameter is a list of length world_size, where each element in the list is dispatched respectively to each worker in the RayWorkerGroup. \n",
    "The return parameter is also a list, corresponding to the return value of each worker.\n",
    "\n",
    "参数传递原理：输入参数是一个长度为 world_size 的列表，列表中的每个元素分别分发给 RayWorkerGroup 中的每个 worker。\n",
    "返回参数也是一个列表，对应每个 worker 的返回值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25c2412",
   "metadata": {},
   "source": [
    "### GPU Resource Sharing\n",
    "\n",
    "### GPU 资源共享"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f6d24",
   "metadata": {},
   "source": [
    "RayWorkerGroups mapped to the same resource pool share the GPU. In this example, we implement three resource pools: the first occupies 4 GPUs, the second also occupies 4 GPUs, and the last occupies all 8 GPUs. Among them, the first resource pool reuses the resource pool mentioned above.\n",
    "\n",
    "映射到同一资源池的 RayWorkerGroups 共享 GPU。在这个例子中，我们实现了三个资源池：第一个占用 4 个 GPU，第二个也占用 4 个 GPU，最后一个占用所有 8 个 GPU。其中，第一个资源池复用了上面提到的资源池。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f9c06f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a new resource pool and then merge the newly created resource pool with the previous one.\n",
    "# 创建一个新的资源池，然后将新创建的资源池与前一个合并。\n",
    "resource_pool_1 = RayResourcePool([4], use_gpu=True, name_prefix=\"a\")\n",
    "resource_pool_merge = merge_resource_pool(resource_pool, resource_pool_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c2e305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Establish a RayWorkerGroup on the newly created resource pool.\n",
    "# 在新创建的资源池上建立一个 RayWorkerGroup。\n",
    "worker_group_1 = RayWorkerGroup(resource_pool_1, class_with_args)\n",
    "worker_group_merge = RayWorkerGroup(resource_pool_merge, class_with_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b13f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([2.]), tensor([3.]), tensor([4.]), tensor([5.])]\n"
     ]
    }
   ],
   "source": [
    "# Run 'add' on the second set of 4 GPUs; the result should be [2, 3, 4, 5].\n",
    "# 在第二组 4 个 GPU 上运行 'add'；结果应该是 [2, 3, 4, 5]。\n",
    "output_1 = worker_group_1.execute_all_sync(\"add\", x=[2, 2, 2, 2])\n",
    "print(output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d856d030",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([3.]), tensor([4.]), tensor([5.]), tensor([6.]), tensor([7.]), tensor([8.]), tensor([9.]), tensor([10.])]\n"
     ]
    }
   ],
   "source": [
    "# Run 'add' on the merged set of 8 GPUs; the result should be [3, 4, 5, 6, 7, 8, 9, 10].\n",
    "# 在合并后的 8 个 GPU 上运行 'add'；结果应该是 [3, 4, 5, 6, 7, 8, 9, 10]。\n",
    "output_merge = worker_group_merge.execute_all_sync(\"add\", x=[3, 3, 3, 3, 3, 3, 3, 3])\n",
    "print(output_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "33a4628c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 8\n"
     ]
    }
   ],
   "source": [
    "print(worker_group.world_size, worker_group_1.world_size, worker_group_merge.world_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df19d13",
   "metadata": {},
   "source": [
    "## Chapter 3: Data Dispatch, Execution and Collection\n",
    "\n",
    "## 第三章：数据分发、执行和收集"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb22d9d",
   "metadata": {},
   "source": [
    "In the above example, we used the `execute_all_sync` function in the RayWorkerGroup to dispatch data from the driver to each worker. This is very inconvenient for coding. \n",
    "In this chapter, we use the form of function decorators to allow RayWorkerGroup to directly call functions written in the Worker, and to greatly simplify parameter passing.\n",
    "\n",
    "在上面的例子中，我们使用 RayWorkerGroup 中的 `execute_all_sync` 函数将数据从 driver 分发到每个 worker。这对于编码来说非常不方便。\n",
    "在本章中，我们使用函数装饰器的形式，允许 RayWorkerGroup 直接调用在 Worker 中编写的函数，并大大简化参数传递。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "35237432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from verl.single_controller.base.decorator import Dispatch, Execute, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b8ba3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class GPUAccumulatorDecorator(Worker):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # The initial value of each rank is the same as the rank\n",
    "        # 每个 rank 的初始值与其 rank 相同\n",
    "        self.value = torch.zeros(size=(1,), device=\"cuda\") + self.rank\n",
    "\n",
    "    # map from a single input to all the worker\n",
    "    # 将单个输入映射到所有 worker\n",
    "    @register(Dispatch.ONE_TO_ALL)\n",
    "    def add(self, x):\n",
    "        print(x)\n",
    "        self.value = self.value + x\n",
    "        print(f\"rank {self.rank}, value: {self.value}\")\n",
    "        return self.value.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "eddaa043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_with_args = RayClassWithInitArgs(cls=GPUAccumulatorDecorator)\n",
    "gpu_accumulator_decorator = RayWorkerGroup(resource_pool_merge, class_with_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10087c91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([10.]), tensor([11.]), tensor([12.]), tensor([13.]), tensor([14.]), tensor([15.]), tensor([16.]), tensor([17.])]\n"
     ]
    }
   ],
   "source": [
    "# As we can see, 10 is automatically dispatched to each Worker in this RayWorkerGroup.\n",
    "# 可以看到，10 被自动分发到这个 RayWorkerGroup 中的每个 Worker。\n",
    "print(gpu_accumulator_decorator.add(x=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540ee6ad",
   "metadata": {},
   "source": [
    "### Custom Dispatch, Collection\n",
    "Users can customize `dispatch` and `collection` function. You only need to write the `dispatch_fn` and `collect_fn` functions yourself. We also support executing RPC only on rank_zero, with specific examples provided below.\n",
    "\n",
    "### 自定义分发和收集\n",
    "用户可以自定义 `dispatch` 和 `collection` 函数。你只需要自己编写 `dispatch_fn` 和 `collect_fn` 函数即可。我们还支持仅在 rank_zero 上执行 RPC，具体示例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8e041270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from verl.single_controller.base.decorator import Dispatch, collect_all_to_all, register"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b5be31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def two_to_all_dispatch_fn(worker_group, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Assume the input is a list of 2. Duplicate the input interleaved and pass to each worker.\n",
    "    假设输入是一个长度为 2 的列表。将输入交替复制并传递给每个 worker。\n",
    "    \"\"\"\n",
    "    for arg in args:\n",
    "        assert len(arg) == 2\n",
    "        for i in range(worker_group.world_size - 2):\n",
    "            arg.append(arg[i % 2])\n",
    "    for k, v in kwargs.items():\n",
    "        assert len(v) == 2\n",
    "        for i in range(worker_group.world_size - 2):\n",
    "            v.append(v[i % 2])\n",
    "    return args, kwargs\n",
    "\n",
    "\n",
    "@ray.remote\n",
    "class TestActor(Worker):\n",
    "    # TODO: pass *args and **kwargs is bug prone and not very convincing\n",
    "    # TODO: 传递 *args 和 **kwargs 容易出错且不太可靠\n",
    "    def __init__(self, x) -> None:\n",
    "        super().__init__()\n",
    "        self._x = x\n",
    "\n",
    "    def foo(self, y):\n",
    "        return self._x + y\n",
    "\n",
    "    @register(dispatch_mode=Dispatch.ALL_TO_ALL, execute_mode=Execute.RANK_ZERO)\n",
    "    def foo_rank_zero(self, x, y):\n",
    "        return self._x + y + x\n",
    "\n",
    "    @register(dispatch_mode={\"dispatch_fn\": two_to_all_dispatch_fn, \"collect_fn\": collect_all_to_all})\n",
    "    def foo_custom(self, x, y):\n",
    "        return self._x + y + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "83ec6609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_with_args = RayClassWithInitArgs(cls=TestActor, x=2)\n",
    "worker_group = RayWorkerGroup(resource_pool, class_with_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "62c58d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_ref = worker_group.foo_custom(x=[1, 2], y=[5, 6])\n",
    "assert output_ref == [8, 10, 8, 10]\n",
    "\n",
    "output_ref = worker_group.foo_rank_zero(x=1, y=2)\n",
    "assert output_ref == 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "14689353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(gpu_accumulator_decorator.world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c80bbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shutdown ray cluster\n",
    "# 关闭 Ray 集群\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c8151c",
   "metadata": {},
   "source": [
    "## Chapter 4: NVMegatronRayWorkerGroup\n",
    "\n",
    "## 第四章：NVMegatronRayWorkerGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5680e9",
   "metadata": {},
   "source": [
    "Due to the Ray issue, we can only support max_colocate_count=1 in RayResourcePool for now. \n",
    "This means that each GPU can only have one process.\n",
    "We can support max_colocate > 1 when applying this pull request: https://github.com/ray-project/ray/pull/44385\n",
    "\n",
    "由于 Ray 的问题，目前我们在 RayResourcePool 中只能支持 max_colocate_count=1。\n",
    "这意味着每个 GPU 只能有一个进程。\n",
    "当应用这个 pull request 后，我们可以支持 max_colocate > 1：https://github.com/ray-project/ray/pull/44385"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92724419",
   "metadata": {},
   "source": [
    "Therefore, we need to restart the ray and initialize a new resource_pool to demonstrate the **NVMegatronRayWorkerGroup**\n",
    "\n",
    "因此，我们需要重启 Ray 并初始化一个新的 resource_pool 来演示 **NVMegatronRayWorkerGroup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b038538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build a local ray cluster. The head node and worker node are on this machine\n",
    "# 构建一个本地 Ray 集群。Head 节点和 Worker 节点都在本机上\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfd8798",
   "metadata": {},
   "source": [
    "Finally, we implement a `NVMegatronRayWorkerGroup`, within which we create a Megatron and then run a tensor parallel (tp) split Llama mlp layer. Here, we use a complex dispatch mode, `Megatron_COMPUTE`. This dispatch mode assumes that user passes the data partitioned by DP dimension. The data is dispatched to all tp/pp ranks within the same dp group, and ultimately only collects output data from tp=0 and the last pp. In this way, for users that only write code on the driver, the Megatron behind the RPC becomes transparent.\n",
    "\n",
    "最后，我们实现一个 `NVMegatronRayWorkerGroup`，在其中创建一个 Megatron，然后运行一个张量并行（tp）切分的 Llama MLP 层。这里，我们使用一个复杂的分发模式 `Megatron_COMPUTE`。这个分发模式假设用户传递的数据是按 DP 维度分区的。数据被分发到同一 dp 组内的所有 tp/pp rank，最终只从 tp=0 和最后一个 pp 收集输出数据。这样，对于只在 driver 上编写代码的用户来说，RPC 后面的 Megatron 变得透明了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5a032154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/tiger/Megatron-LM\n",
      "/opt/tiger/Megatron-LM/megatron/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "current_pythonpath = os.environ.get(\"PYTHONPATH\", \"\")\n",
    "\n",
    "new_path = \"/opt/tiger/Megatron-LM\"\n",
    "\n",
    "new_pythonpath = f\"{new_path}:{current_pythonpath}\" if current_pythonpath else new_path\n",
    "\n",
    "os.environ[\"PYTHONPATH\"] = new_pythonpath\n",
    "\n",
    "print(new_path)\n",
    "sys.path.append(new_path)\n",
    "\n",
    "import megatron\n",
    "\n",
    "print(megatron.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8c84cd5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from megatron.core import parallel_state as mpu\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from verl.single_controller.base.decorator import Dispatch, Execute, register\n",
    "from verl.single_controller.base.megatron.worker import MegatronWorker\n",
    "from verl.single_controller.ray.base import RayClassWithInitArgs, RayResourcePool, RayWorkerGroup\n",
    "from verl.single_controller.ray.megatron import NVMegatronRayWorkerGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1b1debcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource_pool = RayResourcePool([4], use_gpu=True, max_colocate_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "bccbe081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "class MLPLayerWorker(MegatronWorker):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "        torch.distributed.init_process_group(backend=\"nccl\")\n",
    "        torch.cuda.set_device(rank)\n",
    "\n",
    "        mpu.initialize_model_parallel(\n",
    "            tensor_model_parallel_size=4,\n",
    "            pipeline_model_parallel_size=1,\n",
    "            virtual_pipeline_model_parallel_size=None,\n",
    "            pipeline_model_parallel_split_rank=None,\n",
    "            use_sharp=False,\n",
    "            context_parallel_size=1,\n",
    "            expert_model_parallel_size=1,\n",
    "            nccl_communicator_config_path=None,\n",
    "        )\n",
    "        from megatron.core import tensor_parallel\n",
    "\n",
    "        tensor_parallel.model_parallel_cuda_manual_seed(10)\n",
    "\n",
    "    @register(Dispatch.ONE_TO_ALL)\n",
    "    def init_model(self, config):\n",
    "        from omegaconf import OmegaConf\n",
    "\n",
    "        from verl.models.llama.megatron.layers import ParallelLlamaMLP\n",
    "        from verl.utils.megatron_utils import init_model_parallel_config\n",
    "\n",
    "        megatron_config = OmegaConf.create(\n",
    "            {\n",
    "                \"sequence_parallel\": False,\n",
    "                \"param_dtype\": \"fp32\",\n",
    "                \"tensor_model_parallel_size\": mpu.get_tensor_model_parallel_world_size(),\n",
    "                \"pipeline_model_parallel_rank\": mpu.get_pipeline_model_parallel_rank(),\n",
    "                \"pipeline_model_parallel_size\": mpu.get_pipeline_model_parallel_world_size(),\n",
    "                \"virtual_pipeline_model_parallel_rank\": mpu.get_virtual_pipeline_model_parallel_rank(),\n",
    "                \"virtual_pipeline_model_parallel_size\": mpu.get_virtual_pipeline_model_parallel_world_size(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        megatron_config = init_model_parallel_config(megatron_config)\n",
    "        self.parallel_layer = ParallelLlamaMLP(config=config, megatron_config=megatron_config)\n",
    "\n",
    "    @register(Dispatch.ONE_TO_ALL)\n",
    "    def get_weights(self):\n",
    "        output = {}\n",
    "        for key, val in self.parallel_layer.named_parameters():\n",
    "            output[key] = val\n",
    "        return output\n",
    "\n",
    "    @register(Dispatch.MEGATRON_COMPUTE)\n",
    "    def run_layer(self, x):\n",
    "        x = x.to(\"cuda\")\n",
    "        y = self.parallel_layer(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a655271d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "layer_cls = RayClassWithInitArgs(cls=MLPLayerWorker)\n",
    "layer_worker_group = NVMegatronRayWorkerGroup(\n",
    "    resource_pool=resource_pool,\n",
    "    ray_cls_with_init=layer_cls,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f105ebee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 4 1 1\n"
     ]
    }
   ],
   "source": [
    "print(layer_worker_group.world_size, layer_worker_group.tp_size, layer_worker_group.pp_size, layer_worker_group.dp_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "38655091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ffn_hidden_size = 11008\n",
    "batch_size = 16\n",
    "seq_len = 2048\n",
    "hidden_size = 4096\n",
    "\n",
    "config = OmegaConf.create(\n",
    "    {\n",
    "        \"hidden_size\": hidden_size,\n",
    "        \"intermediate_size\": ffn_hidden_size,\n",
    "        \"hidden_act\": \"silu\",\n",
    "        \"pretraining_tp\": 1,\n",
    "        \"tp\": layer_worker_group.tp_size,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a026efca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = torch.rand(size=(seq_len, batch_size, hidden_size), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f5fcaf13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_worker_group.init_model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cc9b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 16, 4096])\n"
     ]
    }
   ],
   "source": [
    "output = layer_worker_group.run_layer(\n",
    "    [x]\n",
    ")  # This must be a list of size 1, ensuring that the input equals the data parallel (dp).\n",
    "# 这必须是一个大小为 1 的列表，确保输入等于数据并行度（dp）。\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49792210",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Shutdown ray cluster\n",
    "# 关闭 Ray 集群\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
