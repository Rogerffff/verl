# Phase 0 Baseline 实验运行指南

本文档详细介绍如何在 vast.ai 云 GPU 平台上运行 Phase 0 Baseline 代码评测实验。

---

## 目录

1. [环境要求](#1-环境要求)
2. [vast.ai 实例配置](#2-vastai-实例配置)
3. [连接实例](#3-连接实例)
4. [环境配置](#4-环境配置)
5. [启动 vLLM 服务](#5-启动-vllm-服务)
6. [启动 SandboxFusion](#6-启动-sandboxfusion)
7. [数据准备](#7-数据准备)
8. [运行评测](#8-运行评测)
9. [检查结果](#9-检查结果)
10. [常见问题排查](#10-常见问题排查)
11. [一键运行脚本](#11-一键运行脚本)
12. [成本估算](#12-成本估算)

---

## 1. 环境要求

### 1.1 GPU 显存需求

**模型**: Qwen/Qwen2.5-Coder-7B-Instruct (7B 参数)

| 组件 | 显存需求 | 说明 |
|------|---------|------|
| 模型权重 (BF16) | ~14 GB | 7B × 2 bytes |
| KV Cache | ~4-6 GB | max_model_len=6144 |
| 运行时开销 | ~2-3 GB | CUDA context |
| **总计** | **~20-24 GB** | 单卡运行需求 |

### 1.2 推荐 GPU 配置

| GPU 型号 | 显存 | 推荐度 | 价格参考 |
|----------|------|--------|---------|
| **RTX 4090** | 24 GB | ⭐⭐⭐ 首选 | ~$0.35-0.50/hr |
| A100 40GB | 40 GB | ⭐⭐ 充裕 | ~$0.90-1.20/hr |
| RTX 3090 | 24 GB | ⭐ 可用 | ~$0.25-0.35/hr |
| A6000 | 48 GB | ⭐⭐ 稳定 | ~$0.50-0.70/hr |

**结论**: 推荐租用 **RTX 4090 (24GB)**，性价比最高。

### 1.3 软件依赖

| 软件 | 版本 | 用途 |
|------|------|------|
| Python | 3.11+ | 运行环境 |
| CUDA | 12.4+ | GPU 计算 |
| vLLM | 0.8.5+ | 模型推理服务 |
| Docker | 20.10+ | 运行 SandboxFusion |
| sandbox-fusion | 0.3.7+ | 代码评测 SDK |

---

## 2. vast.ai 实例配置

### 2.1 注册和登录

1. 访问 [vast.ai](https://vast.ai/) 并注册账号
2. 充值账户余额（建议 $5-10 足够本实验）

### 2.2 搜索实例

访问 [Create Instance](https://cloud.vast.ai/create/) 页面，使用以下过滤条件：

```
GPU RAM: >= 24 GB
GPU Model: RTX 4090 / A100-40GB / A6000
Disk Space: >= 100 GB
CPU: >= 8 cores
RAM: >= 32 GB
CUDA Version: >= 12.4
```

### 2.3 选择 Docker 模板（重要）

**推荐使用 vast.ai 官方 vLLM 模板**，它已经预装了匹配的 vLLM、PyTorch 和 CUDA 版本。

#### 方式 A: 使用 vLLM 模板（强烈推荐）

1. 点击 "Change Template"
2. 搜索 "vLLM" 或选择官方 vLLM 模板
3. 确认镜像版本，推荐使用：
   - `vastai/vllm:v0.8.5-cuda-12.4-pytorch-2.6.0-py312`（稳定版）
   - 或 `vastai/vllm:v0.10.2-cuda-12.8-pytorch-2.8.0-py312`（新版）

**优势**：
- vLLM 已预装，无需手动安装
- CUDA、PyTorch、vLLM 版本已匹配，避免兼容性问题
- 启动更快

#### 方式 B: 使用 PyTorch 模板

如果选择 PyTorch 模板（`vastai/pytorch` 或默认的 `vastai/pytorch:@vastai-automatic-tag`），需要注意：

- 自动标签会选择最新兼容版本
- 需要手动安装 vLLM，可能遇到版本兼容问题
- 建议选择带 CUDA 12.4+ 的版本

**兼容性说明**（参考 [vLLM 安装文档](https://docs.vllm.ai/en/stable/getting_started/installation/gpu/)）：
- vLLM v0.8.x 需要 CUDA 12.4 和 PyTorch 2.6+
- vLLM 的二进制文件与特定 CUDA/PyTorch 版本绑定
- 使用 conda 安装的 PyTorch 可能导致 NCCL 冲突

### 2.4 实例配置参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Disk Space | 100 GB | 模型 + 数据 + 日志 |
| On-start Script | 留空 | 手动配置更灵活 |
| SSH | 启用 | 远程连接 |
| Jupyter | 可选 | 调试用 |

### 2.5 创建实例

点击 "Rent" 按钮，等待实例启动（通常 1-2 分钟）。

---

## 3. 连接实例

### 3.1 获取 SSH 连接信息

在 vast.ai 控制台的 "Instances" 页面，找到你的实例，点击 "Connect" 获取 SSH 命令。

### 3.2 SSH 连接

```bash
# 使用 vast.ai 提供的命令，格式类似：
ssh -p <PORT> root@<HOST> -L 8080:localhost:8080 -L 8000:localhost:8000

# 端口转发说明：
# -L 8080:localhost:8080  转发 SandboxFusion 端口（可选，用于本地调试）
# -L 8000:localhost:8000  转发 vLLM 端口（可选，用于本地调试）
```

### 3.3 验证连接

```bash
# 检查 GPU
nvidia-smi

# 检查 CUDA 版本
nvcc --version

# 检查 vLLM 是否已安装（如果使用 vLLM 模板）
python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

# 检查磁盘空间
df -h
```

---

## 4. 环境配置

### 4.1 安装系统工具

```bash
apt update && apt install -y git tmux htop docker.io
```

### 4.2 克隆代码仓库

**将你的代码上传到 GitHub 后，在远程实例上克隆**：

```bash
# 创建工作目录
mkdir -p /workspace/rlvr_coding
cd /workspace/rlvr_coding

# 克隆你的代码仓库（替换为你的仓库地址）
git clone https://github.com/<your-username>/<your-repo>.git
cd <your-repo>

# 或者如果是私有仓库，使用 SSH 或 token
git clone https://<token>@github.com/<your-username>/<your-repo>.git
```

**预期目录结构**：
```
/workspace/rlvr_coding/<your-repo>/
├── verl/
│   └── coding_model_project/
│       ├── src/
│       │   ├── phase0_eval.py
│       │   ├── data_governance.py
│       │   └── utils/
│       ├── data/
│       │   ├── manifests/
│       │   └── raw/
│       └── outputs/
```

### 4.3 安装 Python 依赖

#### 如果使用 vLLM 模板（vLLM 已预装）

```bash
# 安装 SandboxFusion SDK
pip install sandbox-fusion

# 安装其他依赖
pip install aiohttp numpy pandas datasets transformers
pip install wandb  # 可选，用于指标记录
```

#### 如果使用 PyTorch 模板（需要安装 vLLM）

```bash
# 安装 vLLM（这一步可能需要几分钟）
# 注意：需要与模板的 CUDA 版本匹配
pip install vllm

# 安装 SandboxFusion SDK
pip install sandbox-fusion

# 安装其他依赖
pip install aiohttp numpy pandas datasets transformers
pip install wandb  # 可选
```

### 4.4 验证安装

```bash
# 验证 vLLM
python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

# 验证 SandboxFusion SDK
python -c "from sandbox_fusion import run_code; print('SandboxFusion SDK OK')"

# 验证 GPU 可用
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}, Device: {torch.cuda.get_device_name(0)}')"
```

---

## 5. 启动 vLLM 服务

### 5.1 预下载模型（推荐）

预先下载模型可以避免首次启动时的长时间等待：

```bash
# 使用 huggingface-cli 下载
pip install huggingface_hub
huggingface-cli download Qwen/Qwen2.5-Coder-7B-Instruct \
    --local-dir /workspace/models/Qwen2.5-Coder-7B-Instruct

# 设置环境变量使用本地模型
export MODEL_PATH=/workspace/models/Qwen2.5-Coder-7B-Instruct
```

或者直接使用在线模型（首次启动会自动下载）：

```bash
export MODEL_PATH=Qwen/Qwen2.5-Coder-7B-Instruct
```

### 5.2 启动 vLLM 服务

使用 tmux 保持服务在后台运行：

```bash
# 创建 tmux 会话
tmux new -s vllm

# 启动 vLLM OpenAI-compatible API 服务器
python -m vllm.entrypoints.openai.api_server \
    --model ${MODEL_PATH:-Qwen/Qwen2.5-Coder-7B-Instruct} \
    --port 8000 \
    --host 0.0.0.0 \
    --tensor-parallel-size 1 \
    --gpu-memory-utilization 0.85 \
    --max-model-len 6144 \
    --dtype bfloat16 \
    --trust-remote-code \
    --enable-prefix-caching \
    --disable-log-requests

# 分离 tmux 会话: 按 Ctrl+B, 然后按 D
```

### 5.3 vLLM 启动参数说明

| 参数 | 值 | 说明 |
|------|-----|------|
| `--model` | Qwen/Qwen2.5-Coder-7B-Instruct | 模型路径或 HF ID |
| `--port` | 8000 | API 端口 |
| `--tensor-parallel-size` | 1 | 单卡运行 |
| `--gpu-memory-utilization` | 0.85 | GPU 显存使用率（85%）|
| `--max-model-len` | 6144 | 最大序列长度 (4096 prompt + 2048 generation) |
| `--dtype` | bfloat16 | 模型精度 |
| `--trust-remote-code` | - | 允许执行 Qwen 模型的远程代码 |
| `--enable-prefix-caching` | - | 启用前缀缓存（提高效率）|
| `--disable-log-requests` | - | 减少日志输出 |

### 5.4 验证 vLLM 服务

等待模型加载完成（首次可能需要 2-5 分钟），然后验证：

```bash
# 健康检查
curl http://localhost:8000/health

# 测试生成
curl http://localhost:8000/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
        "messages": [{"role": "user", "content": "Write a Python function to add two numbers"}],
        "temperature": 0.0,
        "max_tokens": 100
    }'
```

预期输出：返回包含生成代码的 JSON 响应。

---

## 6. 启动 SandboxFusion

### 6.1 使用 Docker 启动（推荐）

```bash
# 创建新的 tmux 会话
tmux new -s sandbox

# 启动 SandboxFusion Docker 容器
docker run -d \
    --name sandbox_fusion \
    --rm \
    --privileged \
    -p 8080:8080 \
    volcengine/sandbox-fusion:server-20250609

# 查看启动日志
docker logs -f sandbox_fusion

# 分离 tmux: Ctrl+B, D
```

### 6.2 Docker 参数说明

| 参数 | 说明 |
|------|------|
| `-d` | 后台运行 |
| `--name sandbox_fusion` | 容器名称 |
| `--rm` | 停止时自动删除容器 |
| `--privileged` | 特权模式（代码隔离执行需要）|
| `-p 8080:8080` | 端口映射 |

### 6.3 验证 SandboxFusion

```bash
# 健康检查
curl http://localhost:8080/health

# 测试代码执行
curl -X POST http://localhost:8080/run_code \
    -H "Content-Type: application/json" \
    -d '{
        "code": "print(1 + 1)",
        "language": "python",
        "run_timeout": 10
    }'
```

预期输出：`{"status": "Success", "run_result": {"stdout": "2\n", ...}}`

---

## 7. 数据准备

由于你已经在本地运行了 `data_governance.py`，数据已经包含在 GitHub 仓库中。克隆后验证数据完整性：

### 7.1 验证数据文件

```bash
# 进入项目目录（根据你的仓库结构调整路径）
cd /workspace/rlvr_coding/<your-repo>/verl/coding_model_project

# 检查数据文件
ls -la data/manifests/
ls -la data/raw/

# 检查数据条目数
wc -l data/manifests/*.jsonl
wc -l data/raw/*.jsonl
```

预期输出：
```
   164 data/manifests/humaneval_manifest.jsonl
   200 data/manifests/mbpp_reg_manifest.jsonl
   117 data/manifests/codecontests_valid_manifest.jsonl
```

### 7.2 如果数据缺失

如果 GitHub 仓库中没有包含数据文件（例如 .gitignore 排除了大文件），可以在服务器上重新获取：

```bash
# 安装 datasets 库
pip install datasets

# 运行数据治理脚本
python src/data_governance.py \
    --source huggingface \
    --datasets humaneval mbpp_reg codecontests_valid \
    --output_dir data/
```

---

## 8. 运行评测

### 8.1 评测前检查清单

在运行评测之前，确保：

- [ ] vLLM 服务正在运行且响应正常
- [ ] SandboxFusion 服务正在运行且响应正常
- [ ] 数据文件已就绪
- [ ] Python 环境已配置

```bash
# 快速检查脚本
echo "=== Environment Check ==="
echo "1. vLLM status:"
curl -s http://localhost:8000/health && echo " OK" || echo " FAILED"

echo "2. SandboxFusion status:"
curl -s http://localhost:8080/health && echo " OK" || echo " FAILED"

echo "3. Data files:"
ls data/manifests/*.jsonl 2>/dev/null | wc -l | xargs -I {} echo "  {} manifest files found"
ls data/raw/*.jsonl 2>/dev/null | wc -l | xargs -I {} echo "  {} raw files found"

echo "4. Python environment:"
python -c "import sandbox_fusion; print('  sandbox_fusion OK')"
```

### 8.2 运行评测脚本

```bash
# 进入项目目录
cd /workspace/rlvr_coding/<your-repo>/verl/coding_model_project

# 创建输出目录标识
OUTPUT_DIR="outputs/phase0_$(date +%Y%m%d_%H%M%S)"

# 运行评测
python src/phase0_eval.py \
    --mode simple \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --vllm_url http://localhost:8000 \
    --sandbox_url http://localhost:8080 \
    --manifest_dir data/manifests \
    --datasets humaneval mbpp_reg codecontests_valid \
    --temperature 0.0 \
    --max_tokens 2048 \
    --max_concurrent 32 \
    --batch_size 50 \
    --output_dir ${OUTPUT_DIR}
```

### 8.3 运行参数说明

| 参数 | 值 | 说明 |
|------|-----|------|
| `--mode` | simple | 连接独立的 vLLM 服务器 |
| `--model` | Qwen/Qwen2.5-Coder-7B-Instruct | 模型名称（用于 API 调用）|
| `--vllm_url` | http://localhost:8000 | vLLM API 地址 |
| `--sandbox_url` | http://localhost:8080 | SandboxFusion 地址 |
| `--manifest_dir` | data/manifests | 数据清单目录（使用去重数据）|
| `--datasets` | humaneval mbpp_reg codecontests_valid | 评测数据集列表 |
| `--temperature` | 0.0 | 贪婪解码（EVAL@1 协议）|
| `--max_tokens` | 2048 | 最大生成 token 数 |
| `--max_concurrent` | 32 | 并发请求数 |
| `--batch_size` | 50 | 每批处理题目数 |

### 8.4 预估运行时间

| 数据集 | 题目数 | 预估时间 | 说明 |
|--------|--------|---------|------|
| HumanEval | 164 | ~10-15 分钟 | 题目较简单 |
| MBPP_reg | 200 | ~15-20 分钟 | Python 基础题 |
| CodeContests_valid | 117 | ~30-60 分钟 | 竞赛题，多测试用例 |
| **总计** | **481** | **~1-2 小时** | RTX 4090 单卡 |

### 8.5 监控运行进度

评测脚本会实时输出进度信息：

```
======================================================================
   Phase 0 Baseline Evaluation (verl Standalone Rollout)
======================================================================
Mode: simple
Model: Qwen/Qwen2.5-Coder-7B-Instruct
Datasets: ['humaneval', 'mbpp_reg', 'codecontests_valid']
Output: outputs/phase0_20260203_120000

[Simple Mode] Connecting to http://localhost:8000

[Loading humaneval]
  Loaded 164 problems

============================================================
Evaluating: humaneval (164 problems)
============================================================
  Processing batch 1/4...
  Processing batch 2/4...
  ...
  Results for humaneval:
    accepted@1: 35.37%
    pass_ratio_mean: 0.4012
    avg_gen_tokens: 245.3
    throughput: 0.85 problems/sec
```

---

## 9. 检查结果

### 9.1 输出文件结构

```
outputs/phase0_20260203_120000/
├── metrics.json        # 汇总指标（快速查看）
├── summary.json        # 详细统计（含错误分布）
└── qa_logs/
    ├── humaneval_qa.jsonl           # HumanEval 问答日志
    ├── mbpp_reg_qa.jsonl            # MBPP 问答日志
    └── codecontests_valid_qa.jsonl  # CodeContests 问答日志
```

### 9.2 查看汇总指标

```bash
# 格式化显示 metrics.json
cat outputs/phase0_*/metrics.json | python -m json.tool
```

输出示例：
```json
{
  "humaneval": {
    "total_problems": 164,
    "accepted_at_1": 0.3537,
    "pass_ratio_mean": 0.4012,
    "pass_ratio_p50": 0.4000,
    "pass_ratio_p90": 0.8000,
    "avg_gen_tokens": 245.3,
    "avg_judge_time": 1.23,
    "throughput": 0.85,
    "cost_per_solved_tokens": 689.2,
    "cost_per_solved_judge_time": 3.45
  },
  "mbpp_reg": { ... },
  "codecontests_valid": { ... }
}
```

### 9.3 提取关键指标

```bash
# 使用 Python 提取关键指标
python -c "
import json
import glob

# 找到最新的输出目录
output_files = glob.glob('outputs/phase0_*/metrics.json')
if not output_files:
    print('No output files found')
    exit(1)

latest_file = sorted(output_files)[-1]
print(f'Reading: {latest_file}')
print()

with open(latest_file) as f:
    data = json.load(f)

print('=' * 60)
print('Phase 0 Baseline Results')
print('=' * 60)
print()

for dataset, metrics in data.items():
    print(f'{dataset}:')
    print(f'  accepted@1:       {metrics.get(\"accepted_at_1\", 0):.2%}')
    print(f'  pass_ratio_mean:  {metrics.get(\"pass_ratio_mean\", 0):.4f}')
    print(f'  pass_ratio_p50:   {metrics.get(\"pass_ratio_p50\", 0):.4f}')
    print(f'  pass_ratio_p90:   {metrics.get(\"pass_ratio_p90\", 0):.4f}')
    print(f'  avg_gen_tokens:   {metrics.get(\"avg_gen_tokens\", 0):.1f}')
    print(f'  throughput:       {metrics.get(\"throughput\", 0):.2f} problems/sec')
    print()
"
```

### 9.4 典型基线指标参考

以下是 Qwen2.5-Coder-7B-Instruct 的典型基线指标范围：

| 数据集 | accepted@1 | pass_ratio_mean | 说明 |
|--------|-----------|-----------------|------|
| HumanEval | 30-40% | 0.35-0.45 | 7B Instruct 模型典型值 |
| MBPP_reg | 40-50% | 0.45-0.55 | Python 基础题 |
| CodeContests | 5-10% | 0.15-0.25 | 竞赛题较难 |

如果你的结果明显低于这个范围，请检查评测流程是否正确。

### 9.5 下载结果到本地

```bash
# 在本地机器上执行（替换 <PORT> 和 <HOST>）
scp -P <PORT> -r root@<HOST>:/workspace/rlvr_coding/<your-repo>/verl/coding_model_project/outputs ./local_outputs/

# 或打包后下载
ssh -p <PORT> root@<HOST> "cd /workspace/rlvr_coding/<your-repo>/verl/coding_model_project && tar czf outputs.tar.gz outputs/"
scp -P <PORT> root@<HOST>:/workspace/rlvr_coding/<your-repo>/verl/coding_model_project/outputs.tar.gz ./
```

---

## 10. 常见问题排查

### 10.1 vLLM 问题

#### 问题: CUDA out of memory

```bash
# 症状
RuntimeError: CUDA out of memory

# 解决方案 1：降低 GPU 显存利用率
python -m vllm.entrypoints.openai.api_server \
    --gpu-memory-utilization 0.75 \
    ...

# 解决方案 2：减少 max_model_len
python -m vllm.entrypoints.openai.api_server \
    --max-model-len 4096 \
    ...
```

#### 问题: 模型加载超时

```bash
# 症状
Connection timeout / Model loading taking too long

# 解决方案：预先下载模型
huggingface-cli download Qwen/Qwen2.5-Coder-7B-Instruct \
    --local-dir /workspace/models/Qwen2.5-Coder-7B-Instruct

# 使用本地路径启动
python -m vllm.entrypoints.openai.api_server \
    --model /workspace/models/Qwen2.5-Coder-7B-Instruct \
    ...
```

#### 问题: vLLM 版本不兼容

如果使用 PyTorch 模板遇到版本不兼容问题：

```bash
# 检查当前 CUDA 版本
nvcc --version

# 检查 PyTorch CUDA 版本
python -c "import torch; print(torch.version.cuda)"

# 安装匹配的 vLLM 版本
# 对于 CUDA 12.4:
pip install vllm==0.8.5

# 或者从源码构建
pip install vllm --no-build-isolation
```

**建议**：直接使用 vast.ai 的 vLLM 模板避免此问题。

### 10.2 SandboxFusion 问题

#### 问题: Docker 权限不足

```bash
# 症状
permission denied / cannot create namespace

# 解决方案：确保使用 --privileged
docker run --privileged ...
```

#### 问题: 504 Gateway Timeout

```bash
# 症状
requests timeout / 504 error

# 解决方案 1：降低并发数
python src/phase0_eval.py --max_concurrent 16 ...

# 解决方案 2：增加超时时间
python src/phase0_eval.py --run_timeout 30 ...
```

### 10.3 评测脚本问题

#### 问题: ModuleNotFoundError

```bash
# 症状
ModuleNotFoundError: No module named 'sandbox_fusion'

# 解决方案
pip install sandbox-fusion
```

#### 问题: 数据集加载失败

```bash
# 检查数据文件
ls -la data/manifests/
ls -la data/raw/

# 如果数据缺失，重新获取
python src/data_governance.py --source huggingface --output_dir data/
```

---

## 11. 一键运行脚本

将以下脚本保存为 `run_phase0_eval.sh`：

```bash
#!/bin/bash
set -e

echo "============================================================"
echo "   Phase 0 Baseline Evaluation on vast.ai"
echo "============================================================"
echo ""

# =============================================================================
# 配置（根据你的仓库结构修改）
# =============================================================================
MODEL_NAME="Qwen/Qwen2.5-Coder-7B-Instruct"
VLLM_PORT=8000
SANDBOX_PORT=8080
WORK_DIR="/workspace/rlvr_coding/<your-repo>/verl/coding_model_project"
OUTPUT_DIR="${WORK_DIR}/outputs/phase0_$(date +%Y%m%d_%H%M%S)"

# =============================================================================
# 1. 启动 SandboxFusion
# =============================================================================
echo "[1/5] Starting SandboxFusion..."

docker rm -f sandbox_fusion 2>/dev/null || true

docker run -d \
    --name sandbox_fusion \
    --rm \
    --privileged \
    -p ${SANDBOX_PORT}:8080 \
    volcengine/sandbox-fusion:server-20250609

echo "Waiting for SandboxFusion to start..."
sleep 10

until curl -s http://localhost:${SANDBOX_PORT}/health > /dev/null 2>&1; do
    echo "  Still waiting..."
    sleep 5
done
echo "  SandboxFusion is ready!"

# =============================================================================
# 2. 启动 vLLM
# =============================================================================
echo ""
echo "[2/5] Starting vLLM server..."

if pgrep -f "vllm.entrypoints.openai.api_server" > /dev/null; then
    echo "  vLLM is already running"
else
    nohup python -m vllm.entrypoints.openai.api_server \
        --model ${MODEL_NAME} \
        --port ${VLLM_PORT} \
        --host 0.0.0.0 \
        --tensor-parallel-size 1 \
        --gpu-memory-utilization 0.85 \
        --max-model-len 6144 \
        --dtype bfloat16 \
        --trust-remote-code \
        --enable-prefix-caching \
        --disable-log-requests \
        > ${WORK_DIR}/vllm.log 2>&1 &

    echo "  Waiting for vLLM to load model (this may take 2-5 minutes)..."
fi

until curl -s http://localhost:${VLLM_PORT}/health > /dev/null 2>&1; do
    echo "  Still loading model..."
    sleep 30
done
echo "  vLLM is ready!"

# =============================================================================
# 3. 验证服务
# =============================================================================
echo ""
echo "[3/5] Verifying services..."

echo "  Testing vLLM..."
VLLM_TEST=$(curl -s http://localhost:${VLLM_PORT}/v1/models)
if echo "$VLLM_TEST" | grep -q "Qwen"; then
    echo "    vLLM model loaded: OK"
else
    echo "    vLLM model check: FAILED"
    exit 1
fi

echo "  Testing SandboxFusion..."
SANDBOX_TEST=$(curl -s -X POST http://localhost:${SANDBOX_PORT}/run_code \
    -H "Content-Type: application/json" \
    -d '{"code": "print(1+1)", "language": "python", "run_timeout": 10}')
if echo "$SANDBOX_TEST" | grep -q "Success"; then
    echo "    SandboxFusion: OK"
else
    echo "    SandboxFusion test: FAILED"
    exit 1
fi

# =============================================================================
# 4. 运行评测
# =============================================================================
echo ""
echo "[4/5] Running Phase 0 evaluation..."
echo "  Output directory: ${OUTPUT_DIR}"
echo ""

cd ${WORK_DIR}

python src/phase0_eval.py \
    --mode simple \
    --model ${MODEL_NAME} \
    --vllm_url http://localhost:${VLLM_PORT} \
    --sandbox_url http://localhost:${SANDBOX_PORT} \
    --manifest_dir data/manifests \
    --datasets humaneval mbpp_reg codecontests_valid \
    --temperature 0.0 \
    --max_tokens 2048 \
    --max_concurrent 32 \
    --batch_size 50 \
    --output_dir ${OUTPUT_DIR}

# =============================================================================
# 5. 显示结果
# =============================================================================
echo ""
echo "[5/5] Evaluation complete!"
echo ""
echo "============================================================"
echo "   Results Summary"
echo "============================================================"
echo ""

python -c "
import json
with open('${OUTPUT_DIR}/metrics.json') as f:
    data = json.load(f)
for dataset, metrics in data.items():
    print(f'{dataset}:')
    print(f'  accepted@1:       {metrics.get(\"accepted_at_1\", 0):.2%}')
    print(f'  pass_ratio_mean:  {metrics.get(\"pass_ratio_mean\", 0):.4f}')
    print(f'  avg_gen_tokens:   {metrics.get(\"avg_gen_tokens\", 0):.1f}')
    print(f'  throughput:       {metrics.get(\"throughput\", 0):.2f} problems/sec')
    print()
"

echo "Full results saved to: ${OUTPUT_DIR}"
echo "Done!"
```

使用方法：

```bash
# 修改脚本中的 WORK_DIR 为你的实际路径
chmod +x run_phase0_eval.sh
./run_phase0_eval.sh
```

---

## 12. 成本估算

### 12.1 时间估算

| 阶段 | 预估时间 |
|------|---------|
| 环境配置 | 10-20 分钟 |
| 模型下载（首次）| 5-10 分钟 |
| vLLM 启动 | 2-5 分钟 |
| 评测运行 | 1-2 小时 |
| **总计** | **~1.5-3 小时** |

### 12.2 费用估算

| 资源 | 规格 | 单价 | 时长 | 费用 |
|------|------|------|------|------|
| GPU | RTX 4090 (24GB) | ~$0.40/hr | 3 hr | ~$1.20 |
| GPU | A100 40GB | ~$1.00/hr | 2 hr | ~$2.00 |
| 存储 | 100 GB | ~$0.05/hr | 3 hr | ~$0.15 |

**总费用估算**：
- RTX 4090 方案：**~$1.35**
- A100 方案：**~$2.15**

### 12.3 省钱技巧

1. **使用竞价实例**：vast.ai 的竞价实例比固定价格便宜 50-70%
2. **使用 vLLM 模板**：省去安装 vLLM 的时间
3. **预下载模型**：避免重复下载浪费时间
4. **批量运行**：一次性运行完所有数据集
5. **及时关闭**：评测完成后立即释放实例

---

## 附录：快速参考

### 服务端口

| 服务 | 端口 | 用途 |
|------|------|------|
| vLLM | 8000 | 模型推理 API |
| SandboxFusion | 8080 | 代码评测 API |

### 常用命令

```bash
# 检查服务状态
curl http://localhost:8000/health  # vLLM
curl http://localhost:8080/health  # SandboxFusion

# 查看 GPU 使用
nvidia-smi

# 查看 tmux 会话
tmux ls

# 附加到 tmux 会话
tmux attach -t vllm
tmux attach -t sandbox

# 停止服务
docker stop sandbox_fusion
pkill -f "vllm.entrypoints"
```

### 推荐 Docker 模板

| 模板 | 镜像 | 说明 |
|------|------|------|
| **vLLM (推荐)** | `vastai/vllm:v0.8.5-cuda-12.4-pytorch-2.6.0-py312` | vLLM 已预装 |
| vLLM (新版) | `vastai/vllm:v0.10.2-cuda-12.8-pytorch-2.8.0-py312` | 最新版本 |
| PyTorch | `vastai/pytorch:@vastai-automatic-tag` | 需手动安装 vLLM |

### 参考资料

- [vLLM 安装文档](https://docs.vllm.ai/en/stable/getting_started/installation/gpu/)
- [vast.ai PyTorch 文档](https://docs.vast.ai/pytorch)
- [vast.ai Docker Hub](https://hub.docker.com/u/vastai)
- [SandboxFusion 文档](https://github.com/bytedance/SandboxFusion)
