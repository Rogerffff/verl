# Phase 0: Baseline è¯¦ç»†å®æ–½è®¡åˆ’

---

> **ğŸ“š ç›¸å…³æ–‡æ¡£**ï¼šæœ¬æ–‡æ¡£çš„æŠ€æœ¯ç»†èŠ‚ï¼ˆç‰¹åˆ«æ˜¯ verl Standalone Rollout æ¨¡å¼çš„ä»£ç æ‰§è¡Œæµç¨‹ï¼‰å¯å‚è€ƒ [verl_standalone_rollout_guide.md](./verl_standalone_rollout_guide.md)ï¼Œè¯¥æ–‡æ¡£æä¾›äº†æ›´æ·±å…¥çš„æºç çº§è§£æã€‚

---

## ä¸€ã€Phase 0 æ¦‚è¿°

### 1.1 ç›®æ ‡å®šä½

Phase 0 æ˜¯æ•´ä¸ª RLVR Coding Model é¡¹ç›®çš„èµ·ç‚¹ï¼Œå…¶æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

1. **å»ºç«‹å¯¹ç…§åŸºå‡†**ï¼šä¸ºåç»­æ‰€æœ‰é˜¶æ®µï¼ˆSFTã€DPOã€GRPOï¼‰æä¾›å¯ä¿¡çš„æ€§èƒ½å‚ç…§ç‚¹
2. **éªŒè¯è¯„æµ‹æµæ°´çº¿**ï¼šç¡®ä¿ SandboxFusion åˆ¤é¢˜ç³»ç»Ÿå·¥ä½œæ­£å¸¸
3. **æ”¶é›†æˆæœ¬åŸºçº¿**ï¼šè®°å½•æ¨ç†ååé‡ã€åˆ¤é¢˜æ—¶é—´ç­‰æŒ‡æ ‡
4. **éªŒè¯æ•°æ®æ²»ç†**ï¼šç¡®ä¿æ•°æ®åˆ’åˆ†æ­£ç¡®ã€æ— æ³„æ¼

### 1.2 Phase 0 äº§å‡ºæ¸…å•

| äº§å‡ºç±»å‹ | å…·ä½“å†…å®¹ | é‡è¦æ€§ |
|---------|---------|--------|
| **è´¨é‡æŒ‡æ ‡** | accepted@1, pass_ratio(mean/p50/p90), exec_success_rate, error breakdown | â˜…â˜…â˜… |
| **æˆæœ¬æŒ‡æ ‡** | avg_total_gen_tokens, avg_total_judge_time, throughput, cost_per_solved | â˜…â˜…â˜… |
| **æ•°æ®æ²»ç†** | manifest æ–‡ä»¶, å»é‡æŠ¥å‘Š, æ³„æ¼æ£€æŸ¥æŠ¥å‘Š | â˜…â˜…â˜… |
| **é—®ç­”æ—¥å¿—** | 120 æ¡è¯¦ç»†æ—¥å¿—ï¼ˆæŒ‰æ•°æ®é›†åˆ†å±‚æŠ½æ ·ï¼‰ | â˜…â˜… |
| **WandB é¢æ¿** | åŸºçº¿æŒ‡æ ‡è®°å½• | â˜…â˜… |

### 1.3 è¯„æµ‹æ•°æ®é›†

| æ•°æ®é›† | è§’è‰² | æ ·æœ¬æ•°ï¼ˆé¢„ä¼°ï¼‰ | è¯„æµ‹ç›®çš„ |
|--------|------|---------------|---------|
| CodeContests_valid | Dev/Val | ~200-500 | ä¸»éªŒè¯é›†ï¼Œé«˜é¢‘å›å½’ |
| CodeContests_test | Test | ~200-500 | æœ€ç»ˆè¯„æµ‹ï¼Œç¦æ­¢è°ƒå‚ |
| HumanEval | Test only | 164 | è¡Œä¸šå¯¹æ ‡åŸºçº¿ |
| MBPP_reg | Dev/Val | 100-200 | å›å½’ç›‘æ§åŸºçº¿ |

---

## äºŒã€verl æ¡†æ¶æ ¸å¿ƒæ¦‚å¿µï¼ˆæ•™å­¦å†…å®¹ï¼‰

åœ¨å¼€å§‹å®æ–½ä¹‹å‰ï¼Œä½ éœ€è¦ç†è§£ verl æ¡†æ¶çš„æ ¸å¿ƒæ¶æ„å’Œæ•°æ®æµã€‚

### 2.1 verl æ•´ä½“æ¶æ„

#### è®­ç»ƒæ¨¡å¼ï¼ˆGRPO/PPOï¼‰æ¶æ„
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              PPO Ray Trainer                                  â”‚
â”‚                        (verl/trainer/ppo/ray_trainer.py)                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         è®­ç»ƒå¾ªç¯ (Training Loop)                         â”‚ â”‚
â”‚  â”‚  1. ç”Ÿæˆåºåˆ— (Rollout)  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>   â”‚ â”‚
â”‚  â”‚  2. è®¡ç®—å¥–åŠ± (Reward)   <â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚ â”‚
â”‚  â”‚  3. è®¡ç®—ä¼˜åŠ¿ (Advantage)                                                 â”‚ â”‚
â”‚  â”‚  4. æ›´æ–°ç­–ç•¥ (Policy Update)                                             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                          â”‚                          â”‚
           â–¼                          â–¼                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Rollout Worker    â”‚    â”‚    Actor Worker     â”‚    â”‚   Reward Manager    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    vLLM       â”‚  â”‚    â”‚  â”‚    FSDP       â”‚  â”‚    â”‚  â”‚   compute_    â”‚  â”‚
â”‚  â”‚      or       â”‚  â”‚    â”‚  â”‚      or       â”‚  â”‚    â”‚  â”‚   score()     â”‚  â”‚
â”‚  â”‚   SGLang      â”‚  â”‚    â”‚  â”‚  Megatron-LM  â”‚  â”‚    â”‚  â”‚               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                     â”‚    â”‚                     â”‚    â”‚         â”‚           â”‚
â”‚  - åºåˆ—ç”Ÿæˆ          â”‚    â”‚  - ç­–ç•¥æ›´æ–°          â”‚    â”‚         â–¼           â”‚
â”‚  - æƒé‡åŒæ­¥          â”‚    â”‚  - log_prob è®¡ç®—     â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ SandboxFusion â”‚  â”‚
                                                       â”‚  â”‚   API è°ƒç”¨     â”‚  â”‚
                                                       â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Phase 0 è¯„æµ‹æ¶æ„ï¼ˆStandalone æ¨¡å¼ï¼‰

**é‡è¦**ï¼šPhase 0 æ˜¯çº¯è¯„æµ‹é˜¶æ®µï¼Œä¸æ¶‰åŠè®­ç»ƒï¼Œå› æ­¤ä½¿ç”¨ **Standalone Rollout** æ¨¡å¼ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          Phase 0 è¯„æµ‹æµç¨‹                                     â”‚
â”‚                                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚                         Ray é›†ç¾¤åè°ƒ                                  â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â”‚                                         â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚            â”‚                       â”‚                       â”‚                â”‚
â”‚            â–¼                       â–¼                       â–¼                â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚   â”‚  vLLM Replica 0 â”‚     â”‚  vLLM Replica 1 â”‚     â”‚  vLLM Replica N â”‚      â”‚
â”‚   â”‚  (GPU 0-1, TP=2)â”‚     â”‚  (GPU 2-3, TP=2)â”‚     â”‚  (GPU 2N-2N+1)  â”‚      â”‚
â”‚   â”‚                 â”‚     â”‚                 â”‚     â”‚                 â”‚      â”‚
â”‚   â”‚  HTTP Server    â”‚     â”‚  HTTP Server    â”‚     â”‚  HTTP Server    â”‚      â”‚
â”‚   â”‚  ip:port_0      â”‚     â”‚  ip:port_1      â”‚     â”‚  ip:port_N      â”‚      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚            â”‚                       â”‚                       â”‚                â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   OpenAI-Compatible API     â”‚                         â”‚
â”‚                     â”‚   POST /v1/chat/completions â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   Code Generation Results   â”‚                         â”‚
â”‚                     â”‚   (completions)             â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   SandboxFusion åˆ¤é¢˜        â”‚                         â”‚
â”‚                     â”‚   æ–¹å¼A: submit() API       â”‚                         â”‚
â”‚                     â”‚   æ–¹å¼B: compute_score()    â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                    â”‚                                         â”‚
â”‚                                    â–¼                                         â”‚
â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚                     â”‚   Metrics + QA Logs         â”‚                         â”‚
â”‚                     â”‚   - accepted@1              â”‚                         â”‚
â”‚                     â”‚   - pass_ratio              â”‚                         â”‚
â”‚                     â”‚   - error_breakdown         â”‚                         â”‚
â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Phase 0 å…³æ³¨çš„ç»„ä»¶

ç”±äº Phase 0 æ˜¯çº¯è¯„æµ‹é˜¶æ®µï¼ˆä¸è®­ç»ƒï¼‰ï¼Œæˆ‘ä»¬ä¸»è¦å…³æ³¨ï¼š

| ç»„ä»¶ | æ–‡ä»¶ä½ç½® | Phase 0 ç”¨é€” |
|------|----------|-------------|
| **Rollout Worker** | `verl/verl/workers/rollout/` | ä½¿ç”¨ vLLM/SGLang ç”Ÿæˆä»£ç  |
| **Reward Manager** | `verl/verl/workers/reward_manager/` | è°ƒç”¨ SandboxFusion è®¡ç®— pass_ratio |
| **compute_score** | `verl/verl/utils/reward_score/sandbox_fusion/__init__.py` | æ ¸å¿ƒè¯„åˆ†é€»è¾‘ |
| **check_correctness** | `verl/verl/utils/reward_score/sandbox_fusion/utils.py` | é€æµ‹è¯•ç”¨ä¾‹åˆ¤é¢˜ |

### 2.3 ä»£ç æ‰§è¡Œæµç¨‹ï¼ˆPhase 0 è§†è§’ï¼‰- ä½¿ç”¨ verl åˆ†å¸ƒå¼æ¶æ„

**æ ¸å¿ƒè¦ç‚¹**ï¼šPhase 0 è¯„æµ‹ä½¿ç”¨ verl çš„ **Standalone Rollout æ¨¡å¼**ï¼Œé€šè¿‡ vLLM/SGLang å¼•æ“æä¾›é«˜æ•ˆçš„åˆ†å¸ƒå¼æ¨ç†ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         1. Ray é›†ç¾¤åˆå§‹åŒ–                                   â”‚
â”‚  ray.init(runtime_env={"env_vars": {...}})                                 â”‚
â”‚  è®¾ç½®ç¯å¢ƒ: TOKENIZERS_PARALLELISM, NCCL_DEBUG, VLLM_USE_V1                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    2. åˆ›å»º Standalone Rollout Server                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  rollout_class = get_rollout_replica_class("vllm")  # æˆ– "sglang"   â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  rollout_servers = [                                                 â”‚  â”‚
â”‚  â”‚      rollout_class(                                                  â”‚  â”‚
â”‚  â”‚          replica_rank=i,                                             â”‚  â”‚
â”‚  â”‚          config=RolloutConfig(...),      # æ¨ç†é…ç½®                  â”‚  â”‚
â”‚  â”‚          model_config=HFModelConfig(...) # æ¨¡å‹é…ç½®                  â”‚  â”‚
â”‚  â”‚      ) for i in range(num_replicas)                                  â”‚  â”‚
â”‚  â”‚  ]                                                                   â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  await asyncio.gather(*[                                             â”‚  â”‚
â”‚  â”‚      server.init_standalone()  # åˆå§‹åŒ–ç‹¬ç«‹æ¨ç†æœåŠ¡å™¨                â”‚  â”‚
â”‚  â”‚      for server in rollout_servers                                   â”‚  â”‚
â”‚  â”‚  ])                                                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â”‚  ç»“æœ: æ¯ä¸ª replica å¯åŠ¨ä¸€ä¸ª HTTP æœåŠ¡å™¨ï¼Œæä¾› OpenAI å…¼å®¹ API            â”‚
â”‚  server_addresses = ["ip:port", ...]                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       3. åŠ è½½è¯„æµ‹æ•°æ®é›†                                     â”‚
â”‚  data = pd.read_parquet("data/eval/codecontests_valid.parquet")            â”‚
â”‚  æ ¼å¼: {prompt, ground_truth (test_cases JSON), problem_id, ...}           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               4. é€šè¿‡ OpenAI API è°ƒç”¨ vLLM/SGLang ç”Ÿæˆä»£ç                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  async with aiohttp.ClientSession() as session:                      â”‚  â”‚
â”‚  â”‚      response = await session.post(                                  â”‚  â”‚
â”‚  â”‚          f"http://{server_address}/v1/chat/completions",             â”‚  â”‚
â”‚  â”‚          json={                                                      â”‚  â”‚
â”‚  â”‚              "model": model_path,                                    â”‚  â”‚
â”‚  â”‚              "messages": [{"role": "user", "content": prompt}],      â”‚  â”‚
â”‚  â”‚              "temperature": 0.0,  # EVAL@1 åè®®: greedy               â”‚  â”‚
â”‚  â”‚              "max_tokens": 2048                                      â”‚  â”‚
â”‚  â”‚          }                                                           â”‚  â”‚
â”‚  â”‚      )                                                               â”‚  â”‚
â”‚  â”‚  completion = response.choices[0].message.content                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â”‚  ç‰¹ç‚¹: å¼‚æ­¥å¹¶å‘è¯·æ±‚ï¼Œæ”¯æŒå¤š replica è´Ÿè½½å‡è¡¡                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    5. ä»£ç æå–ä¸é¢„å¤„ç†                                      â”‚
â”‚  ä» completion ä¸­æå–ä»£ç å—ï¼Œå¤„ç† ```python ... ``` æ ¼å¼                   â”‚
â”‚  åº”ç”¨ Guardrails: ç©ºè¾“å‡ºæ£€æµ‹ã€è¶…é•¿æˆªæ–­ã€éä»£ç è¿‡æ»¤                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    6. SandboxFusion åˆ¤é¢˜                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  score, metadata = compute_score(                                    â”‚  â”‚
â”‚  â”‚      sandbox_fusion_url=sandbox_url,                                 â”‚  â”‚
â”‚  â”‚      completion=code,                                                â”‚  â”‚
â”‚  â”‚      test_cases=test_cases,                                          â”‚  â”‚
â”‚  â”‚      timeout=10,                                                     â”‚  â”‚
â”‚  â”‚      memory_limit_mb=1024                                            â”‚  â”‚
â”‚  â”‚  )                                                                   â”‚  â”‚
â”‚  â”‚                                                                      â”‚  â”‚
â”‚  â”‚  # score = pass_ratio (0.0 ~ 1.0)                                    â”‚  â”‚
â”‚  â”‚  # metadata = [æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯¦ç»†ç»“æœ]                               â”‚  â”‚
â”‚  â”‚  #   - status: success/wrong_answer/runtime_error/timeout/compile    â”‚  â”‚
â”‚  â”‚  #   - duration: æ‰§è¡Œæ—¶é—´                                            â”‚  â”‚
â”‚  â”‚  #   - stdout/stderr: è¾“å‡ºä¿¡æ¯                                       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                            â”‚
â”‚  è¿”å›ç : True(é€šè¿‡) / False(WA) / -1(APIé”™è¯¯) / -2(RE) / -3(TLE) / -4(CE)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    7. æŒ‡æ ‡èšåˆä¸æ—¥å¿—è®°å½•                                    â”‚
â”‚  - è´¨é‡æŒ‡æ ‡: accepted@1, pass_ratio (mean/p50/p90), exec_success_rate      â”‚
â”‚  - æˆæœ¬æŒ‡æ ‡: avg_gen_tokens, avg_judge_time, throughput, cost_per_solved   â”‚
â”‚  - é”™è¯¯åˆ†å¸ƒ: syntax_error_rate, runtime_error_rate, timeout_rate, wa_rate  â”‚
â”‚  - WandB è®°å½• + JSONL é—®ç­”æ—¥å¿—                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.4 verl Standalone æ¨¡å¼è¯¦è§£

**ä¸ºä»€ä¹ˆä½¿ç”¨ Standalone æ¨¡å¼ï¼Ÿ**

verl æä¾›ä¸‰ç§ Rollout æ¨¡å¼ï¼š

| æ¨¡å¼ | è¯´æ˜ | é€‚ç”¨åœºæ™¯ |
|------|------|---------|
| **HYBRID** | Rollout ä¸è®­ç»ƒå¼•æ“èåˆåœ¨åŒä¸€è¿›ç¨‹ | On-policy è®­ç»ƒï¼ˆGRPO/PPOï¼‰ |
| **COLOCATED** | Rollout ä¸ Hybrid å¼•æ“å…±äº« GPUï¼Œç‹¬ç«‹è¿›ç¨‹ | GRM (LLM as a Judge) |
| **STANDALONE** | ç‹¬ç«‹ GPU èµ„æºï¼Œdisaggregated æ¶æ„ | **Phase 0 è¯„æµ‹**ã€Off-policy è®­ç»ƒ |

å¯¹äº Phase 0 çº¯è¯„æµ‹åœºæ™¯ï¼Œ**STANDALONE æ¨¡å¼**æ˜¯æœ€ä½³é€‰æ‹©ï¼š
- ä¸éœ€è¦è®­ç»ƒå¼•æ“ï¼ˆæ— æ¢¯åº¦è®¡ç®—ï¼‰
- å¯ä»¥ç‹¬å  GPU èµ„æºæœ€å¤§åŒ–æ¨ç†åå
- é€šè¿‡ HTTP API æä¾›çµæ´»çš„æ¥å…¥æ–¹å¼

**å…³é”®ä»£ç è·¯å¾„**ï¼š

```
verl/verl/workers/rollout/replica.py
â”œâ”€â”€ RolloutReplica (åŸºç±»)
â”‚   â”œâ”€â”€ init_standalone()  â† Phase 0 ä½¿ç”¨è¿™ä¸ªæ–¹æ³•
â”‚   â”œâ”€â”€ init_hybrid()      â† GRPO/PPO è®­ç»ƒä½¿ç”¨
â”‚   â””â”€â”€ init_colocated()   â† GRM ä½¿ç”¨
â”‚
â”œâ”€â”€ get_rollout_replica_class("vllm")
â”‚   â””â”€â”€ returns vLLMReplica
â”‚
â””â”€â”€ get_rollout_replica_class("sglang")
    â””â”€â”€ returns SGLangReplica

verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py
â”œâ”€â”€ vLLMReplica
â”‚   â”œâ”€â”€ launch_servers()   â† å¯åŠ¨ HTTP æœåŠ¡å™¨
â”‚   â””â”€â”€ generate()         â† Token-in-token-out ç”Ÿæˆ
â”‚
â””â”€â”€ vLLMHttpServer         â† å•èŠ‚ç‚¹ HTTP æœåŠ¡å™¨

# å®˜æ–¹å‚è€ƒå®ç°ï¼ˆæ¨èå‚è€ƒï¼‰
verl/verl/trainer/main_generation_server.py
â”œâ”€â”€ start_server()         â† å¯åŠ¨å¤š replica æœåŠ¡å™¨
â”œâ”€â”€ submit_request()       â† OpenAI API è°ƒç”¨ç¤ºä¾‹
â””â”€â”€ generate()             â† æ‰¹é‡ç”Ÿæˆæµç¨‹
```

> **ğŸ’¡ æç¤º**ï¼š`main_generation_server.py` æ˜¯ verl å®˜æ–¹æä¾›çš„ Standalone æ¨¡å¼å‚è€ƒå®ç°ï¼ŒPhase 0 è¯„æµ‹è„šæœ¬å¯ç›´æ¥å‚è€ƒè¯¥æ–‡ä»¶çš„å®ç°æ–¹å¼ã€‚è¯¦ç»†è§£æè§ [verl_standalone_rollout_guide.md](./verl_standalone_rollout_guide.md) ç¬¬äº”ç« ã€‚

### 2.5 SandboxFusion è¿”å›å€¼è¯¦è§£

```python
# check_correctness() è¿”å›å€¼
(results_list, metadata_list) = check_correctness(...)

# results_list å…ƒç´ å«ä¹‰ï¼š
#   True:  æµ‹è¯•é€šè¿‡
#   False: Wrong Answerï¼ˆèƒ½è¿è¡Œä½†è¾“å‡ºä¸å¯¹ï¼‰
#   -1:    API é”™è¯¯ / Sandbox å†…éƒ¨é”™è¯¯
#   -2:    Runtime Errorï¼ˆè¿è¡Œæ—¶å´©æºƒï¼‰
#   -3:    Timeoutï¼ˆè¶…æ—¶ï¼‰
#   -4:    Compile Errorï¼ˆç¼–è¯‘/è¯­æ³•é”™è¯¯ï¼‰

# metadata_list æ¯ä¸ªå…ƒç´ çš„ç»“æ„ï¼š
{
    "case_index": 0,
    "input": "1 2\n",
    "expected_output": "3\n",
    "status": "success" | "wrong_answer" | "runtime_error" | "timeout" | "compile_error" | "api_error",
    "stdout": "3\n",
    "stderr": "",
    "exit_code": 0,
    "duration": 0.05,  # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
    "compile_duration": 0.01,
    "compile_stderr": None,
}
```

### 2.6 ä¸ºä»€ä¹ˆä¸ä½¿ç”¨çº¯ HuggingFaceï¼Ÿ

| æ–¹é¢ | çº¯ HuggingFace | verl åˆ†å¸ƒå¼æ¶æ„ |
|------|---------------|----------------|
| **æ¨¡å‹åŠ è½½** | `AutoModelForCausalLM.from_pretrained()` | vLLM/SGLang å¼•æ“è‡ªåŠ¨åŠ è½½ |
| **æ¨ç†** | `model.generate()` åŒæ­¥ä¸²è¡Œ | HTTP API å¼‚æ­¥å¹¶å‘ |
| **å¹¶è¡Œåº¦** | å• GPU æˆ– device_map="auto" | çœŸæ­£çš„ Tensor Parallel |
| **ååé‡** | ä½ï¼ˆæ—  PagedAttentionï¼‰ | é«˜ï¼ˆKV Cache ä¼˜åŒ–ï¼‰ |
| **ä¸ GRPO ä¸€è‡´æ€§** | ä¸ä¸€è‡´ | **å®Œå…¨ä¸€è‡´** |
| **å¯æ‰©å±•æ€§** | å—é™ | å¤šèŠ‚ç‚¹å¤š replica |

**å…³é”®åŸå› **ï¼šPhase 0 çš„åŸºçº¿éœ€è¦ä¸åç»­ Phaseï¼ˆSFTã€GRPOï¼‰çš„è¯„æµ‹ä¿æŒä¸€è‡´ã€‚ä½¿ç”¨ verl æ¶æ„å¯ä»¥ç¡®ä¿ï¼š

1. **å…¬å¹³å¯¹æ¯”**ï¼šåŒæ ·çš„æ¨ç†å¼•æ“å’Œè§£ç å‚æ•°
2. **ä»£ç å¤ç”¨**ï¼šè¯„æµ‹è„šæœ¬å¯ç›´æ¥ç”¨äº GRPO è®­ç»ƒä¸­çš„ rollout é˜¶æ®µ
3. **æ€§èƒ½åŸºçº¿**ï¼šthroughput æŒ‡æ ‡å…·æœ‰å‚è€ƒä»·å€¼

---

## ä¸‰ã€SandboxFusion ä½¿ç”¨æŒ‡å—ï¼ˆæ•™å­¦å†…å®¹ï¼‰

### 3.1 SandboxFusion æ¶æ„

SandboxFusion æ˜¯å­—èŠ‚è·³åŠ¨å¼€å‘çš„å®‰å…¨ä»£ç æ²™ç›’ç³»ç»Ÿï¼Œæ”¯æŒï¼š
- 30+ ç¼–ç¨‹è¯­è¨€
- 13+ è¯„æµ‹æ•°æ®é›†ï¼ˆHumanEval, MBPP, CodeContests ç­‰ï¼‰
- è¿›ç¨‹éš”ç¦»ä¸èµ„æºé™åˆ¶

### 3.2 æ ¸å¿ƒ API ç«¯ç‚¹

| ç«¯ç‚¹ | æ–¹æ³• | ç”¨é€” |
|------|------|------|
| `/run_code` | POST | æ‰§è¡Œå•æ®µä»£ç  |
| `/list_datasets` | GET | åˆ—å‡ºå¯ç”¨æ•°æ®é›† |
| `/get_prompts` | POST | è·å–æ•°æ®é›†é¢˜ç›® |
| `/submit` | POST | æäº¤ä»£ç è¯„æµ‹ |

### 3.3 ä¸¤ç§ä½¿ç”¨æ¨¡å¼

**æ¨¡å¼ Aï¼šç›´æ¥ä½¿ç”¨ `/run_code`ï¼ˆverl é»˜è®¤æ–¹å¼ï¼‰**

```python
# verl ä½¿ç”¨è¿™ç§æ–¹å¼ï¼šç›´æ¥è°ƒç”¨ /run_code æ‰§è¡Œä»£ç 
# æµ‹è¯•ç”¨ä¾‹é€šè¿‡ stdin/stdout ä¼ å…¥ä¼ å‡º

payload = {
    "compile_timeout": 10,
    "run_timeout": 10,
    "code": "a, b = map(int, input().split())\nprint(a + b)",
    "stdin": "1 2\n",
    "memory_limit_MB": 1024,
    "language": "python",
}
response = requests.post(f"{sandbox_url}/run_code", json=payload)
# å“åº”åŒ…å« stdout, stderr, status, duration ç­‰
```

**æ¨¡å¼ Bï¼šä½¿ç”¨ Dataset APIï¼ˆSandboxFusion SDKï¼‰**

```python
from sandbox_fusion import get_prompts, submit, GetPromptsRequest, SubmitRequest, TestConfig

# 1. è·å–æ•°æ®é›†é¢˜ç›®
prompts = get_prompts(GetPromptsRequest(
    dataset='codecontests',
    config=TestConfig(language='python', locale='en')
))

# 2. æäº¤è¯„æµ‹
result = submit(SubmitRequest(
    dataset='codecontests',
    id='problem_id',
    completion='<model generated code>',
    config=TestConfig(language='python', run_timeout=20)
))

# 3. è·å–ç»“æœ
print(result.accepted)  # True/False
print(result.tests)     # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯¦ç»†ç»“æœ
```

### 3.4 æ•°æ®é›†ç±»å‹å¯¹æ¯”

| ç±»å‹ | ä»£è¡¨æ•°æ®é›† | è¯„æµ‹æ–¹å¼ | æ•°æ®ç»“æ„ |
|------|-----------|---------|---------|
| **AutoEval** | HumanEval, MBPP | å‡½æ•°æµ‹è¯• | `{id, content, test, canonical_solution}` |
| **CommonOJ** | CodeContests | stdin/stdout | `{id, content, test_cases: [{input, output}]}` |

### 3.5 Phase 0 æ¨èä½¿ç”¨æ¨¡å¼

å¯¹äº Phase 0ï¼Œæœ‰ä¸¤ç§è¯„æµ‹æ–¹å¼å¯é€‰ï¼š

#### æ–¹å¼ Aï¼šä½¿ç”¨ `submit()` APIï¼ˆæ¨èï¼Œæ›´ç®€å•ï¼‰

```python
from sandbox_fusion import submit, SubmitRequest, TestConfig

# ç›´æ¥æäº¤è¯„æµ‹ï¼Œæ— éœ€ç®¡ç†æµ‹è¯•ç”¨ä¾‹
result = submit(SubmitRequest(
    dataset='humaneval',              # SandboxFusion æ•°æ®é›†å
    id=record['sandbox_id'],          # é—®é¢˜ ID
    completion=generated_code,
    config=TestConfig(language='python', run_timeout=10)
))

print(f"Accepted: {result.accepted}")
```

**ä¼˜ç‚¹**ï¼š
- æ— éœ€è‡ªå·±ç®¡ç†æµ‹è¯•ç”¨ä¾‹
- ä¸æ•°æ®è·å–ä½¿ç”¨ç›¸åŒçš„ SandboxFusion SDK
- ä»£ç ç®€æ´

#### æ–¹å¼ Bï¼šä½¿ç”¨ `compute_score()` å‡½æ•°

```python
from verl.utils.reward_score.sandbox_fusion import compute_score

score, metadata = compute_score(
    sandbox_fusion_url=sandbox_url,
    completion=code,
    test_cases=test_cases,  # éœ€è¦è‡ªå·±ä¼ å…¥æµ‹è¯•ç”¨ä¾‹
    timeout=10,
)
```

**ä¼˜ç‚¹**ï¼š
- è¿”å›çš„ metadata åŒ…å«è¯¦ç»†çš„æ‰§è¡Œä¿¡æ¯ï¼ˆduration, status ç­‰ï¼‰
- ä¸åç»­ Phase çš„ GRPO è®­ç»ƒä»£ç ä¿æŒä¸€è‡´

**å»ºè®®**ï¼šPhase 0 ä¼˜å…ˆä½¿ç”¨ `submit()` API ç®€åŒ–å®ç°ï¼Œä½†åœ¨ä»£ç ä¸­ä¿ç•™å¯¹ `compute_score()` çš„æ”¯æŒï¼Œä»¥ä¾¿ä¸ GRPO é˜¶æ®µå¯¹æ¯”ã€‚

---

## å››ã€æ•°æ®è·å–ä¸æ²»ç†

### 4.1 æ•°æ®é›†è·å–æ­¥éª¤

#### æ¨èæ–¹å¼ï¼šä» SandboxFusion SDK è·å–ï¼ˆPhase 0ï¼‰

ä½¿ç”¨ SandboxFusion SDK è·å–æ•°æ®æ˜¯æœ€ç®€å•çš„æ–¹å¼ï¼Œå› ä¸ºæ•°æ®æ ¼å¼å·²ç»é€‚é…è¯„æµ‹ç³»ç»Ÿï¼š

```python
# scripts/download_datasets_from_sandbox.py
from sandbox_fusion import get_prompts, GetPromptsRequest, TestConfig
from pathlib import Path
import json

def download_from_sandbox(output_dir: str = "data/sandbox"):
    """ä» SandboxFusion SDK è·å–æ‰€æœ‰éœ€è¦çš„æ•°æ®é›†"""
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    # CodeContests (valid + test)
    for split in ['valid', 'test']:
        prompts = get_prompts(GetPromptsRequest(
            dataset='code_contests',
            config=TestConfig(language='python', locale='en', extra={'split': split}),
            offset=0, limit=100000
        ))

        records = []
        for item in prompts.prompts:
            records.append({
                "dataset": "codecontests",
                "split": split,
                "problem_id": item.id,
                "prompt": item.prompt,
                "sandbox_dataset": "code_contests",
                "sandbox_id": item.id,
            })

        save_path = output_path / f"codecontests_{split}.jsonl"
        with open(save_path, 'w') as f:
            for r in records:
                f.write(json.dumps(r, ensure_ascii=False) + '\n')
        print(f"  {split}: {len(records)} samples -> {save_path}")

    # HumanEval
    prompts = get_prompts(GetPromptsRequest(
        dataset='humaneval',
        config=TestConfig(language='python')
    ))
    records = [{"dataset": "humaneval", "split": "test",
                "problem_id": f"HumanEval/{item.id}", "prompt": item.prompt,
                "sandbox_dataset": "humaneval", "sandbox_id": item.id}
               for item in prompts.prompts]
    with open(output_path / "humaneval.jsonl", 'w') as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + '\n')

    # MBPP (å›å½’å­é›† ID 11-210)
    prompts = get_prompts(GetPromptsRequest(
        dataset='mbpp',
        config=TestConfig(is_fewshot=False)
    ))
    records = [{"dataset": "mbpp", "split": "test",
                "problem_id": f"MBPP/{item.id}", "prompt": item.prompt,
                "sandbox_dataset": "mbpp", "sandbox_id": item.id}
               for item in prompts.prompts
               if 11 <= int(item.id) <= 210]
    with open(output_path / "mbpp_reg.jsonl", 'w') as f:
        for r in records:
            f.write(json.dumps(r, ensure_ascii=False) + '\n')

    return output_path

if __name__ == "__main__":
    download_from_sandbox()
```

**ä¼˜ç‚¹**ï¼š
- æ•°æ®æ ¼å¼å·²é€‚é… `submit()` API è¯„æµ‹
- æ— éœ€è‡ªå·±ç®¡ç†æµ‹è¯•ç”¨ä¾‹
- ä»£ç ç®€æ´

#### å¤‡é€‰æ–¹å¼ï¼šä» HuggingFace ä¸‹è½½

å¦‚æœéœ€è¦æµ‹è¯•ç”¨ä¾‹ï¼ˆç”¨äº `compute_score()`ï¼‰æˆ– SandboxFusion æ•°æ®åº“ä¸­ç¼ºå°‘æŸäº›æ•°æ®ï¼š

```python
from datasets import load_dataset

codecontests = load_dataset("deepmind/code_contests")
humaneval = load_dataset("openai_humaneval")
mbpp = load_dataset("mbpp")
```

### 4.2 æ•°æ®é¢„å¤„ç†æµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. åŸå§‹æ•°æ®ä¸‹è½½  â”‚  HuggingFace Datasets
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. æ ¼å¼æ ‡å‡†åŒ–   â”‚  è½¬æ¢ä¸ºç»Ÿä¸€çš„ {prompt, test_cases, metadata} æ ¼å¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Canonicalize â”‚  è§„èŒƒåŒ– prompt æ–‡æœ¬ï¼ˆå»ç©ºç™½ã€ç»Ÿä¸€æ¢è¡Œç¬¦ï¼‰
â”‚                 â”‚  è®¡ç®— prompt_sha256
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Split å†…å»é‡  â”‚  æŒ‰ prompt_sha256 å»é‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. è·¨ Split æ£€æŸ¥ â”‚  ç¡®ä¿ train âˆ© valid = âˆ…
â”‚                 â”‚  ç¡®ä¿ train âˆ© test = âˆ…
â”‚                 â”‚  ç¡®ä¿ valid âˆ© test = âˆ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. å¤–éƒ¨æ³„æ¼æ£€æŸ¥  â”‚  CodeContests_train âˆ© HumanEval = âˆ…
â”‚                 â”‚  CodeContests_train âˆ© MBPP_reg = âˆ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. ç”Ÿæˆ Manifest â”‚  ä¿å­˜åˆ° data_manifests/
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3 Manifest æ–‡ä»¶æ ¼å¼

```python
# data_manifests/codecontests_valid.jsonl
# æ¯è¡Œä¸€æ¡è®°å½•

{
    "dataset": "codecontests",
    "split": "valid",
    "problem_id": "cc_valid_001",
    "prompt_sha256": "a1b2c3d4e5f6...",
    "prompt_length": 1234,
    "num_test_cases": 10,
    "version": "2024-01-31",
    "source_url": "https://huggingface.co/datasets/deepmind/code_contests"
}
```

### 4.4 æ•°æ®æ²»ç†è„šæœ¬æ¡†æ¶

```python
# ä¼ªä»£ç ï¼šscripts/data_governance.py

import hashlib
import json
from datasets import load_dataset
from typing import Dict, List, Set

def canonicalize_prompt(prompt: str) -> str:
    """è§„èŒƒåŒ– prompt æ–‡æœ¬"""
    # 1. ç»Ÿä¸€æ¢è¡Œç¬¦
    prompt = prompt.replace('\r\n', '\n')
    # 2. å»é™¤é¦–å°¾ç©ºç™½
    prompt = prompt.strip()
    # 3. å¤šä¸ªç©ºæ ¼å‹ç¼©ä¸ºå•ä¸ªï¼ˆä¿ç•™å¿…è¦æ ¼å¼ï¼‰
    # ... æ ¹æ®éœ€è¦æ·»åŠ æ›´å¤šè§„åˆ™
    return prompt

def compute_hash(text: str) -> str:
    """è®¡ç®— SHA256 å“ˆå¸Œ"""
    return hashlib.sha256(text.encode('utf-8')).hexdigest()

def deduplicate_split(records: List[Dict]) -> tuple[List[Dict], List[Dict]]:
    """Split å†…å»é‡ï¼Œè¿”å› (å»é‡å, é‡å¤è®°å½•)"""
    seen_hashes: Set[str] = set()
    unique, duplicates = [], []

    for record in records:
        h = record['prompt_sha256']
        if h not in seen_hashes:
            seen_hashes.add(h)
            unique.append(record)
        else:
            duplicates.append(record)

    return unique, duplicates

def check_cross_split_overlap(split_a: List[Dict], split_b: List[Dict]) -> List[str]:
    """æ£€æŸ¥è·¨ split é‡å ï¼Œè¿”å›é‡å çš„ hash åˆ—è¡¨"""
    hashes_a = {r['prompt_sha256'] for r in split_a}
    hashes_b = {r['prompt_sha256'] for r in split_b}
    return list(hashes_a & hashes_b)

def generate_manifest(records: List[Dict], output_path: str):
    """ç”Ÿæˆ manifest æ–‡ä»¶"""
    with open(output_path, 'w') as f:
        for record in records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

# ä¸»æµç¨‹
def main():
    # 1. åŠ è½½æ•°æ®
    codecontests = load_dataset("deepmind/code_contests")

    # 2. å¤„ç†æ¯ä¸ª split
    splits = {}
    for split_name in ['train', 'valid', 'test']:
        records = []
        for item in codecontests[split_name]:
            canonical = canonicalize_prompt(item['description'])
            records.append({
                'dataset': 'codecontests',
                'split': split_name,
                'problem_id': item['name'],
                'prompt_sha256': compute_hash(canonical),
                'num_test_cases': len(item.get('public_tests', {}).get('input', [])),
                'version': '2024-01-31',
            })

        # 3. Split å†…å»é‡
        unique, dups = deduplicate_split(records)
        splits[split_name] = unique

        # ä¿å­˜å»é‡è®°å½•
        if dups:
            generate_manifest(dups, f'data_manifests/duplicates_{split_name}.jsonl')

    # 4. è·¨ split æ£€æŸ¥
    overlaps_train_valid = check_cross_split_overlap(splits['train'], splits['valid'])
    overlaps_train_test = check_cross_split_overlap(splits['train'], splits['test'])

    assert len(overlaps_train_valid) == 0, f"train/valid overlap: {len(overlaps_train_valid)}"
    assert len(overlaps_train_test) == 0, f"train/test overlap: {len(overlaps_train_test)}"

    # 5. ç”Ÿæˆ manifest
    for split_name, records in splits.items():
        generate_manifest(records, f'data_manifests/codecontests_{split_name}.jsonl')

    # 6. è¾“å‡ºå®¡è®¡æŠ¥å‘Š
    print_audit_report(splits)
```

### 4.5 å®¡è®¡æŠ¥å‘Šæ¨¡æ¿

```markdown
# æ•°æ®æ²»ç†å®¡è®¡æŠ¥å‘Š

## 1. æ ·æœ¬ç»Ÿè®¡

| Split | å»é‡å‰ | å»é‡å | åˆ é™¤æ•° |
|-------|--------|--------|--------|
| train | 13328  | 13200  | 128    |
| valid | 117    | 117    | 0      |
| test  | 165    | 165    | 0      |

## 2. è·¨ Split ç²¾ç¡®é‡å æ£€æŸ¥

| æ£€æŸ¥å¯¹ | é‡å æ•° | çŠ¶æ€ |
|--------|--------|------|
| train âˆ© valid | 0 | âœ“ |
| train âˆ© test | 0 | âœ“ |
| valid âˆ© test | 0 | âœ“ |

## 3. å¤–éƒ¨æ³„æ¼æ£€æŸ¥

| æ£€æŸ¥å¯¹ | é‡å æ•° | çŠ¶æ€ |
|--------|--------|------|
| codecontests_train âˆ© humaneval | 0 | âœ“ |
| codecontests_train âˆ© mbpp_reg | 0 | âœ“ |

## 4. MBPP_reg å›ºå®šé¢˜å·åˆ—è¡¨

é€‰æ‹© MBPP ID 11-210ï¼ˆå…± 200 é¢˜ï¼‰ä½œä¸ºå›å½’ç›‘æ§å­é›†ã€‚

## 5. ç‰ˆæœ¬ä¿¡æ¯

- CodeContests: deepmind/code_contests @ 2024-01-31
- HumanEval: openai_humaneval @ 2024-01-31
- MBPP: google-research-datasets/mbpp @ 2024-01-31
```

---

## äº”ã€Phase 0 è¯„æµ‹å®æ–½æ­¥éª¤

### 5.1 ç¯å¢ƒå‡†å¤‡

#### Step 1: å¯åŠ¨ SandboxFusion æœåŠ¡

```bash
# æ–¹å¼ä¸€ï¼šæœ¬åœ°å¼€å‘æ¨¡å¼
cd SandboxFusion
make run  # ç«¯å£ 8080

# æ–¹å¼äºŒï¼šDocker ç”Ÿäº§æ¨¡å¼
docker run -d --rm --privileged -p 8080:8080 code_sandbox:server

# éªŒè¯æœåŠ¡
curl http://localhost:8080/health
```

#### Step 2: å®‰è£…ä¾èµ–

```bash
# verl ä¾èµ–
pip install -e verl/
pip install vllm  # æˆ– sglang

# SandboxFusion SDK
pip install sandbox-fusion

# å…¶ä»–
pip install wandb datasets transformers aiohttp
```

#### Step 3: verl é…ç½®æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œç”¨äº Hydra é›†æˆï¼‰

å¦‚æœä½¿ç”¨ Hydra é…ç½®ç³»ç»Ÿï¼Œå¯ä»¥åˆ›å»ºé…ç½®æ–‡ä»¶ï¼š

```yaml
# config/phase0_eval.yaml

# Ray é…ç½®
trainer:
  n_gpus_per_node: 8
  nnodes: 1

# æ¨¡å‹é…ç½®
actor_rollout_ref:
  model:
    path: "Qwen/Qwen2.5-Coder-7B-Instruct"
    trust_remote_code: true
    load_tokenizer: true
    lora_rank: 0

  # Rollout é…ç½®ï¼ˆä½¿ç”¨ vLLM Standalone æ¨¡å¼ï¼‰
  rollout:
    name: "vllm"  # æˆ– "sglang"
    mode: "async"
    tensor_model_parallel_size: 2
    data_parallel_size: 1
    pipeline_model_parallel_size: 1

    # EVAL@1 åè®®ï¼šgreedy decoding
    temperature: 0.0
    top_p: 1.0
    top_k: -1
    do_sample: false
    n: 1

    # è¾“å‡ºé•¿åº¦
    prompt_length: 4096
    response_length: 2048

    # å¼•æ“é…ç½®
    dtype: "bfloat16"
    gpu_memory_utilization: 0.8
    load_format: "auto"  # Standalone æ¨¡å¼å¿…é¡»ä½¿ç”¨ "auto"
    enforce_eager: true
    enable_prefix_caching: true
    enable_chunked_prefill: true

    # æ‰¹å¤„ç†
    max_num_seqs: 256
    max_num_batched_tokens: 8192

# æ•°æ®é…ç½®
data:
  eval_files:
    - "data/eval/codecontests_valid.parquet"
    - "data/eval/codecontests_test.parquet"
    - "data/eval/humaneval.parquet"
    - "data/eval/mbpp_reg.parquet"

# SandboxFusion é…ç½®
sandbox:
  url: "http://localhost:8080/run_code"
  timeout: 10
  memory_limit_mb: 1024
```

> **âš ï¸ å…³é”®é…ç½®ï¼š`load_format` å‚æ•°**
>
> | æ¨¡å¼ | `load_format` | è¯´æ˜ |
> |------|---------------|------|
> | **STANDALONE** (Phase 0 è¯„æµ‹) | `"auto"` | ä»ç£ç›˜/HDFS åŠ è½½**çœŸå®**æ¨¡å‹æƒé‡ |
> | **HYBRID** (GRPO/PPO è®­ç»ƒ) | `"dummy"` | åˆ›å»ºç©ºå£³æ¨¡å‹ï¼Œç”±è®­ç»ƒå¼•æ“åŒæ­¥æƒé‡ |
>
> **å¦‚æœ Phase 0 ä½¿ç”¨ `load_format: "dummy"`ï¼Œæ¨¡å‹æƒé‡å°†ä¸ä¼šè¢«åŠ è½½ï¼Œæ‰€æœ‰ç”Ÿæˆç»“æœéƒ½æ˜¯éšæœºçš„ï¼** è¿™æ˜¯æœ€å¸¸è§çš„é…ç½®é”™è¯¯ä¹‹ä¸€ã€‚è¯¦è§ [verl_standalone_rollout_guide.md](./verl_standalone_rollout_guide.md) ç¬¬ä¸ƒç« ã€‚

### 5.2 æ•°æ®å‡†å¤‡

#### Step 1: ä¸‹è½½æ•°æ®é›†

```python
# scripts/download_datasets.py
from datasets import load_dataset

# CodeContests
codecontests = load_dataset("deepmind/code_contests")
codecontests.save_to_disk("data/codecontests")

# HumanEval
humaneval = load_dataset("openai_humaneval")
humaneval.save_to_disk("data/humaneval")

# MBPP
mbpp = load_dataset("mbpp")
mbpp.save_to_disk("data/mbpp")
```

#### Step 2: è¿è¡Œæ•°æ®æ²»ç†

```bash
python scripts/data_governance.py
# è¾“å‡º:
#   - data_manifests/codecontests_train.jsonl
#   - data_manifests/codecontests_valid.jsonl
#   - data_manifests/codecontests_test.jsonl
#   - data_manifests/humaneval.jsonl
#   - data_manifests/mbpp_reg.jsonl
#   - data_manifests/audit_report.md
```

#### Step 3: è½¬æ¢ä¸º verl æ•°æ®æ ¼å¼

```python
# ä¼ªä»£ç ï¼šscripts/prepare_eval_data.py

def convert_codecontests_to_verl_format(dataset, split: str) -> List[Dict]:
    """è½¬æ¢ CodeContests ä¸º verl è¯„æµ‹æ ¼å¼"""
    records = []
    for item in dataset[split]:
        # CodeContests ä½¿ç”¨ stdin/stdout æ ¼å¼
        test_cases = {
            "inputs": item['public_tests']['input'] + item['private_tests']['input'],
            "outputs": item['public_tests']['output'] + item['private_tests']['output'],
        }

        records.append({
            "prompt": item['description'],
            "data_source": "codecontests",
            "ground_truth": json.dumps(test_cases),
            "problem_id": item['name'],
            "difficulty": item.get('difficulty', 'unknown'),
        })
    return records

def convert_humaneval_to_verl_format(dataset) -> List[Dict]:
    """è½¬æ¢ HumanEval ä¸º verl è¯„æµ‹æ ¼å¼"""
    records = []
    for item in dataset['test']:
        # HumanEval ä½¿ç”¨å‡½æ•°è°ƒç”¨ + assert æ ¼å¼
        records.append({
            "prompt": item['prompt'],
            "data_source": "humaneval",
            "ground_truth": json.dumps({
                "test": item['test'],
                "entry_point": item['entry_point'],
            }),
            "problem_id": f"HumanEval/{item['task_id']}",
        })
    return records

# ä¿å­˜ä¸º parquet
import pandas as pd
pd.DataFrame(records).to_parquet("data/eval/codecontests_valid.parquet")
```

### 5.3 è¯„æµ‹è„šæœ¬æ¡†æ¶ï¼ˆä½¿ç”¨ verl åˆ†å¸ƒå¼æ¨ç†ï¼‰

**é‡è¦**: Phase 0 è¯„æµ‹è„šæœ¬ä½¿ç”¨ verl çš„ **Standalone Rollout æ¨¡å¼**ï¼Œé€šè¿‡ vLLM/SGLang å¼•æ“è¿›è¡Œåˆ†å¸ƒå¼æ¨ç†ï¼Œè€Œéçº¯ HuggingFaceã€‚

```python
# scripts/phase0_baseline_eval.py

"""
Phase 0 Baseline è¯„æµ‹è„šæœ¬ - ä½¿ç”¨ verl åˆ†å¸ƒå¼æ¨ç†æ¶æ„

æ ¸å¿ƒç»„ä»¶ï¼š
- verl RolloutReplica: ç®¡ç† vLLM/SGLang æ¨ç†æœåŠ¡å™¨
- Ray: åˆ†å¸ƒå¼åè°ƒ
- OpenAI-compatible API: ç»Ÿä¸€çš„ç”Ÿæˆæ¥å£
- SandboxFusion compute_score: ä»£ç è¯„æµ‹

è¿è¡Œï¼š
python scripts/phase0_baseline_eval.py \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --rollout vllm \
    --tensor_parallel_size 2 \
    --sandbox_url http://localhost:8080/run_code \
    --output_dir outputs/phase0
"""

import asyncio
import argparse
import json
import os
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional
from pathlib import Path

import aiohttp
import numpy as np
import pandas as pd
import ray
import wandb
from tqdm.asyncio import tqdm_asyncio

# verl æ ¸å¿ƒç»„ä»¶
from verl.workers.rollout.replica import get_rollout_replica_class
from verl.utils.reward_score.sandbox_fusion import compute_score

# =============================================================================
# é…ç½®ç±»
# =============================================================================

@dataclass
class EvalConfig:
    """è¯„æµ‹é…ç½®"""
    # æ¨¡å‹é…ç½®
    model_path: str = "Qwen/Qwen2.5-Coder-7B-Instruct"

    # verl Rollout é…ç½®
    rollout_name: str = "vllm"  # "vllm" æˆ– "sglang"
    tensor_parallel_size: int = 2
    n_gpus_per_node: int = 8
    gpu_memory_utilization: float = 0.8

    # è§£ç å‚æ•° (EVAL@1 åè®®)
    temperature: float = 0.0  # greedy decoding
    top_p: float = 1.0
    max_new_tokens: int = 2048

    # SandboxFusion é…ç½®
    sandbox_url: str = "http://localhost:8080/run_code"
    memory_limit_mb: int = 1024
    timeout: int = 10

    # è¯„æµ‹æ–¹å¼é€‰æ‹©
    use_submit_api: bool = True  # True: ä½¿ç”¨ submit() API (æ¨è)
                                  # False: ä½¿ç”¨ compute_score() (ä¸ GRPO ä¸€è‡´)

    # å¹¶å‘æ§åˆ¶
    max_concurrent_requests: int = 64

    # è¾“å‡º
    output_dir: str = "outputs/phase0"
    wandb_project: str = "rlvr_coding_model"
    wandb_run_name: str = "phase0_baseline"

@dataclass
class EvalResult:
    """å•ä¸ªé—®é¢˜çš„è¯„æµ‹ç»“æœ"""
    problem_id: str
    prompt: str
    completion: str

    # è´¨é‡æŒ‡æ ‡
    pass_ratio: float = 0.0
    accepted: bool = False
    final_status: str = "unknown"

    # æˆæœ¬æŒ‡æ ‡
    output_tokens: int = 0
    gen_time: float = 0.0
    judge_time: float = 0.0

    # è¯¦ç»†ç»“æœ
    metadata: List = field(default_factory=list)

# =============================================================================
# verl Rollout Server ç®¡ç†
# =============================================================================

async def start_rollout_servers(config: EvalConfig):
    """
    å¯åŠ¨ verl Standalone Rollout æœåŠ¡å™¨

    å…³é”®æ­¥éª¤ï¼š
    1. è·å– RolloutReplica ç±»ï¼ˆvLLMReplica æˆ– SGLangReplicaï¼‰
    2. æ ¹æ® GPU é…ç½®è®¡ç®— replica æ•°é‡
    3. è°ƒç”¨ init_standalone() åˆå§‹åŒ–ç‹¬ç«‹æœåŠ¡å™¨
    4. è¿”å›æœåŠ¡å™¨åœ°å€åˆ—è¡¨
    """
    from omegaconf import OmegaConf

    # æ„å»º RolloutConfig
    rollout_config = OmegaConf.create({
        "name": config.rollout_name,
        "mode": "async",
        "tensor_model_parallel_size": config.tensor_parallel_size,
        "data_parallel_size": 1,
        "pipeline_model_parallel_size": 1,
        "temperature": config.temperature,
        "top_p": config.top_p,
        "response_length": config.max_new_tokens,
        "prompt_length": 4096,
        "dtype": "bfloat16",
        "gpu_memory_utilization": config.gpu_memory_utilization,
        "load_format": "auto",  # Standalone æ¨¡å¼ä½¿ç”¨ "auto" åŠ è½½çœŸå®æƒé‡
        "enforce_eager": True,
        "enable_prefix_caching": True,
        "enable_chunked_prefill": True,
        "max_num_seqs": 256,
        "max_num_batched_tokens": 8192,
        "disable_log_stats": True,
    })

    # æ„å»º HFModelConfig
    model_config = OmegaConf.create({
        "path": config.model_path,
        "trust_remote_code": True,
        "load_tokenizer": True,
        "lora_rank": 0,
    })

    # è®¡ç®— replica æ•°é‡
    num_replicas = config.n_gpus_per_node // config.tensor_parallel_size

    # è·å– Rollout ç±»å¹¶åˆ›å»ºå®ä¾‹
    rollout_class = get_rollout_replica_class(config.rollout_name)

    rollout_servers = [
        rollout_class(
            replica_rank=replica_rank,
            config=rollout_config,
            model_config=model_config,
            gpus_per_node=config.n_gpus_per_node,
        )
        for replica_rank in range(num_replicas)
    ]

    # åˆå§‹åŒ– Standalone æ¨¡å¼æœåŠ¡å™¨
    print(f"Initializing {num_replicas} {config.rollout_name} rollout servers...")
    await asyncio.gather(*[server.init_standalone() for server in rollout_servers])

    # è·å–æœåŠ¡å™¨åœ°å€
    server_addresses = [server._server_address for server in rollout_servers]
    print(f"Rollout servers ready at: {server_addresses}")

    return rollout_servers, server_addresses

# =============================================================================
# ä»£ç ç”Ÿæˆï¼ˆé€šè¿‡ OpenAI-compatible APIï¼‰
# =============================================================================

async def generate_code(
    session: aiohttp.ClientSession,
    server_address: str,
    model_path: str,
    prompt: str,
    sampling_params: dict,
    semaphore: asyncio.Semaphore,
) -> tuple[str, float]:
    """
    é€šè¿‡ OpenAI API è°ƒç”¨ vLLM/SGLang ç”Ÿæˆä»£ç 

    è¿”å›: (completion, generation_time)
    """
    async with semaphore:
        start_time = time.time()

        try:
            async with session.post(
                url=f"http://{server_address}/v1/chat/completions",
                headers={"Authorization": "Bearer token-abc123"},
                json={
                    "model": model_path,
                    "messages": [{"role": "user", "content": prompt}],
                    **sampling_params
                },
                timeout=aiohttp.ClientTimeout(total=300),
            ) as resp:
                data = await resp.json()
                completion = data["choices"][0]["message"]["content"]
                gen_time = time.time() - start_time
                return completion, gen_time

        except Exception as e:
            print(f"Generation error: {e}")
            return "", time.time() - start_time

async def batch_generate(
    server_addresses: List[str],
    model_path: str,
    prompts: List[str],
    sampling_params: dict,
    max_concurrent: int = 64,
) -> List[tuple[str, float]]:
    """
    æ‰¹é‡ç”Ÿæˆä»£ç ï¼Œè´Ÿè½½å‡è¡¡åˆ°å¤šä¸ª replica
    """
    semaphore = asyncio.Semaphore(max_concurrent)

    async with aiohttp.ClientSession() as session:
        tasks = []
        for i, prompt in enumerate(prompts):
            # Round-robin è´Ÿè½½å‡è¡¡
            server_idx = i % len(server_addresses)
            server_address = server_addresses[server_idx]

            task = generate_code(
                session, server_address, model_path, prompt,
                sampling_params, semaphore
            )
            tasks.append(task)

        # å¸¦è¿›åº¦æ¡çš„å¹¶å‘æ‰§è¡Œ
        results = await tqdm_asyncio.gather(*tasks, desc="Generating")

    return results

# =============================================================================
# ä»£ç è¯„æµ‹ï¼ˆä½¿ç”¨ SandboxFusionï¼‰
# =============================================================================

# æ–¹å¼ Aï¼šä½¿ç”¨ submit() APIï¼ˆæ¨èï¼Œæ›´ç®€å•ï¼‰
def evaluate_code_with_submit(
    completion: str,
    sandbox_dataset: str,
    sandbox_id: str,
    config: EvalConfig,
) -> tuple[float, str, float]:
    """
    ä½¿ç”¨ SandboxFusion submit() API è¯„æµ‹ä»£ç 

    å‚æ•°:
        completion: æ¨¡å‹ç”Ÿæˆçš„ä»£ç 
        sandbox_dataset: SandboxFusion æ•°æ®é›†å (e.g., "humaneval")
        sandbox_id: é—®é¢˜ ID

    è¿”å›: (pass_ratio, final_status, judge_time)
    """
    from sandbox_fusion import submit, SubmitRequest, TestConfig

    start_time = time.time()

    if not completion or not completion.strip():
        return 0.0, "empty_output", time.time() - start_time

    try:
        result = submit(SubmitRequest(
            dataset=sandbox_dataset,
            id=sandbox_id,
            completion=completion,
            config=TestConfig(language='python', run_timeout=config.timeout)
        ))

        judge_time = time.time() - start_time

        if result.accepted:
            return 1.0, "success", judge_time
        else:
            # ä» result.tests æ¨æ–­é”™è¯¯ç±»å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            return 0.0, "wrong_answer", judge_time

    except Exception as e:
        print(f"Evaluation error: {e}")
        return 0.0, "api_error", time.time() - start_time

# æ–¹å¼ Bï¼šä½¿ç”¨ compute_score()ï¼ˆä¸ GRPO ä¸€è‡´ï¼‰
def evaluate_code_with_compute_score(
    completion: str,
    test_cases: Dict,
    config: EvalConfig,
) -> tuple[float, str, List, float]:
    """
    ä½¿ç”¨ verl compute_score() è¯„æµ‹ä»£ç 

    è¿”å›: (pass_ratio, final_status, metadata, judge_time)
    """
    start_time = time.time()

    if not completion or not completion.strip():
        return 0.0, "empty_output", [], time.time() - start_time

    try:
        score, metadata_list = compute_score(
            sandbox_fusion_url=config.sandbox_url,
            memory_limit_mb=config.memory_limit_mb,
            completion=completion,
            test_cases=test_cases,
            continuous=False,
            timeout=config.timeout,
        )

        judge_time = time.time() - start_time

        if not metadata_list:
            final_status = "api_error"
        elif score == 1.0:
            final_status = "success"
        else:
            statuses = [m.get('status', '') for m in metadata_list]
            if any('compile' in s for s in statuses):
                final_status = "syntax_error"
            elif any('runtime' in s for s in statuses):
                final_status = "runtime_error"
            elif any('timeout' in s for s in statuses):
                final_status = "timeout"
            else:
                final_status = "wrong_answer"

        return score, final_status, metadata_list, judge_time

    except Exception as e:
        print(f"Evaluation error: {e}")
        return 0.0, "api_error", [], time.time() - start_time

# ç»Ÿä¸€æ¥å£ï¼šæ ¹æ®é…ç½®é€‰æ‹©è¯„æµ‹æ–¹å¼
def evaluate_code(
    completion: str,
    record: Dict,
    config: EvalConfig,
) -> tuple[float, str, List, float]:
    """
    è¯„æµ‹ä»£ç  - æ ¹æ®é…ç½®é€‰æ‹© submit() æˆ– compute_score()

    å‚æ•°:
        record: é—®é¢˜è®°å½•ï¼ŒåŒ…å« sandbox_dataset/sandbox_id æˆ– ground_truth
    """
    if config.use_submit_api and 'sandbox_dataset' in record:
        # ä½¿ç”¨ submit() API
        pass_ratio, final_status, judge_time = evaluate_code_with_submit(
            completion,
            record['sandbox_dataset'],
            record['sandbox_id'],
            config
        )
        return pass_ratio, final_status, [], judge_time
    else:
        # ä½¿ç”¨ compute_score()
        test_cases = json.loads(record.get('ground_truth', '{}'))
        return evaluate_code_with_compute_score(completion, test_cases, config)

# =============================================================================
# æŒ‡æ ‡èšåˆ
# =============================================================================

def aggregate_metrics(results: List[EvalResult]) -> Dict:
    """èšåˆè¯„æµ‹æŒ‡æ ‡"""
    n = len(results)
    if n == 0:
        return {}

    pass_ratios = [r.pass_ratio for r in results]

    # è´¨é‡æŒ‡æ ‡
    metrics = {
        "accepted_at_1": sum(r.accepted for r in results) / n,
        "pass_ratio_mean": np.mean(pass_ratios),
        "pass_ratio_p50": np.percentile(pass_ratios, 50),
        "pass_ratio_p90": np.percentile(pass_ratios, 90),
        "exec_success_rate": sum(
            r.final_status in ['success', 'wrong_answer'] for r in results
        ) / n,
    }

    # é”™è¯¯åˆ†å¸ƒ
    for status in ['syntax_error', 'runtime_error', 'timeout', 'wrong_answer', 'empty_output']:
        count = sum(r.final_status == status for r in results)
        metrics[f"{status}_rate"] = count / n

    # æˆæœ¬æŒ‡æ ‡
    metrics["avg_gen_tokens"] = np.mean([r.output_tokens for r in results])
    metrics["avg_gen_time"] = np.mean([r.gen_time for r in results])
    metrics["avg_judge_time"] = np.mean([r.judge_time for r in results])

    solved = [r for r in results if r.accepted]
    if solved:
        metrics["cost_per_solved_tokens"] = sum(r.output_tokens for r in solved) / len(solved)
        metrics["cost_per_solved_time"] = sum(r.gen_time + r.judge_time for r in solved) / len(solved)

    return metrics

def sample_qa_logs(results: List[EvalResult], num_samples: int) -> List[Dict]:
    """åˆ†å±‚æŠ½æ · QA æ—¥å¿—"""
    by_status = {}
    for r in results:
        if r.final_status not in by_status:
            by_status[r.final_status] = []
        by_status[r.final_status].append(r)

    samples = []
    samples_per_status = max(1, num_samples // max(len(by_status), 1))

    for status, group in by_status.items():
        for r in group[:samples_per_status]:
            samples.append({
                "problem_id": r.problem_id,
                "prompt": r.prompt[:500],
                "response": r.completion[:1000],
                "pass_ratio": r.pass_ratio,
                "final_status": r.final_status,
                "output_tokens": r.output_tokens,
                "gen_time": r.gen_time,
                "judge_time": r.judge_time,
            })

    return samples[:num_samples]

# =============================================================================
# ä¸»å‡½æ•°
# =============================================================================

async def evaluate_dataset(
    dataset_name: str,
    data_path: str,
    server_addresses: List[str],
    config: EvalConfig,
    log_samples: int,
) -> tuple[Dict, List[Dict]]:
    """è¯„æµ‹å•ä¸ªæ•°æ®é›†"""
    print(f"\n{'='*60}")
    print(f"Evaluating {dataset_name}...")
    print(f"{'='*60}")

    # åŠ è½½æ•°æ®
    df = pd.read_parquet(data_path)
    prompts = df['prompt'].tolist()

    # æ‰¹é‡ç”Ÿæˆ
    sampling_params = {
        "temperature": config.temperature,
        "top_p": config.top_p,
        "max_tokens": config.max_new_tokens,
    }

    gen_results = await batch_generate(
        server_addresses, config.model_path, prompts,
        sampling_params, config.max_concurrent_requests
    )

    # è¯„æµ‹æ¯ä¸ªç»“æœ
    results = []
    for idx, ((completion, gen_time), row) in enumerate(zip(gen_results, df.itertuples())):
        # æ„å»ºè®°å½•å­—å…¸ï¼ˆæ”¯æŒ submit() å’Œ compute_score() ä¸¤ç§æ–¹å¼ï¼‰
        record = {
            'sandbox_dataset': getattr(row, 'sandbox_dataset', None),
            'sandbox_id': getattr(row, 'sandbox_id', None),
            'ground_truth': getattr(row, 'ground_truth', '{}'),
        }
        pass_ratio, final_status, metadata, judge_time = evaluate_code(
            completion, record, config
        )

        result = EvalResult(
            problem_id=getattr(row, 'problem_id', f'problem_{idx}'),
            prompt=row.prompt,
            completion=completion,
            pass_ratio=pass_ratio,
            accepted=(pass_ratio == 1.0),
            final_status=final_status,
            output_tokens=len(completion.split()),  # ç®€åŒ–è®¡ç®—
            gen_time=gen_time,
            judge_time=judge_time,
            metadata=metadata,
        )
        results.append(result)

    # èšåˆæŒ‡æ ‡
    metrics = aggregate_metrics(results)
    metrics["total_problems"] = len(results)

    # æŠ½æ ·æ—¥å¿—
    qa_logs = sample_qa_logs(results, log_samples)

    # æ‰“å°æ‘˜è¦
    print(f"  Total problems: {len(results)}")
    print(f"  accepted@1: {metrics['accepted_at_1']:.2%}")
    print(f"  pass_ratio_mean: {metrics['pass_ratio_mean']:.3f}")
    print(f"  exec_success_rate: {metrics['exec_success_rate']:.2%}")

    return metrics, qa_logs

async def main_async(config: EvalConfig):
    """å¼‚æ­¥ä¸»å‡½æ•°"""

    # åˆå§‹åŒ– Ray
    ray.init(
        runtime_env={
            "env_vars": {
                "TOKENIZERS_PARALLELISM": "true",
                "NCCL_DEBUG": "WARN",
                "VLLM_USE_V1": "1",
            }
        }
    )

    try:
        # å¯åŠ¨ Rollout æœåŠ¡å™¨
        rollout_servers, server_addresses = await start_rollout_servers(config)

        # åˆå§‹åŒ– WandB
        wandb.init(
            project=config.wandb_project,
            name=config.wandb_run_name,
            config=vars(config),
        )

        # è¯„æµ‹æ•°æ®é›†åˆ—è¡¨
        datasets_to_eval = [
            ("codecontests_valid", "data/eval/codecontests_valid.parquet", 50),
            ("codecontests_test", "data/eval/codecontests_test.parquet", 30),
            ("humaneval", "data/eval/humaneval.parquet", 20),
            ("mbpp_reg", "data/eval/mbpp_reg.parquet", 20),
        ]

        all_metrics = {}
        output_dir = Path(config.output_dir)

        for dataset_name, data_path, log_samples in datasets_to_eval:
            if not Path(data_path).exists():
                print(f"Warning: {data_path} not found, skipping...")
                continue

            metrics, qa_logs = await evaluate_dataset(
                dataset_name, data_path, server_addresses,
                config, log_samples
            )

            # è®°å½•åˆ° WandB
            for k, v in metrics.items():
                wandb.log({f"eval/{dataset_name}/{k}": v})

            all_metrics[dataset_name] = metrics

            # ä¿å­˜ QA æ—¥å¿—
            log_path = output_dir / "qa_logs" / f"{dataset_name}.jsonl"
            log_path.parent.mkdir(parents=True, exist_ok=True)
            with open(log_path, 'w') as f:
                for log in qa_logs:
                    f.write(json.dumps(log, ensure_ascii=False) + '\n')

        # ä¿å­˜æ±‡æ€»
        summary_path = output_dir / "phase0_summary.json"
        summary_path.parent.mkdir(parents=True, exist_ok=True)
        with open(summary_path, 'w') as f:
            json.dump(all_metrics, f, indent=2, ensure_ascii=False)

        wandb.finish()
        print(f"\n{'='*60}")
        print(f"Results saved to {config.output_dir}")
        print(f"{'='*60}")

    finally:
        ray.shutdown()

def main():
    parser = argparse.ArgumentParser(description="Phase 0 Baseline Evaluation with verl")
    parser.add_argument("--model", default="Qwen/Qwen2.5-Coder-7B-Instruct")
    parser.add_argument("--rollout", default="vllm", choices=["vllm", "sglang"])
    parser.add_argument("--tensor_parallel_size", type=int, default=2)
    parser.add_argument("--n_gpus", type=int, default=8)
    parser.add_argument("--sandbox_url", default="http://localhost:8080/run_code")
    parser.add_argument("--output_dir", default="outputs/phase0")
    parser.add_argument("--use-submit-api", action="store_true", default=True,
                        help="ä½¿ç”¨ SandboxFusion submit() API è¯„æµ‹ï¼ˆæ¨èï¼‰")
    parser.add_argument("--use-compute-score", dest="use_submit_api", action="store_false",
                        help="ä½¿ç”¨ verl compute_score() è¯„æµ‹ï¼ˆä¸ GRPO ä¸€è‡´ï¼‰")
    args = parser.parse_args()

    config = EvalConfig(
        model_path=args.model,
        rollout_name=args.rollout,
        tensor_parallel_size=args.tensor_parallel_size,
        n_gpus_per_node=args.n_gpus,
        sandbox_url=args.sandbox_url,
        output_dir=args.output_dir,
        use_submit_api=args.use_submit_api,
    )

    asyncio.run(main_async(config))

if __name__ == "__main__":
    main()
```

### 5.4 è¿è¡Œå‘½ä»¤ç¤ºä¾‹

```bash
# å•æœº 8 GPUï¼Œä½¿ç”¨ vLLMï¼ŒTP=2ï¼ˆ4 ä¸ª replicaï¼‰
python scripts/phase0_baseline_eval.py \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --rollout vllm \
    --tensor_parallel_size 2 \
    --n_gpus 8 \
    --sandbox_url http://localhost:8080/run_code \
    --output_dir outputs/phase0

# å•æœº 4 GPUï¼Œä½¿ç”¨ SGLangï¼ŒTP=2ï¼ˆ2 ä¸ª replicaï¼‰
python scripts/phase0_baseline_eval.py \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --rollout sglang \
    --tensor_parallel_size 2 \
    --n_gpus 4 \
    --sandbox_url http://localhost:8080/run_code \
    --output_dir outputs/phase0

# å¿«é€Ÿæµ‹è¯•ï¼ˆ1 GPUï¼‰
python scripts/phase0_baseline_eval.py \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --rollout vllm \
    --tensor_parallel_size 1 \
    --n_gpus 1 \
    --output_dir outputs/phase0_test
```

---

## å…­ã€æŒ‡æ ‡æ”¶é›†è¯¦è§£

### 6.1 è´¨é‡æŒ‡æ ‡æ”¶é›†

| æŒ‡æ ‡ | è®¡ç®—å…¬å¼ | æ•°æ®æ¥æº |
|------|----------|---------|
| accepted@1 | `sum(pass_ratio == 1.0) / total` | compute_score è¿”å›å€¼ |
| pass_ratio_mean | `mean(pass_ratios)` | compute_score è¿”å›å€¼ |
| pass_ratio_p50 | `median(pass_ratios)` | compute_score è¿”å›å€¼ |
| pass_ratio_p90 | `percentile(pass_ratios, 90)` | compute_score è¿”å›å€¼ |
| exec_success_rate | `sum(final_status in ['success', 'wrong_answer']) / total` | metadata çš„ status å­—æ®µ |

### 6.2 é”™è¯¯åˆ†å¸ƒæŒ‡æ ‡æ”¶é›†

| æŒ‡æ ‡ | å¯¹åº” status | åˆ¤æ–­é€»è¾‘ |
|------|-------------|---------|
| syntax_error_rate | compile_error | `result == -4` |
| runtime_error_rate | runtime_error | `result == -2` |
| timeout_rate | timeout | `result == -3` |
| wrong_answer_rate | wrong_answer | `result == False` |

```python
# ä» metadata ä¸­æå–é”™è¯¯ç±»å‹
def extract_error_distribution(metadata_list):
    """æå–é”™è¯¯åˆ†å¸ƒ"""
    counts = {
        'success': 0,
        'wrong_answer': 0,
        'syntax_error': 0,
        'runtime_error': 0,
        'timeout': 0,
        'api_error': 0,
    }

    for m in metadata_list:
        status = m.get('status', 'unknown')
        if status == 'success':
            counts['success'] += 1
        elif status == 'wrong_answer':
            counts['wrong_answer'] += 1
        elif status in ['compile_error', 'compile_timeout']:
            counts['syntax_error'] += 1
        elif status == 'runtime_error':
            counts['runtime_error'] += 1
        elif status == 'timeout':
            counts['timeout'] += 1
        else:
            counts['api_error'] += 1

    total = len(metadata_list)
    return {k: v/total for k, v in counts.items()}
```

### 6.3 æˆæœ¬æŒ‡æ ‡æ”¶é›†

| æŒ‡æ ‡ | è®¡ç®—å…¬å¼ | æ•°æ®æ¥æº |
|------|----------|---------|
| avg_total_gen_tokens | `mean(output_tokens per problem)` | tokenizer.encode(completion) |
| avg_total_judge_time | `mean(sum(duration per case) per problem)` | metadata çš„ duration å­—æ®µ |
| p95_total_judge_time | `percentile(total_judge_times, 95)` | metadata çš„ duration å­—æ®µ |
| throughput | `total_problems / wall_clock_time` | ç«¯åˆ°ç«¯è®¡æ—¶ |
| cost_per_solved_tokens | `sum(output_tokens) / solved_count` | ä»…å¯¹ accepted çš„é—®é¢˜ |
| cost_per_solved_judge_time | `sum(judge_time) / solved_count` | ä»…å¯¹ accepted çš„é—®é¢˜ |

```python
def compute_cost_metrics(results: List[EvalResult]) -> Dict:
    """è®¡ç®—æˆæœ¬æŒ‡æ ‡"""
    total_tokens = sum(r.output_tokens for r in results)
    total_judge_time = sum(r.total_judge_time for r in results)
    solved_count = sum(r.accepted for r in results)

    metrics = {
        'avg_total_gen_tokens': total_tokens / len(results),
        'avg_total_judge_time': total_judge_time / len(results),
        'p95_total_judge_time': np.percentile(
            [r.total_judge_time for r in results], 95
        ),
    }

    if solved_count > 0:
        # åªè®¡ç®— solved çš„é—®é¢˜
        solved_tokens = sum(r.output_tokens for r in results if r.accepted)
        solved_time = sum(r.total_judge_time for r in results if r.accepted)

        metrics['cost_per_solved_tokens'] = solved_tokens / solved_count
        metrics['cost_per_solved_judge_time'] = solved_time / solved_count

    return metrics
```

### 6.4 WandB æ—¥å¿—æ ¼å¼

```python
# è®°å½•åˆ° WandB
wandb.log({
    # æŒ‰æ•°æ®é›†åˆ†åˆ«è®°å½•
    "eval/codecontests_valid/accepted_at_1": 0.15,
    "eval/codecontests_valid/pass_ratio_mean": 0.25,
    "eval/codecontests_valid/pass_ratio_p50": 0.20,
    "eval/codecontests_valid/pass_ratio_p90": 0.45,
    "eval/codecontests_valid/exec_success_rate": 0.70,
    "eval/codecontests_valid/syntax_error_rate": 0.10,
    "eval/codecontests_valid/runtime_error_rate": 0.08,
    "eval/codecontests_valid/timeout_rate": 0.12,
    "eval/codecontests_valid/wrong_answer_rate": 0.55,
    "eval/codecontests_valid/avg_total_gen_tokens": 450,
    "eval/codecontests_valid/avg_total_judge_time": 2.5,
    "eval/codecontests_valid/cost_per_solved_tokens": 500,
    "eval/codecontests_valid/cost_per_solved_judge_time": 3.0,
    "eval/codecontests_valid/throughput": 5.0,

    # åŒæ ·æ ¼å¼è®°å½•å…¶ä»–æ•°æ®é›†...
})
```

---

## ä¸ƒã€é—®ç­”æ—¥å¿—æ ¼å¼è§„èŒƒ

### 7.1 æ—¥å¿—æ–‡ä»¶ç»“æ„

```
outputs/phase0/
â”œâ”€â”€ qa_logs/
â”‚   â”œâ”€â”€ codecontests_valid_50.jsonl
â”‚   â”œâ”€â”€ codecontests_test_30.jsonl
â”‚   â”œâ”€â”€ humaneval_20.jsonl
â”‚   â””â”€â”€ mbpp_reg_20.jsonl
â”œâ”€â”€ metrics/
â”‚   â””â”€â”€ phase0_summary.json
â””â”€â”€ audit/
    â””â”€â”€ data_governance_report.md
```

### 7.2 å•æ¡æ—¥å¿—æ ¼å¼

```json
{
    "problem_id": "cc_valid_042",
    "dataset": "codecontests_valid",
    "prompt": "Given an array of integers, find the maximum sum of a contiguous subarray...",
    "response": "```python\ndef max_subarray_sum(arr):\n    max_sum = float('-inf')\n    current_sum = 0\n    for num in arr:\n        current_sum = max(num, current_sum + num)\n        max_sum = max(max_sum, current_sum)\n    return max_sum\n\nn = int(input())\narr = list(map(int, input().split()))\nprint(max_subarray_sum(arr))\n```",
    "ground_truth": {
        "inputs": ["5\n-2 1 -3 4 -1 2 1 -5 4", ...],
        "outputs": ["6", ...]
    },
    "pass_ratio": 0.8,
    "accepted": false,
    "final_status": "wrong_answer",
    "output_tokens": 156,
    "total_judge_time": 1.23,
    "error_breakdown": {
        "passed": 8,
        "wrong_answer": 2,
        "timeout": 0,
        "runtime_error": 0,
        "syntax_error": 0
    },
    "execution_output": {
        "first_failed_case": {
            "input": "10\n1 2 3 -10 5 6 7 -8 9 10",
            "expected": "29",
            "actual": "28",
            "stderr": ""
        }
    },
    "metadata": {
        "model": "Qwen2.5-Coder-7B-Instruct",
        "temperature": 0.0,
        "max_new_tokens": 2048,
        "timestamp": "2024-01-31T10:30:00Z"
    }
}
```

### 7.3 åˆ†å±‚æŠ½æ ·ç­–ç•¥

æ ¹æ®å®éªŒè®¾è®¡ï¼Œå»ºè®®æŒ‰ error_type åˆ†å±‚æŠ½æ ·ï¼š

| æ•°æ®é›† | æ€»é‡‡æ ·æ•° | æŠ½æ ·ç­–ç•¥ |
|--------|---------|---------|
| CodeContests_valid | 50 æ¡ | success: 10, WA: 20, Syntax: 10, Runtime: 5, Timeout: 5 |
| CodeContests_test | 30 æ¡ | success: 6, WA: 12, Syntax: 6, Runtime: 3, Timeout: 3 |
| HumanEval | 20 æ¡ | success: 4, WA: 8, Syntax: 4, Runtime: 2, Timeout: 2 |
| MBPP_reg | 20 æ¡ | success: 4, WA: 8, Syntax: 4, Runtime: 2, Timeout: 2 |

---

## å…«ã€é¢„æœŸäº§å‡ºä¸éªŒæ”¶æ ‡å‡†

### 8.1 å¿…é¡»äº§å‡ºæ¸…å•

| äº§å‡º | æ–‡ä»¶è·¯å¾„ | éªŒæ”¶æ ‡å‡† |
|------|----------|---------|
| **æ•°æ® Manifest** | `data_manifests/*.jsonl` | æ¯ä¸ª split æœ‰ç‹¬ç«‹ manifest |
| **å®¡è®¡æŠ¥å‘Š** | `data_manifests/audit_report.md` | æ— è·¨ split é‡å ï¼Œæ— å¤–éƒ¨æ³„æ¼ |
| **è¯„æµ‹æŒ‡æ ‡** | `outputs/phase0/phase0_summary.json` | 4 ä¸ªæ•°æ®é›†å…¨éƒ¨å®Œæˆ |
| **é—®ç­”æ—¥å¿—** | `outputs/phase0/qa_logs/*.jsonl` | æ€»è®¡ 120 æ¡ |
| **WandB é¢æ¿** | åœ¨çº¿ | æ‰€æœ‰æŒ‡æ ‡å·²è®°å½• |

### 8.2 æŒ‡æ ‡éªŒæ”¶æ ‡å‡†

Phase 0 æ˜¯åŸºçº¿è¯„æµ‹ï¼Œä¸é¢„è®¾æ€§èƒ½ç›®æ ‡ï¼Œä½†éœ€è¦éªŒè¯ï¼š

1. **è¯„æµ‹æµæ°´çº¿æ­£å¸¸**ï¼šæ‰€æœ‰æ•°æ®é›†éƒ½èƒ½å®Œæˆè¯„æµ‹ï¼Œæ— ç³»ç»Ÿæ€§å¤±è´¥
2. **æŒ‡æ ‡å®Œæ•´**ï¼šè´¨é‡ã€æˆæœ¬ã€é”™è¯¯åˆ†å¸ƒä¸‰ç±»æŒ‡æ ‡å…¨éƒ¨æ”¶é›†
3. **æ•°æ®éš”ç¦»æ­£ç¡®**ï¼šmanifest å’Œå®¡è®¡æŠ¥å‘Šè¯æ˜æ— æ³„æ¼
4. **æ—¥å¿—å¯ç”¨**ï¼šQA æ—¥å¿—åŒ…å«è¶³å¤Ÿä¿¡æ¯ç”¨äºåç»­åˆ†æ

### 8.3 å…¸å‹åŸºçº¿å€¼å‚è€ƒ

åŸºäºå…¬å¼€ benchmark å’Œç±»ä¼¼é¡¹ç›®ç»éªŒï¼Œ7B Base æ¨¡å‹åœ¨ CodeContests ä¸Šçš„å…¸å‹è¡¨ç°ï¼š

| æŒ‡æ ‡ | å…¸å‹å€¼èŒƒå›´ | è¯´æ˜ |
|------|-----------|------|
| accepted@1 | 3% - 10% | æœªè®­ç»ƒæ¨¡å‹é€šå¸¸è¾ƒä½ |
| pass_ratio_mean | 0.10 - 0.25 | éƒ¨åˆ†æµ‹è¯•ç‚¹èƒ½é€šè¿‡ |
| exec_success_rate | 50% - 80% | èƒ½äº§å‡ºå¯è¿è¡Œä»£ç  |
| syntax_error_rate | 5% - 20% | å¶å°”æœ‰è¯­æ³•é”™è¯¯ |
| timeout_rate | 5% - 15% | ç®—æ³•æ•ˆç‡é—®é¢˜ |

è¿™äº›åªæ˜¯å‚è€ƒï¼Œå®é™…å€¼å¯èƒ½å› æ¨¡å‹å’Œæ•°æ®é›†ä¸åŒè€Œå¼‚ã€‚

---

## ä¹ã€æ—¶é—´çº¿è§„åˆ’

### å»ºè®®æ‰§è¡Œé¡ºåº

| æ­¥éª¤ | å†…å®¹ | é¢„ä¼°æ—¶é—´ | äº§å‡º |
|------|------|---------|------|
| 1 | ç¯å¢ƒæ­å»ºä¸éªŒè¯ | 2-4 å°æ—¶ | SandboxFusion æ­£å¸¸è¿è¡Œ |
| 2 | æ•°æ®ä¸‹è½½ä¸æ²»ç† | 4-6 å°æ—¶ | Manifest + å®¡è®¡æŠ¥å‘Š |
| 3 | è¯„æµ‹è„šæœ¬å¼€å‘ | 6-8 å°æ—¶ | å¯è¿è¡Œçš„è¯„æµ‹è„šæœ¬ |
| 4 | CodeContests_valid è¯„æµ‹ | 2-4 å°æ—¶ | æŒ‡æ ‡ + 50 æ¡æ—¥å¿— |
| 5 | CodeContests_test è¯„æµ‹ | 2-4 å°æ—¶ | æŒ‡æ ‡ + 30 æ¡æ—¥å¿— |
| 6 | HumanEval è¯„æµ‹ | 1-2 å°æ—¶ | æŒ‡æ ‡ + 20 æ¡æ—¥å¿— |
| 7 | MBPP_reg è¯„æµ‹ | 1-2 å°æ—¶ | æŒ‡æ ‡ + 20 æ¡æ—¥å¿— |
| 8 | ç»“æœæ±‡æ€»ä¸æŠ¥å‘Š | 2-4 å°æ—¶ | phase0_summary.json |

**æ€»é¢„ä¼°æ—¶é—´**ï¼š20-34 å°æ—¶

---

## åã€é™„å½•

### A. verl å…³é”®æ–‡ä»¶ç´¢å¼•

> **âš ï¸ æ³¨æ„**ï¼šverl ä»£ç åº“é‡‡ç”¨åŒå±‚ç›®å½•ç»“æ„ï¼Œæ ¸å¿ƒä»£ç ä½äº `verl/verl/` ç›®å½•ä¸‹ã€‚

#### Phase 0 è¯„æµ‹æ ¸å¿ƒæ–‡ä»¶ï¼ˆStandalone Rolloutï¼‰

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|------|----------|------|
| **Rollout Replica åŸºç±»** | `verl/verl/workers/rollout/replica.py` | `RolloutReplica`, `RolloutMode`, `get_rollout_replica_class()` |
| **vLLM Replica å®ç°** | `verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py` | `vLLMReplica`, `vLLMHttpServer` |
| **SGLang Replica å®ç°** | `verl/verl/workers/rollout/sglang_rollout/async_sglang_server.py` | `SGLangReplica` |
| **vLLM Rollout Worker** | `verl/verl/workers/rollout/vllm_rollout/vllm_rollout.py` | `vLLMAsyncRollout` |
| **Rollout é…ç½®** | `verl/verl/workers/config/rollout.py` | `RolloutConfig`, `SamplingConfig` |
| **æ¨¡å‹é…ç½®** | `verl/verl/workers/config/model.py` | `HFModelConfig` |
| **å®˜æ–¹ Standalone ç¤ºä¾‹** â˜… | `verl/verl/trainer/main_generation_server.py` | **æ¨èå‚è€ƒ**ï¼šå®Œæ•´çš„ Standalone æ¨¡å¼å®ç° |

#### SandboxFusion é›†æˆæ–‡ä»¶

| åŠŸèƒ½ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|------|----------|------|
| SandboxFusion è¯„åˆ† | `verl/verl/utils/reward_score/sandbox_fusion/__init__.py` | `compute_score()` å‡½æ•° |
| SandboxFusion API è°ƒç”¨ | `verl/verl/utils/reward_score/sandbox_fusion/utils.py` | `check_correctness()`, `call_sandbox_api()` |
| å¥–åŠ±è·¯ç”± | `verl/verl/utils/reward_score/__init__.py` | å¥–åŠ±å‡½æ•°è·¯ç”± |
| å¥–åŠ±ç®¡ç†å™¨ | `verl/verl/workers/reward_manager/naive.py`, `prime.py` | è®­ç»ƒæ—¶ä½¿ç”¨ |

#### å‚è€ƒç¤ºä¾‹

| ç¤ºä¾‹ | æ–‡ä»¶è·¯å¾„ | è¯´æ˜ |
|------|----------|------|
| Standalone æµ‹è¯• | `verl/tests/experimental/agent_loop/test_standalone_rollout.py` | å¦‚ä½•ä½¿ç”¨ `init_standalone()` |
| ç”ŸæˆæœåŠ¡å™¨ â˜… | `verl/verl/trainer/main_generation_server.py` | **é¦–é€‰å‚è€ƒ**ï¼šå®˜æ–¹æ‰¹é‡ç”Ÿæˆç¤ºä¾‹ |
| ç”Ÿæˆè„šæœ¬ | `verl/verl/trainer/main_generation.py` | å¤‡é€‰å‚è€ƒï¼šå¦ä¸€ç§ç”Ÿæˆå®ç° |

### B. SandboxFusion API çŠ¶æ€ç 

| çŠ¶æ€ç  | å«ä¹‰ |
|--------|------|
| True | æµ‹è¯•é€šè¿‡ |
| False | Wrong Answer |
| -1 | API/Sandbox é”™è¯¯ |
| -2 | Runtime Error |
| -3 | Timeout |
| -4 | Compile/Syntax Error |

### C. å¸¸è§é—®é¢˜æ’æŸ¥

1. **æ¨¡å‹è¾“å‡ºéšæœº/æ— æ„ä¹‰ï¼ˆæœ€å¸¸è§é”™è¯¯ï¼‰**
   - **åŸå› **ï¼š`load_format` å‚æ•°é…ç½®é”™è¯¯
   - **è§£å†³**ï¼šç¡®ä¿ Standalone æ¨¡å¼ä½¿ç”¨ `load_format: "auto"`ï¼ˆä¸æ˜¯ `"dummy"`ï¼‰
   - **éªŒè¯**ï¼šæ£€æŸ¥å¯åŠ¨æ—¥å¿—æ˜¯å¦æ˜¾ç¤ºæ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡

2. **SandboxFusion 504 Gateway Timeout**
   - é™ä½ `max_concurrent`
   - å¢åŠ  `timeout` å€¼
   - æ£€æŸ¥æœåŠ¡ç«¯èµ„æº

3. **Reward å…¨ä¸º 0**
   - æ£€æŸ¥ä»£ç æå–é€»è¾‘ï¼ˆæ˜¯å¦æ­£ç¡®è¯†åˆ« ```python``` å—ï¼‰
   - æ£€æŸ¥ test_cases æ ¼å¼æ˜¯å¦æ­£ç¡®
   - **æ£€æŸ¥ `load_format` æ˜¯å¦ä¸º `"auto"`**

4. **NCCL åˆå§‹åŒ–é”™è¯¯**
   - æ£€æŸ¥ GPU å¯ç”¨æ€§ï¼š`nvidia-smi`
   - è®¾ç½®ç¯å¢ƒå˜é‡ï¼š`NCCL_DEBUG=INFO`
   - ç¡®ä¿ tensor_parallel_size ä¸è¶…è¿‡å¯ç”¨ GPU æ•°

5. **è¯„æµ‹å¡ä½**
   - æ£€æŸ¥ç½‘ç»œè¿æ¥
   - æŸ¥çœ‹ SandboxFusion æ—¥å¿—
   - è€ƒè™‘æ·»åŠ å•æ¬¡è¶…æ—¶å’Œé‡è¯•é€»è¾‘

> **ğŸ’¡ æ›´å¤šæ’æŸ¥æŒ‡å—**ï¼šè¯¦è§ [verl_standalone_rollout_guide.md](./verl_standalone_rollout_guide.md) ç¬¬å…«ç« ã€Œå¸¸è§é—®é¢˜ä¸æ’æŸ¥ã€ã€‚

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv2.2*
*åˆ›å»ºæ—¥æœŸï¼š2024-01-31*
*æœ€åæ›´æ–°ï¼š2026-01-31*

**v2.2 æ›´æ–°è¯´æ˜**ï¼š
- **ä¿®æ­£æ–‡ä»¶è·¯å¾„**ï¼šverl ä»£ç åº“é‡‡ç”¨åŒå±‚ç›®å½•ç»“æ„ï¼ˆ`verl/verl/`ï¼‰ï¼Œæ›´æ–°æ‰€æœ‰æ–‡ä»¶è·¯å¾„å¼•ç”¨
- **æ–°å¢å‚è€ƒæ–‡æ¡£é“¾æ¥**ï¼šå…³è” `verl_standalone_rollout_guide.md` æä¾›æ·±å…¥æŠ€æœ¯è§£æ
- **çªå‡º `load_format` å…³é”®é…ç½®**ï¼šStandalone æ¨¡å¼å¿…é¡»ä½¿ç”¨ `"auto"`ï¼Œè¿™æ˜¯æœ€å¸¸è§çš„é…ç½®é”™è¯¯
- **æ–°å¢ `main_generation_server.py` å‚è€ƒ**ï¼šæ¨èä½¿ç”¨ verl å®˜æ–¹ Standalone æ¨¡å¼ç¤ºä¾‹
- **æ›´æ–°å¸¸è§é—®é¢˜æ’æŸ¥**ï¼šæ–°å¢æ¨¡å‹è¾“å‡ºéšæœºã€NCCL é”™è¯¯ç­‰æ’æŸ¥æŒ‡å—
- **æ›´æ–°é™„å½•æ–‡ä»¶ç´¢å¼•**ï¼šæ·»åŠ ç›®å½•ç»“æ„è¯´æ˜å’Œæ¨èå‚è€ƒæ ‡è®°

**v2.1 æ›´æ–°è¯´æ˜**ï¼š
- æ›´æ–°æ•°æ®è·å–æ–¹å¼ï¼Œä¼˜å…ˆä½¿ç”¨ SandboxFusion SDK `get_prompts()` è·å–æ•°æ®
- æ–°å¢ `submit()` API è¯„æµ‹æ–¹å¼ï¼Œç®€åŒ–è¯„æµ‹æµç¨‹
- è¯„æµ‹è„šæœ¬æ”¯æŒä¸¤ç§æ–¹å¼ï¼š`submit()` APIï¼ˆæ¨èï¼‰å’Œ `compute_score()`ï¼ˆä¸ GRPO ä¸€è‡´ï¼‰
- æ–°å¢ `--use-submit-api` å’Œ `--use-compute-score` å‘½ä»¤è¡Œå‚æ•°
- æ›´æ–°æ¶æ„å›¾ï¼Œå±•ç¤ºä¸¤ç§è¯„æµ‹æ–¹å¼é€‰é¡¹

**v2.0 æ›´æ–°è¯´æ˜**ï¼š
- ä¿®æ­£è¯„æµ‹è„šæœ¬ï¼Œä»çº¯ HuggingFace æ”¹ä¸ºä½¿ç”¨ verl çš„ vLLM/SGLang åˆ†å¸ƒå¼æ¨ç†æ¶æ„
- æ–°å¢ verl Standalone Rollout æ¨¡å¼è¯¦è§£ï¼ˆSection 2.3, 2.4ï¼‰
- æ›´æ–°ä»£ç æ‰§è¡Œæµç¨‹å›¾ï¼Œå±•ç¤º Ray é›†ç¾¤åè°ƒå’Œ OpenAI-compatible API
- æ–°å¢ verl ä¸ HuggingFace å¯¹æ¯”è¯´æ˜ï¼ˆSection 2.6ï¼‰
- æ›´æ–°è¯„æµ‹è„šæœ¬æ¡†æ¶ï¼Œä½¿ç”¨ `RolloutReplica.init_standalone()` å’Œ async HTTP è¯·æ±‚
- æ–°å¢ verl é…ç½®æ–‡ä»¶ç¤ºä¾‹ï¼ˆYAML æ ¼å¼ï¼‰
- æ›´æ–°é™„å½•ä¸­çš„å…³é”®æ–‡ä»¶ç´¢å¼•
