# verl Standalone Rollout æ¨¡å¼è¯¦è§£

> æœ¬æ–‡æ¡£ä¸º Phase 0 Baseline è¯„æµ‹æä¾› verl æ¡†æ¶çš„æ·±åº¦æŠ€æœ¯è®²è§£ï¼Œå¸®åŠ©æ–°æ‰‹ç†è§£ Standalone Rollout æ¨¡å¼çš„å·¥ä½œåŸç†å’Œä»£ç æ‰§è¡Œæµç¨‹ã€‚
>
> **ğŸ“š ç›¸å…³æ–‡æ¡£**ï¼š
> - [phase0_implementation_plan.md](./phase0_implementation_plan.md) â€” Phase 0 æ•´ä½“å®æ–½è®¡åˆ’ã€è¯„æµ‹è„šæœ¬å’ŒæŒ‡æ ‡æ”¶é›†
> - [data_governance_guide.md](./data_governance_guide.md) â€” æ•°æ®æ²»ç†è¯¦ç»†æŒ‡å—
> - [metrics_collection_spec.md](./metrics_collection_spec.md) â€” æŒ‡æ ‡æ”¶é›†è§„èŒƒ

---

## ä¸€ã€æ¦‚è¿°

### 1.1 ä»€ä¹ˆæ˜¯ Standalone Rollout æ¨¡å¼

**Rollout** åœ¨ verl æ¡†æ¶ä¸­æŒ‡çš„æ˜¯ä½¿ç”¨ LLM æ¨ç†å¼•æ“ï¼ˆvLLM æˆ– SGLangï¼‰ç”Ÿæˆæ–‡æœ¬åºåˆ—çš„è¿‡ç¨‹ã€‚verl æä¾›ä¸‰ç§ Rollout éƒ¨ç½²æ¨¡å¼ï¼Œå®šä¹‰åœ¨ `verl/verl/workers/rollout/replica.py`:

```python
class RolloutMode(Enum):
    # Rollout ä¸è®­ç»ƒå¼•æ“èåˆåœ¨åŒä¸€è¿›ç¨‹
    HYBRID = "hybrid"

    # Rollout ä¸è®­ç»ƒå¼•æ“å…±äº« GPUï¼Œç‹¬ç«‹è¿›ç¨‹
    COLOCATED = "colocated"

    # ç‹¬ç«‹ GPU èµ„æºï¼Œdisaggregated æ¶æ„
    STANDALONE = "standalone"
```

**Standalone æ¨¡å¼**çš„æ ¸å¿ƒç‰¹å¾ï¼š
- **ç‹¬ç«‹ GPU èµ„æº**ï¼šä¸ºæ¨ç†åˆ†é…ä¸“ç”¨ GPUï¼Œä¸ä¸è®­ç»ƒå…±äº«
- **æ— æƒé‡åŒæ­¥**ï¼šæ¨¡å‹æƒé‡ä»ç£ç›˜ä¸€æ¬¡æ€§åŠ è½½ï¼Œè¿è¡ŒæœŸé—´ä¿æŒé™æ€
- **HTTP API**ï¼šæä¾› OpenAI å…¼å®¹çš„ REST API æ¥å£

### 1.2 ä¸ºä»€ä¹ˆ Phase 0 ä½¿ç”¨ Standalone æ¨¡å¼

Phase 0 æ˜¯çº¯è¯„æµ‹é˜¶æ®µï¼Œ**ä¸æ¶‰åŠä»»ä½•è®­ç»ƒ**ï¼Œå…¶ç›®æ ‡æ˜¯ï¼š
- åœ¨æœªè®­ç»ƒçš„ Base æ¨¡å‹ä¸Šå»ºç«‹æ€§èƒ½åŸºçº¿
- éªŒè¯è¯„æµ‹æµæ°´çº¿ï¼ˆSandboxFusion åˆ¤é¢˜ï¼‰æ­£å¸¸å·¥ä½œ
- æ”¶é›†è´¨é‡ã€æˆæœ¬ã€é”™è¯¯åˆ†å¸ƒç­‰æŒ‡æ ‡

**Standalone æ¨¡å¼å®Œç¾å¥‘åˆè¿™äº›éœ€æ±‚**ï¼š

| ç‰¹æ€§ | Standalone | Hybrid | è¯´æ˜ |
|------|------------|--------|------|
| éœ€è¦è®­ç»ƒå¼•æ“ | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ | Phase 0 æ— è®­ç»ƒ |
| æƒé‡åŒæ­¥ | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ | æ¨¡å‹é™æ€ä¸å˜ |
| GPU ç‹¬å æ¨ç† | âœ… æœ€å¤§åŒ–åˆ©ç”¨ | âš ï¸ éœ€ä¸è®­ç»ƒå…±äº« | è¯„æµ‹ååé‡æ›´é«˜ |
| å®ç°å¤æ‚åº¦ | â­ ä½ | â­â­â­ é«˜ | æ›´å®¹æ˜“è°ƒè¯• |

### 1.3 ä¸‰ç§æ¨¡å¼å¯¹æ¯”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         verl Rollout æ¨¡å¼å¯¹æ¯”                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   HYBRID (æ··åˆæ¨¡å¼)                                                          â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  åŒä¸€è¿›ç¨‹                                                     â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    æƒé‡åŒæ­¥     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚          â”‚
â”‚   â”‚  â”‚ è®­ç»ƒå¼•æ“     â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Rollout å¼•æ“ â”‚            â”‚          â”‚
â”‚   â”‚  â”‚ (FSDP)      â”‚                â”‚ (vLLM)      â”‚            â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   é€‚ç”¨ï¼šOn-policy è®­ç»ƒ (GRPO/PPO)                                           â”‚
â”‚                                                                             â”‚
â”‚   COLOCATED (å…±ç½®æ¨¡å¼)                                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  åŒä¸€ GPUï¼Œä¸åŒè¿›ç¨‹                                           â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    æ— æƒé‡åŒæ­¥    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚          â”‚
â”‚   â”‚  â”‚ è®­ç»ƒå¼•æ“     â”‚ â”€ â”€ â”€ â”€ â”€ â”€ â”€  â”‚ Rollout å¼•æ“ â”‚            â”‚          â”‚
â”‚   â”‚  â”‚ (è¿›ç¨‹ A)    â”‚                â”‚ (è¿›ç¨‹ B)     â”‚            â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   é€‚ç”¨ï¼šGRM (LLM as a Judge)                                               â”‚
â”‚                                                                             â”‚
â”‚   STANDALONE (ç‹¬ç«‹æ¨¡å¼) â† Phase 0 ä½¿ç”¨                                       â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  ç‹¬ç«‹ GPU èµ„æº                                                â”‚          â”‚
â”‚   â”‚                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚          â”‚
â”‚   â”‚      æ— è®­ç»ƒå¼•æ“          â”‚ Rollout å¼•æ“ â”‚                     â”‚          â”‚
â”‚   â”‚                         â”‚ (vLLM/SGLang)â”‚                     â”‚          â”‚
â”‚   â”‚                         â”‚ HTTP Server  â”‚                     â”‚          â”‚
â”‚   â”‚                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚   é€‚ç”¨ï¼šPhase 0 è¯„æµ‹ã€Off-policy è®­ç»ƒã€æ‰¹é‡æ¨ç†                              â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€verl Rollout æ¶æ„è¯¦è§£

### 2.1 æ ¸å¿ƒç±»å±‚æ¬¡ç»“æ„

```
RolloutReplica (æŠ½è±¡åŸºç±»)
â”œâ”€â”€ å®šä¹‰ä¸‰ç§åˆå§‹åŒ–æ–¹æ³•ï¼š
â”‚   â”œâ”€â”€ init_hybrid(worker_group)     # æ··åˆæ¨¡å¼
â”‚   â”œâ”€â”€ init_colocated(resource_pool) # å…±ç½®æ¨¡å¼
â”‚   â””â”€â”€ init_standalone()             # ç‹¬ç«‹æ¨¡å¼ â† Phase 0
â”‚
â”œâ”€â”€ vLLMReplica (vLLM å®ç°)
â”‚   â””â”€â”€ verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py
â”‚
â””â”€â”€ SGLangReplica (SGLang å®ç°)
    â””â”€â”€ verl/verl/workers/rollout/sglang_rollout/async_sglang_server.py
```

### 2.2 RolloutReplica åŸºç±»

**æ–‡ä»¶ä½ç½®**ï¼š`verl/verl/workers/rollout/replica.py`

```python
class RolloutReplica(ABC):
    """
    Rollout replica æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æœåŠ¡å™¨å®ä¾‹ï¼Œå¯éƒ¨ç½²åœ¨å•èŠ‚ç‚¹æˆ–å¤šèŠ‚ç‚¹ä¸Šã€‚

    ç­‰æ•ˆäºå‘½ä»¤è¡Œå¯åŠ¨ï¼š
    - SGLang: python -m sglang.launch_server --node-rank 0 --nnode 2 ...
    - vLLM:   vllm serve --data-parallel-size 16 ...

    å‚æ•°ï¼š
        replica_rank: int, å½“å‰ replica çš„ç¼–å·
        config: RolloutConfig, æ¨ç†é…ç½®
        model_config: HFModelConfig, æ¨¡å‹é…ç½®
        gpus_per_node: int, æ¯èŠ‚ç‚¹ GPU æ•°é‡
    """

    def __init__(
        self,
        replica_rank: int,
        config: RolloutConfig,
        model_config: DictConfig,
        gpus_per_node: int = 8,
    ) -> None:
        self.replica_rank = replica_rank
        self.config = config
        self.model_config = model_config

        # è®¡ç®—å¹¶è¡Œåº¦
        self.world_size = (
            config.tensor_model_parallel_size
            * config.data_parallel_size
            * config.pipeline_model_parallel_size
        )
        self.gpus_per_node = min(gpus_per_node, self.world_size)
        self.nnodes = self.world_size // self.gpus_per_node

        # è¿è¡Œæ—¶çŠ¶æ€
        self.rollout_mode: RolloutMode = None
        self.workers: list[ActorHandle] = []
        self.resource_pool: RayResourcePool = None
        self._server_address: str = None      # HTTP æœåŠ¡å™¨åœ°å€
        self._server_handle: ActorHandle = None
```

### 2.3 init_standalone() è¯¦è§£

è¿™æ˜¯ Phase 0 è¯„æµ‹ä½¿ç”¨çš„æ ¸å¿ƒæ–¹æ³•ï¼š

```python
async def init_standalone(self):
    """
    åˆå§‹åŒ– Standalone Rollout æœåŠ¡å™¨

    æµç¨‹ï¼š
    1. åˆ›å»ºç‹¬ç«‹çš„ GPU èµ„æºæ± 
    2. åˆ›å»º Worker ç»„
    3. å¯åŠ¨ HTTP æœåŠ¡å™¨
    """
    # Step 1: è®¾ç½®æ¨¡å¼
    self.rollout_mode = RolloutMode.STANDALONE

    # Step 2: åˆ›å»ºèµ„æºæ± 
    # æ¯ä¸ª replica æ‹¥æœ‰ç‹¬ç«‹çš„ GPU èµ„æº
    resource_pool_name = f"rollout_pool_{self.replica_rank}"
    resource_pool_spec = {
        resource_pool_name: [self.gpus_per_node] * self.nnodes,
    }

    # ä½¿ç”¨ ResourcePoolManager ç®¡ç† Ray placement group
    resource_pool_manager = ResourcePoolManager(
        resource_pool_spec=resource_pool_spec,
        mapping=None
    )
    resource_pool_manager.create_resource_pool()
    self.resource_pool = resource_pool_manager.resource_pool_dict[resource_pool_name]

    # Step 3: åˆ›å»º Worker ç»„
    # Worker æ˜¯ Ray Actorï¼Œè¿è¡Œ vLLM/SGLang æ¨ç†å¼•æ“
    worker_group = RayWorkerGroup(
        resource_pool=self.resource_pool,
        ray_cls_with_init=self.get_ray_class_with_init_args(),  # æŠ½è±¡æ–¹æ³•
        bin_pack=False,
        name_prefix=f"rollout_standalone_{self.replica_rank}",
    )
    self.workers = worker_group.workers

    # Step 4: å¯åŠ¨ HTTP æœåŠ¡å™¨
    # æä¾› OpenAI-compatible API
    await self.launch_servers()  # æŠ½è±¡æ–¹æ³•
```

**å…³é”®æ¦‚å¿µè§£é‡Š**ï¼š

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| `ResourcePool` | Ray placement group çš„å°è£…ï¼Œç®¡ç† GPU èµ„æºåˆ†é… |
| `RayWorkerGroup` | ç®¡ç†ä¸€ç»„ Ray Actorï¼Œæä¾›æ•°æ®åˆ†å‘å’Œæ”¶é›† |
| `replica_rank` | å½“å‰ replica çš„ç¼–å·ï¼ˆ0, 1, 2, ...ï¼‰ |
| `world_size` | æ€» GPU æ•°é‡ = TP Ã— DP Ã— PP |

### 2.4 vLLMReplica å®ç°

**æ–‡ä»¶ä½ç½®**ï¼š`verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py`

```python
class vLLMReplica(RolloutReplica):
    """vLLM åç«¯çš„ Rollout Replica å®ç°"""

    def get_ray_class_with_init_args(self) -> RayClassWithInitArgs:
        """è¿”å› vLLM Worker ç±»åŠå…¶åˆå§‹åŒ–å‚æ•°"""
        worker_dict_cls = RayClassWithInitArgs(
            cls=ray.remote(vLLMAsyncRollout),  # Ray Actor ç±»
            config=self.config,
            model_config=self.model_config,
            device_mesh=None,  # Standalone æ¨¡å¼ä¸éœ€è¦ device mesh
        )
        return worker_dict_cls

    async def launch_servers(self):
        """å¯åŠ¨ vLLM HTTP æœåŠ¡å™¨"""
        # åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå¯åŠ¨ä¸€ä¸ª HTTP æœåŠ¡å™¨
        self._http_server = vLLMHttpServer(
            workers=self.workers,
            config=self.config,
            model_config=self.model_config,
            rollout_mode=self.rollout_mode,
        )

        # å¯åŠ¨æœåŠ¡å™¨å¹¶è·å–åœ°å€
        self._server_address = await self._http_server.start()
        self._server_handle = self._http_server
```

### 2.5 HTTP Server æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         vLLM HTTP Server æ¶æ„                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚   Client (è¯„æµ‹è„šæœ¬)                                                          â”‚
â”‚       â”‚                                                                     â”‚
â”‚       â”‚ POST /v1/chat/completions                                          â”‚
â”‚       â–¼                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚   â”‚  vLLMHttpServer (Ray Actor)                                  â”‚          â”‚
â”‚   â”‚                                                              â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚          â”‚
â”‚   â”‚  â”‚  FastAPI Application                                  â”‚    â”‚          â”‚
â”‚   â”‚  â”‚                                                       â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  /v1/chat/completions  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  /v1/completions       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  /health               â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚    â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”€â”€â”€â”˜    â”‚          â”‚
â”‚   â”‚                                                      â”‚        â”‚          â”‚
â”‚   â”‚                                                      â–¼        â”‚          â”‚
â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚          â”‚
â”‚   â”‚  â”‚  AsyncLLM Engine (vLLM v1)                          â”‚    â”‚          â”‚
â”‚   â”‚  â”‚                                                       â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  â”‚  GPU 0 (TP=0) â”‚  â”‚  GPU 1 (TP=1) â”‚  ...          â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  â”‚  Attention    â”‚  â”‚  Attention    â”‚               â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  â”‚  FFN          â”‚  â”‚  FFN          â”‚               â”‚    â”‚          â”‚
â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚    â”‚          â”‚
â”‚   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚          â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ä¸‰ã€ä»£ç æ‰§è¡Œæµç¨‹ï¼ˆå®Œæ•´è¿½è¸ªï¼‰

### 3.1 ç«¯åˆ°ç«¯æµç¨‹å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Phase 0 Baseline è¯„æµ‹æµç¨‹                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  1. ç¯å¢ƒåˆå§‹åŒ–                                                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  ray.init(runtime_env={                                      â”‚        â”‚
â”‚     â”‚      "env_vars": {                                           â”‚        â”‚
â”‚     â”‚          "TOKENIZERS_PARALLELISM": "true",                   â”‚        â”‚
â”‚     â”‚          "NCCL_DEBUG": "WARN",                               â”‚        â”‚
â”‚     â”‚          "VLLM_USE_V1": "1"  # ä½¿ç”¨ vLLM v1 å¼•æ“             â”‚        â”‚
â”‚     â”‚      }                                                       â”‚        â”‚
â”‚     â”‚  })                                                          â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  2. åˆ›å»º Rollout Replica                                                    â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  rollout_class = get_rollout_replica_class("vllm")           â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚  num_replicas = total_gpus / tensor_parallel_size            â”‚        â”‚
â”‚     â”‚  ä¾‹å¦‚: 8 GPU, TP=2 â†’ 4 replicas                              â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚  replicas = [                                                â”‚        â”‚
â”‚     â”‚      rollout_class(replica_rank=i, config=..., model=...)    â”‚        â”‚
â”‚     â”‚      for i in range(num_replicas)                            â”‚        â”‚
â”‚     â”‚  ]                                                           â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  3. åˆå§‹åŒ– Standalone æ¨¡å¼                                                   â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  await asyncio.gather(*[                                     â”‚        â”‚
â”‚     â”‚      replica.init_standalone()                               â”‚        â”‚
â”‚     â”‚      for replica in replicas                                 â”‚        â”‚
â”‚     â”‚  ])                                                          â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚  å†…éƒ¨æ‰§è¡Œï¼š                                                   â”‚        â”‚
â”‚     â”‚  â”œâ”€â”€ åˆ›å»º ResourcePool (Ray placement group)                 â”‚        â”‚
â”‚     â”‚  â”œâ”€â”€ åˆ›å»º RayWorkerGroup (Ray Actors)                        â”‚        â”‚
â”‚     â”‚  â”œâ”€â”€ åŠ è½½æ¨¡å‹æƒé‡ (load_format="auto")                       â”‚        â”‚
â”‚     â”‚  â””â”€â”€ å¯åŠ¨ HTTP æœåŠ¡å™¨                                        â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  4. è·å–æœåŠ¡å™¨åœ°å€                                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  server_addresses = [replica._server_address for replica...] â”‚        â”‚
â”‚     â”‚  ä¾‹å¦‚: ["10.0.0.1:8000", "10.0.0.1:8001", ...]              â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  5. åŠ è½½è¯„æµ‹æ•°æ®                                                             â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  data = pd.read_parquet("codecontests_valid.parquet")        â”‚        â”‚
â”‚     â”‚  prompts = data['prompt'].tolist()                           â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  6. æ‰¹é‡ç”Ÿæˆï¼ˆå¼‚æ­¥ HTTP è¯·æ±‚ï¼‰                                               â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  async def generate(prompts):                                â”‚        â”‚
â”‚     â”‚      # å°†æ•°æ®åˆ†å‘åˆ°å¤šä¸ª replicaï¼ˆè´Ÿè½½å‡è¡¡ï¼‰                   â”‚        â”‚
â”‚     â”‚      chunks = np.array_split(prompts, num_replicas)          â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚      # å¹¶è¡Œè¯·æ±‚æ‰€æœ‰ replica                                  â”‚        â”‚
â”‚     â”‚      results = await asyncio.gather(*[                       â”‚        â”‚
â”‚     â”‚          generate_per_replica(server_addresses[i], chunks[i])â”‚        â”‚
â”‚     â”‚          for i in range(num_replicas)                        â”‚        â”‚
â”‚     â”‚      ])                                                      â”‚        â”‚
â”‚     â”‚      return results                                          â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚  async def generate_per_replica(server_addr, prompts):       â”‚        â”‚
â”‚     â”‚      async with aiohttp.ClientSession() as session:          â”‚        â”‚
â”‚     â”‚          response = await session.post(                      â”‚        â”‚
â”‚     â”‚              f"http://{server_addr}/v1/chat/completions",    â”‚        â”‚
â”‚     â”‚              json={                                          â”‚        â”‚
â”‚     â”‚                  "model": model_path,                        â”‚        â”‚
â”‚     â”‚                  "messages": [{"role":"user","content":...}],â”‚        â”‚
â”‚     â”‚                  "temperature": 0.0,  # EVAL@1 greedy        â”‚        â”‚
â”‚     â”‚                  "max_tokens": 2048                          â”‚        â”‚
â”‚     â”‚              }                                               â”‚        â”‚
â”‚     â”‚          )                                                   â”‚        â”‚
â”‚     â”‚      return response.choices[0].message.content              â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  7. SandboxFusion è¯„æµ‹                                                      â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  from sandbox_fusion import submit, SubmitRequest, TestConfigâ”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚  for completion, record in zip(completions, data):           â”‚        â”‚
â”‚     â”‚      result = submit(SubmitRequest(                          â”‚        â”‚
â”‚     â”‚          dataset=record['sandbox_dataset'],                  â”‚        â”‚
â”‚     â”‚          id=record['sandbox_id'],                            â”‚        â”‚
â”‚     â”‚          completion=completion,                              â”‚        â”‚
â”‚     â”‚          config=TestConfig(language='python', run_timeout=10)â”‚        â”‚
â”‚     â”‚      ))                                                      â”‚        â”‚
â”‚     â”‚                                                              â”‚        â”‚
â”‚     â”‚      metrics.append({                                        â”‚        â”‚
â”‚     â”‚          "accepted": result.accepted,                        â”‚        â”‚
â”‚     â”‚          "pass_ratio": 1.0 if result.accepted else 0.0,      â”‚        â”‚
â”‚     â”‚      })                                                      â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                    â”‚                                        â”‚
â”‚                                    â–¼                                        â”‚
â”‚  8. æŒ‡æ ‡èšåˆä¸æ—¥å¿—                                                           â”‚
â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚     â”‚  wandb.log({                                                 â”‚        â”‚
â”‚     â”‚      "eval/codecontests_valid/accepted_at_1": 0.08,          â”‚        â”‚
â”‚     â”‚      "eval/codecontests_valid/pass_ratio_mean": 0.23,        â”‚        â”‚
â”‚     â”‚      ...                                                     â”‚        â”‚
â”‚     â”‚  })                                                          â”‚        â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Ray é›†ç¾¤åˆå§‹åŒ–è¯¦è§£

```python
import ray

# åˆå§‹åŒ– Ray é›†ç¾¤
ray.init(
    runtime_env={
        "env_vars": {
            # å¯ç”¨ tokenizer å¹¶è¡ŒåŒ–
            "TOKENIZERS_PARALLELISM": "true",

            # NCCL è°ƒè¯•çº§åˆ«ï¼ˆWARN å‡å°‘è¾“å‡ºï¼‰
            "NCCL_DEBUG": "WARN",

            # ä½¿ç”¨ vLLM v1 å¼•æ“ï¼ˆæ¨èï¼‰
            "VLLM_USE_V1": "1",
        }
    }
)
```

**ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ç¯å¢ƒå˜é‡ï¼Ÿ**

| å˜é‡ | ä½œç”¨ |
|------|------|
| `TOKENIZERS_PARALLELISM` | å…è®¸ HuggingFace tokenizer å¹¶è¡Œå¤„ç†ï¼Œæé«˜æ•ˆç‡ |
| `NCCL_DEBUG` | NVIDIA é›†åˆé€šä¿¡åº“æ—¥å¿—çº§åˆ«ï¼ŒWARN å‡å°‘å™ªéŸ³ |
| `VLLM_USE_V1` | å¯ç”¨ vLLM æ–°ç‰ˆå¼•æ“ï¼Œæ€§èƒ½æ›´å¥½ |

### 3.3 Replica æ•°é‡è®¡ç®—

```python
# é…ç½®å‚æ•°
n_gpus_per_node = 8
nnodes = 1
tensor_model_parallel_size = 2  # TP å¹¶è¡Œåº¦

# è®¡ç®—æ€» GPU æ•°é‡
total_gpus = n_gpus_per_node * nnodes  # = 8

# è®¡ç®— replica æ•°é‡
# æ¯ä¸ª replica éœ€è¦ TP ä¸ª GPU
num_replicas = total_gpus // tensor_model_parallel_size  # = 4

# ç»“æœï¼š4 ä¸ª replicaï¼Œæ¯ä¸ªä½¿ç”¨ 2 ä¸ª GPU (TP=2)
# replica 0: GPU 0-1
# replica 1: GPU 2-3
# replica 2: GPU 4-5
# replica 3: GPU 6-7
```

### 3.4 HTTP API è¯·æ±‚æ ¼å¼

verl çš„ Rollout æœåŠ¡å™¨æä¾› **OpenAI å…¼å®¹ API**ï¼š

```python
# è¯·æ±‚
POST /v1/chat/completions
{
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "messages": [
        {"role": "user", "content": "Write a Python function..."}
    ],
    "temperature": 0.0,      # EVAL@1 åè®®ä½¿ç”¨ greedy decoding
    "top_p": 1.0,
    "max_tokens": 2048
}

# å“åº”
{
    "id": "chatcmpl-xxx",
    "object": "chat.completion",
    "created": 1706745600,
    "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
    "choices": [
        {
            "index": 0,
            "message": {
                "role": "assistant",
                "content": "```python\ndef solution():\n    ..."
            },
            "finish_reason": "stop"
        }
    ],
    "usage": {
        "prompt_tokens": 100,
        "completion_tokens": 200,
        "total_tokens": 300
    }
}
```

---

## å››ã€SandboxFusion é›†æˆ

### 4.1 ä¸¤ç§è¯„æµ‹æ–¹å¼å¯¹æ¯”

| æ–¹å¼ | API | ç®€å•æ€§ | ä¸ GRPO ä¸€è‡´æ€§ | æ¨èåœºæ™¯ |
|------|-----|--------|---------------|---------|
| `submit()` | SandboxFusion SDK | â­â­â­ ç®€å• | âŒ ä¸åŒ | Phase 0 å¿«é€Ÿè¯„æµ‹ |
| `compute_score()` | verl å†…éƒ¨ | â­â­ ä¸­ç­‰ | âœ… ä¸€è‡´ | éœ€è¦ä¸è®­ç»ƒå¯¹é½ |

### 4.2 ä½¿ç”¨ submit() APIï¼ˆæ¨èç”¨äº Phase 0ï¼‰

```python
from sandbox_fusion import submit, SubmitRequest, TestConfig

def evaluate_with_submit(completion: str, record: dict) -> dict:
    """
    ä½¿ç”¨ SandboxFusion submit() API è¯„æµ‹

    ä¼˜ç‚¹ï¼š
    - æ— éœ€ç®¡ç†æµ‹è¯•ç”¨ä¾‹
    - ä»£ç ç®€æ´
    - ç›´æ¥è¿”å› accepted ç»“æœ
    """
    result = submit(SubmitRequest(
        dataset=record['sandbox_dataset'],  # e.g., "humaneval"
        id=record['sandbox_id'],             # e.g., "0"
        completion=completion,
        config=TestConfig(
            language='python',
            run_timeout=10
        )
    ))

    return {
        "accepted": result.accepted,
        "tests": result.tests if hasattr(result, 'tests') else None,
    }
```

### 4.3 ä½¿ç”¨ compute_score()ï¼ˆä¸ GRPO ä¸€è‡´ï¼‰

```python
from verl.utils.reward_score.sandbox_fusion import compute_score

def evaluate_with_compute_score(
    completion: str,
    test_cases: dict,
    sandbox_url: str = "http://localhost:8080/run_code"
) -> tuple[float, list]:
    """
    ä½¿ç”¨ verl compute_score() è¯„æµ‹

    ä¼˜ç‚¹ï¼š
    - ä¸ GRPO è®­ç»ƒé˜¶æ®µå®Œå…¨ä¸€è‡´
    - è¿”å›è¯¦ç»†çš„ metadata

    å‚æ•°ï¼š
        test_cases: {"inputs": [...], "outputs": [...]}
    """
    score, metadata = compute_score(
        sandbox_fusion_url=sandbox_url,
        concurrent_semaphore=None,
        memory_limit_mb=1024,
        completion=completion,
        test_cases=test_cases,
        continuous=False,
        timeout=10,
    )

    return score, metadata
```

### 4.4 è¿”å›å€¼ä¸çŠ¶æ€ç 

| ç»“æœå€¼ | å«ä¹‰ | ç»Ÿè®¡ç±»åˆ« |
|--------|------|---------|
| `True` | æµ‹è¯•é€šè¿‡ | success |
| `False` | è¾“å‡ºé”™è¯¯ (Wrong Answer) | wrong_answer |
| `-1` | API/Sandbox é”™è¯¯ | api_error |
| `-2` | è¿è¡Œæ—¶é”™è¯¯ (Runtime Error) | runtime_error |
| `-3` | è¶…æ—¶ (Timeout) | timeout |
| `-4` | ç¼–è¯‘é”™è¯¯ (Compile Error) | syntax_error |

```python
def determine_final_status(results: list) -> str:
    """æ ¹æ®æµ‹è¯•ç»“æœç¡®å®šæœ€ç»ˆçŠ¶æ€"""
    for r in results:
        if r == -4:
            return "syntax_error"
        elif r == -2:
            return "runtime_error"
        elif r == -3:
            return "timeout"

    if all(r is True for r in results):
        return "success"
    elif any(r is False for r in results):
        return "wrong_answer"
    else:
        return "api_error"
```

---

## äº”ã€å®˜æ–¹å‚è€ƒå®ç°åˆ†æ

### 5.1 main_generation_server.py å®Œæ•´è§£æ

**æ–‡ä»¶ä½ç½®**ï¼š`verl/verl/trainer/main_generation_server.py`

è¿™æ˜¯ verl å®˜æ–¹æä¾›çš„ Standalone æ¨¡å¼å‚è€ƒå®ç°ï¼ŒPhase 0 è¯„æµ‹å¯ä»¥ç›´æ¥åŸºäºæ­¤ä¿®æ”¹ã€‚

```python
"""
Generate responses given a dataset of prompts
"""

import os
import aiohttp
import hydra
import numpy as np
import ray

# ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["NCCL_DEBUG"] = "WARN"
os.environ["TOKENIZERS_PARALLELISM"] = "true"

import asyncio
from pprint import pprint
import pandas as pd
from omegaconf import OmegaConf
from openai.types.chat import ChatCompletion

from verl.utils.hdfs_io import makedirs
from verl.workers.rollout.replica import get_rollout_replica_class


async def start_server(config):
    """
    åˆ›å»ºå¹¶åˆå§‹åŒ– Standalone Rollout æœåŠ¡å™¨

    Returns:
        server_handles: Ray Actor å¥æŸ„åˆ—è¡¨
        server_addresses: HTTP æœåŠ¡å™¨åœ°å€åˆ—è¡¨
    """
    # è®¡ç®— replica æ•°é‡
    tp_size = config.actor_rollout_ref.rollout.tensor_model_parallel_size
    num_replicas = (config.trainer.n_gpus_per_node * config.trainer.nnodes) // tp_size

    rollout_config = config.actor_rollout_ref.rollout
    model_config = config.actor_rollout_ref.model

    # è·å–å¯¹åº”çš„ Replica ç±»ï¼ˆvLLM æˆ– SGLangï¼‰
    rollout_server_class = get_rollout_replica_class(config.actor_rollout_ref.rollout.name)

    # åˆ›å»ºæ‰€æœ‰ replica å®ä¾‹
    rollout_servers = [
        rollout_server_class(
            replica_rank=replica_rank,
            config=rollout_config,
            model_config=model_config,
            gpus_per_node=config.trainer.n_gpus_per_node,
        )
        for replica_rank in range(num_replicas)
    ]

    # å¹¶è¡Œåˆå§‹åŒ–æ‰€æœ‰ replicaï¼ˆStandalone æ¨¡å¼ï¼‰
    await asyncio.gather(*[server.init_standalone() for server in rollout_servers])

    # æ”¶é›†æœåŠ¡å™¨ä¿¡æ¯
    server_handles = [server._server_handle for server in rollout_servers]
    server_addresses = [server._server_address for server in rollout_servers]

    assert len(server_handles) == num_replicas
    assert len(server_addresses) == num_replicas

    return server_handles, server_addresses


async def submit_request(server_address, **chat_complete_request):
    """
    å‘å•ä¸ªæœåŠ¡å™¨æäº¤è¯·æ±‚

    ä½¿ç”¨ aiohttp è€Œé openai åº“ï¼Œé¿å…å¤§é‡è¯·æ±‚æ—¶çš„æ­»é”é—®é¢˜
    """
    try:
        extra_headers = chat_complete_request.pop("extra_headers", {})
        timeout = aiohttp.ClientTimeout(total=None)  # æ— è¶…æ—¶é™åˆ¶
        session = aiohttp.ClientSession(timeout=timeout)

        async with session.post(
            url=f"http://{server_address}/v1/chat/completions",
            headers={"Authorization": "Bearer token-abc123", **extra_headers},
            json=chat_complete_request,
        ) as resp:
            data = await resp.json()
            return ChatCompletion(**data)
    finally:
        await session.close()


async def generate_per_replica(server_address, model_path: str, n_samples: int,
                               sampling_params: dict, chat_lst: list):
    """
    åœ¨å•ä¸ª replica ä¸Šç”Ÿæˆ

    Args:
        n_samples: æ¯ä¸ª prompt ç”Ÿæˆçš„æ ·æœ¬æ•°
    """
    # æ„å»ºè¯·æ±‚åˆ—è¡¨
    chat_complete_request = [
        {
            "model": model_path,
            "messages": messages,
            **sampling_params,
        }
        for messages in chat_lst
        for _ in range(n_samples)  # æ¯ä¸ª prompt é‡å¤ n_samples æ¬¡
    ]

    # å¹¶è¡Œæäº¤æ‰€æœ‰è¯·æ±‚
    tasks = [submit_request(server_address, **req) for req in chat_complete_request]
    results = await asyncio.gather(*tasks)
    return results


async def generate(server_addresses: list, model_path: str, n_samples: int,
                   sampling_params: dict, chat_numpy: np.ndarray):
    """
    åœ¨å¤šä¸ª replica ä¸Šå¹¶è¡Œç”Ÿæˆï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
    """
    num_replicas = len(server_addresses)

    # å°†æ•°æ®å‡åŒ€åˆ†é…åˆ°å„ replica
    chat_sub_array = np.array_split(chat_numpy, num_replicas)
    chat_sub_array = [chat.tolist() for chat in chat_sub_array]

    assert len(server_addresses) == len(chat_sub_array)

    # å¹¶è¡Œè°ƒç”¨æ‰€æœ‰ replica
    results = await asyncio.gather(*[
        generate_per_replica(
            server_addresses[i], model_path, n_samples,
            sampling_params, chat_sub_array[i]
        )
        for i in range(num_replicas)
    ])
    return results


@hydra.main(config_path="config", config_name="ppo_trainer", version_base=None)
def main(config):
    """ä¸»å…¥å£"""
    # åˆå§‹åŒ– Ray
    ray.init(runtime_env={
        "env_vars": {
            "TOKENIZERS_PARALLELISM": "true",
            "NCCL_DEBUG": "WARN",
            "VLLM_USE_V1": "1"
        }
    })

    pprint(OmegaConf.to_container(config, resolve=True))
    OmegaConf.resolve(config)

    n_samples = config.actor_rollout_ref.rollout.n

    # éªŒè¯é‡‡æ ·å‚æ•°
    if config.actor_rollout_ref.rollout.temperature == 0.0:
        assert n_samples == 1, "When temperature=0, n_samples must be 1."
    assert n_samples >= 1

    # é‡‡æ ·å‚æ•°
    sampling_params = {
        "temperature": config.actor_rollout_ref.rollout.temperature,
        "top_p": config.actor_rollout_ref.rollout.top_p,
        "max_tokens": config.actor_rollout_ref.rollout.response_length,
    }

    # åŠ è½½æ•°æ®
    train_files = config.data.train_files
    if not isinstance(train_files, list):
        train_files = [train_files]

    datasets = [pd.read_parquet(f) for f in train_files]
    dataset = pd.concat(datasets, axis=0, ignore_index=True)

    chat_lst = dataset[config.data.prompt_key].tolist()
    chat_lst = [chat.tolist() for chat in chat_lst]
    chat_numpy = np.array(chat_lst)

    # å¯åŠ¨æœåŠ¡å™¨
    server_handles, server_addresses = asyncio.run(start_server(config))

    # ç”Ÿæˆ
    gen_results = asyncio.run(
        generate(server_addresses, config.actor_rollout_ref.model.path,
                 n_samples, sampling_params, chat_numpy)
    )

    # å¤„ç†ç»“æœ
    import itertools
    results = list(itertools.chain.from_iterable(gen_results))
    results = np.array([r.choices[0].message.content for r in results])
    results = np.reshape(results, (-1, n_samples))

    assert results.shape == (len(chat_lst), n_samples)

    dataset["responses"] = results.tolist()

    # ä¿å­˜ç»“æœ
    output_dir = os.path.dirname(config.data.output_path)
    makedirs(output_dir, exist_ok=True)
    print(f"Saving results to {config.data.output_path}")
    dataset.to_parquet(config.data.output_path)


if __name__ == "__main__":
    main()
```

### 5.2 å…³é”®å‡½æ•°æ€»ç»“

| å‡½æ•° | åŠŸèƒ½ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| `start_server()` | åˆå§‹åŒ– Standalone æœåŠ¡å™¨ | config | (handles, addresses) |
| `submit_request()` | å•ä¸ª HTTP è¯·æ±‚ | server_addr, request | ChatCompletion |
| `generate_per_replica()` | å• replica æ‰¹é‡ç”Ÿæˆ | addr, prompts | results |
| `generate()` | å¤š replica è´Ÿè½½å‡è¡¡ | addrs, prompts | all_results |

---

## å…­ã€é…ç½®å‚æ•°è¯¦è§£

### 6.1 RolloutConfig å…³é”®å‚æ•°

**æ–‡ä»¶ä½ç½®**ï¼š`verl/verl/workers/config/rollout.py`

```python
@dataclass
class RolloutConfig:
    # æ¨ç†å¼•æ“é€‰æ‹©
    name: str = "vllm"  # "vllm" æˆ– "sglang"
    mode: str = "async"  # ä»…æ”¯æŒ "async"

    # å¹¶è¡Œåº¦é…ç½®
    tensor_model_parallel_size: int = 2   # TP å¹¶è¡Œåº¦
    data_parallel_size: int = 1           # DP å¹¶è¡Œåº¦
    pipeline_model_parallel_size: int = 1 # PP å¹¶è¡Œåº¦

    # é‡‡æ ·å‚æ•°
    temperature: float = 1.0   # æ¸©åº¦ï¼ˆEVAL@1 ç”¨ 0.0ï¼‰
    top_p: float = 1.0
    top_k: int = -1
    n: int = 1                 # æ¯ prompt ç”Ÿæˆæ•°é‡

    # é•¿åº¦é™åˆ¶
    prompt_length: int = 4096
    response_length: int = 2048

    # å†…å­˜ä¸æ€§èƒ½
    gpu_memory_utilization: float = 0.8
    enforce_eager: bool = True           # ç¦ç”¨ CUDA Graph
    enable_prefix_caching: bool = True   # å¯ç”¨å‰ç¼€ç¼“å­˜
    enable_chunked_prefill: bool = True  # å¯ç”¨åˆ†å—é¢„å¡«å……

    # âš ï¸ å…³é”®å‚æ•°
    load_format: str = "auto"  # Standalone å¿…é¡»ç”¨ "auto"

    # æ‰¹å¤„ç†
    max_num_seqs: int = 256
    max_num_batched_tokens: int = 8192

    # æ—¥å¿—
    disable_log_stats: bool = True
```

### 6.2 load_format å‚æ•°ï¼ˆé‡ç‚¹ï¼‰

**è¿™æ˜¯ Standalone æ¨¡å¼æœ€å…³é”®çš„å‚æ•°**ï¼š

| load_format | è¯´æ˜ | é€‚ç”¨æ¨¡å¼ |
|-------------|------|---------|
| `"auto"` | ä»ç£ç›˜/HDFS è‡ªåŠ¨åŠ è½½æ¨¡å‹æƒé‡ | **STANDALONE** |
| `"dummy"` | åˆ›å»ºç©ºå£³æ¨¡å‹ï¼ˆæƒé‡ç”±è®­ç»ƒå¼•æ“åŒæ­¥ï¼‰ | HYBRID |
| `"safetensors"` | å¼ºåˆ¶ä½¿ç”¨ safetensors æ ¼å¼ | ç‰¹æ®Šæƒ…å†µ |

**Phase 0 å¿…é¡»ä½¿ç”¨ `load_format: "auto"`**ï¼Œå¦åˆ™ï¼š
- æ¨¡å‹æƒé‡ä¸ä¼šè¢«åŠ è½½
- æ¨ç†è¾“å‡ºä¸ºéšæœºå€¼
- è¯„æµ‹ç»“æœæ— æ„ä¹‰

### 6.3 Hydra é…ç½®æ–‡ä»¶ç¤ºä¾‹

```yaml
# config/phase0_eval.yaml

defaults:
  - _self_

trainer:
  n_gpus_per_node: 8
  nnodes: 1
  device: "cuda"

actor_rollout_ref:
  model:
    path: "Qwen/Qwen2.5-Coder-7B-Instruct"
    trust_remote_code: true
    lora_rank: 0

  rollout:
    name: "vllm"
    mode: "async"

    # å¹¶è¡Œåº¦
    tensor_model_parallel_size: 2
    data_parallel_size: 1
    pipeline_model_parallel_size: 1

    # EVAL@1 åè®®
    temperature: 0.0
    top_p: 1.0
    n: 1

    # é•¿åº¦
    prompt_length: 4096
    response_length: 2048

    # å†…å­˜
    dtype: "bfloat16"
    gpu_memory_utilization: 0.8

    # âš ï¸ å…³é”®ï¼šå¿…é¡»ä¸º "auto"
    load_format: "auto"

    # æ€§èƒ½
    enforce_eager: true
    enable_prefix_caching: true
    enable_chunked_prefill: true
    max_num_seqs: 256
    max_num_batched_tokens: 8192
    disable_log_stats: true

data:
  train_files:
    - "data/codecontests_valid.parquet"
  prompt_key: "prompt"
  output_path: "outputs/phase0/results.parquet"

ray_kwargs:
  ray_init:
    num_cpus: null
    runtime_env:
      env_vars:
        TOKENIZERS_PARALLELISM: "true"
        NCCL_DEBUG: "WARN"
        VLLM_USE_V1: "1"
```

---

## ä¸ƒã€å¸¸è§é—®é¢˜ä¸æ’æŸ¥

### 7.1 æ¨¡å‹æœªåŠ è½½ï¼ˆload_format é”™è¯¯ï¼‰

**ç—‡çŠ¶**ï¼š
- æ¨¡å‹è¾“å‡ºä¸ºéšæœº token
- æ‰€æœ‰è¯„æµ‹ç»“æœä¸º 0
- æ—¥å¿—æ˜¾ç¤º "Loading model with dummy weights"

**åŸå› **ï¼šä½¿ç”¨äº† `load_format: "dummy"`

**è§£å†³**ï¼šç¡®ä¿é…ç½®ä¸­ `load_format: "auto"`

```yaml
rollout:
  load_format: "auto"  # ä¸æ˜¯ "dummy"!
```

### 7.2 NCCL é”™è¯¯

**ç—‡çŠ¶**ï¼š
```
NCCL error: unhandled system error
```

**è§£å†³**ï¼š
1. æ£€æŸ¥ GPU é©±åŠ¨ç‰ˆæœ¬
2. è®¾ç½®ç¯å¢ƒå˜é‡ï¼š
```python
os.environ["NCCL_DEBUG"] = "INFO"  # è·å–æ›´å¤šä¿¡æ¯
os.environ["NCCL_P2P_DISABLE"] = "1"  # ç¦ç”¨ P2Pï¼ˆå¦‚æœæœ‰é—®é¢˜ï¼‰
```

### 7.3 SandboxFusion è¶…æ—¶

**ç—‡çŠ¶**ï¼š
```
aiohttp.ClientError: Connection timeout
```

**è§£å†³**ï¼š
1. ç¡®è®¤ SandboxFusion æœåŠ¡è¿è¡Œä¸­ï¼š
```bash
curl http://localhost:8080/health
```
2. å¢åŠ è¶…æ—¶æ—¶é—´ï¼š
```python
config=TestConfig(run_timeout=30)  # å¢åŠ åˆ° 30 ç§’
```
3. å‡å°‘å¹¶å‘è¯·æ±‚æ•°

### 7.4 å¥–åŠ±å…¨ä¸º 0

**ç—‡çŠ¶**ï¼š
- `compute_score()` æ€»æ˜¯è¿”å› 0
- æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹æ ‡è®°ä¸ºå¤±è´¥

**æ’æŸ¥æ­¥éª¤**ï¼š
1. æ£€æŸ¥ä»£ç æå–é€»è¾‘ï¼ˆæ˜¯å¦æ­£ç¡®å¤„ç† ```python``` å—ï¼‰
2. æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹æ ¼å¼ï¼š
```python
# æ­£ç¡®æ ¼å¼
test_cases = {
    "inputs": ["1 2\n", "3 4\n"],
    "outputs": ["3\n", "7\n"]
}
```
3. æ‰‹åŠ¨æµ‹è¯•å•ä¸ªç”¨ä¾‹ï¼š
```python
from sandbox_fusion import run_code, RunCodeRequest

result = run_code(RunCodeRequest(
    code="print(sum(map(int, input().split())))",
    stdin="1 2\n",
    language="python",
    run_timeout=10
))
print(result.stdout)  # åº”è¯¥æ˜¯ "3\n"
```

---

## é™„å½•

### A. å…³é”®æ–‡ä»¶ç´¢å¼•

| ç”¨é€” | æ–‡ä»¶è·¯å¾„ |
|------|----------|
| RolloutMode æšä¸¾ | `verl/verl/workers/rollout/replica.py:44-57` |
| RolloutReplica åŸºç±» | `verl/verl/workers/rollout/replica.py:60-210` |
| init_standalone() | `verl/verl/workers/rollout/replica.py:149-176` |
| get_rollout_replica_class() | `verl/verl/workers/rollout/replica.py:286-287` |
| vLLMReplica | `verl/verl/workers/rollout/vllm_rollout/vllm_async_server.py` |
| SGLangReplica | `verl/verl/workers/rollout/sglang_rollout/async_sglang_server.py` |
| main_generation_server.py | `verl/verl/trainer/main_generation_server.py` |
| compute_score() | `verl/verl/utils/reward_score/sandbox_fusion/__init__.py` |
| check_correctness() | `verl/verl/utils/reward_score/sandbox_fusion/utils.py` |
| RolloutConfig | `verl/verl/workers/config/rollout.py` |
| HFModelConfig | `verl/verl/workers/config/model.py` |

### B. æœ¯è¯­è¡¨

| æœ¯è¯­ | è‹±æ–‡ | è¯´æ˜ |
|------|------|------|
| Rollout | - | ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬åºåˆ—çš„è¿‡ç¨‹ |
| Replica | - | ä¸€ä¸ªç‹¬ç«‹çš„æ¨ç†æœåŠ¡å™¨å®ä¾‹ |
| TP | Tensor Parallelism | å¼ é‡å¹¶è¡Œï¼Œå°†æ¨¡å‹å±‚åˆ‡åˆ†åˆ°å¤š GPU |
| DP | Data Parallelism | æ•°æ®å¹¶è¡Œï¼Œç›¸åŒæ¨¡å‹å¤„ç†ä¸åŒæ•°æ® |
| PP | Pipeline Parallelism | æµæ°´çº¿å¹¶è¡Œï¼Œä¸åŒå±‚åœ¨ä¸åŒ GPU |
| Ray Actor | - | Ray æ¡†æ¶ä¸­çš„è¿œç¨‹å¯¹è±¡ï¼Œå¯è°ƒç”¨æ–¹æ³• |
| Placement Group | - | Ray ä¸­çš„èµ„æºåˆ†é…å•ä½ |
| ResourcePool | - | verl å¯¹ Placement Group çš„å°è£… |
| WorkerGroup | - | verl ä¸­ç®¡ç†ä¸€ç»„ Worker çš„ç±» |

### C. å‚è€ƒé“¾æ¥

- [verl å®˜æ–¹æ–‡æ¡£](https://github.com/volcengine/verl)
- [vLLM æ–‡æ¡£](https://docs.vllm.ai/)
- [SGLang æ–‡æ¡£](https://github.com/sgl-project/sglang)
- [Ray æ–‡æ¡£](https://docs.ray.io/)
- [SandboxFusion](https://github.com/bytedance/SandboxFusion)

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*
*åˆ›å»ºæ—¥æœŸï¼š2026-01-31*
*é€‚ç”¨ç‰ˆæœ¬ï¼šverl 0.7+, vLLM 0.8+*
