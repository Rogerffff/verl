# RLVR Coding Model Project - é¡¹ç›®ä»‹ç»ä¸æ–‡ä»¶ç»“æ„

---

## ä¸€ã€é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®åç§°
**RLVR Coding Post-Training (Offline DPO + Online GRPO with Verifiable Rewards)**

### é¡¹ç›®ç›®æ ‡
æ„å»ºä¸€ä¸ªç«¯åˆ°ç«¯çš„ LLM åè®­ç»ƒé—­ç¯ï¼Œä½¿ç”¨å¯éªŒè¯å¥–åŠ±ï¼ˆä»£ç åˆ¤é¢˜ï¼‰å®Œæˆï¼š
**SFT â†’ (ç¦»çº¿ DPO) â†’ åœ¨çº¿ GRPO â†’ å¤šè½®ä¿®å¤**

äº§å‡ºå·¥ä¸šç•Œè®¤å¯çš„"åè®­ç»ƒæµç¨‹ + å¯ä¿¡è¯„æµ‹ + æˆæœ¬/ç¨³å®šæ€§é¢æ¿"ï¼Œä½œä¸ºç®€å†é¡¹ç›®å±•ç¤º RL/LLM åè®­ç»ƒèƒ½åŠ›ã€‚

### æŠ€æœ¯æ ˆ
- **è®­ç»ƒæ¡†æ¶**: verl (åˆ†å¸ƒå¼ RL è®­ç»ƒæ¡†æ¶)
- **æ¨ç†å¼•æ“**: vLLM (é«˜æ€§èƒ½ LLM æ¨ç†)
- **ä»£ç è¯„æµ‹**: SandboxFusion (å®‰å…¨æ²™ç›’æ‰§è¡Œç¯å¢ƒ)
- **åŸºç¡€æ¨¡å‹**: Qwen2.5-Coder-7B-Instruct
- **å®éªŒè¿½è¸ª**: Weights & Biases (WandB)

### äº”é˜¶æ®µè®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            è®­ç»ƒæµç¨‹æ€»è§ˆ                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Phase 0          Phase 1         Phase 2           Phase 3      Phase 4  â”‚
â”‚   Baseline    â†’      SFT      â†’   DPO (å¯é€‰)    â†’    GRPO     â†’  å¤šè½®ä¿®å¤   â”‚
â”‚     â”‚                 â”‚               â”‚               â”‚           (å¯é€‰)    â”‚
â”‚     â†“                 â†“               â†“               â†“             â†“       â”‚
â”‚   å»ºç«‹åŸºçº¿       æå‡æ ¼å¼/æ‰§è¡Œ    åå¥½å¯¹é½å†·å¯åŠ¨    åœ¨çº¿RLä¼˜åŒ–    Agenticæ‰©å±• â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

| é˜¶æ®µ | ç›®æ ‡ | æ ¸å¿ƒæŒ‡æ ‡ | çŠ¶æ€ |
|------|------|----------|------|
| Phase 0 | å»ºç«‹åŸºçº¿ | accepted@1, pass_ratio, error_breakdown | âœ… å·²å®Œæˆ |
| Phase 1 | é™ä½ä½çº§é”™è¯¯ | exec_success_rate â†‘, syntax_error_rate â†“ | å¾…å®ç° |
| Phase 2 | åå¥½å¯¹é½å†·å¯åŠ¨ | pass_ratio â†‘, zero_reward_rate â†“ | å¯é€‰ |
| Phase 3 | åœ¨çº¿ RL ä¼˜åŒ– | CodeContests_test accepted@1 +3~10pp | æ ¸å¿ƒé˜¶æ®µ |
| Phase 4 | å¤šè½®ä¿®å¤ | recovery_rate 20~40% | åŠ åˆ†é¡¹ |

---

## äºŒã€æ•°æ®é›†è§’è‰²å®šä¹‰

| æ•°æ®é›† | è§’è‰² | é¢˜ç›®æ•° | ä½¿ç”¨è§„åˆ™ |
|--------|------|--------|----------|
| **CodeContests_train** | Train | ~13k | è®­ç»ƒ/æ„é€ åå¥½å¯¹ |
| **CodeContests_valid** | Dev/Val | 117 | é«˜é¢‘å›å½’ã€æ—©åœã€é€‰è¶…å‚ |
| **CodeContests_test** | Test | - | é˜¶æ®µç»“æŸè¯„æµ‹ï¼Œç¦æ­¢è®­ç»ƒ/è°ƒå‚ |
| **HumanEval** | Test only | 164 | è¡Œä¸šå¯¹æ ‡ï¼Œç¦æ­¢è®­ç»ƒ |
| **MBPP_reg** | Dev/Val | 100-200 | å¿«é€Ÿå›å½’ç›‘æ§ |

---

## ä¸‰ã€å®Œæ•´æ–‡ä»¶ç»“æ„

```
verl/coding_model_project/
â”‚
â”œâ”€â”€ experiment_design/                    # ğŸ“‹ æ ¸å¿ƒå®éªŒè®¾è®¡æ–‡æ¡£
â”‚   â”œâ”€â”€ final_experiment_design.md        # â˜… å®Œæ•´äº”é˜¶æ®µå®éªŒè®¾è®¡ï¼ˆæœ€æƒå¨å‚è€ƒï¼‰
â”‚   â”œâ”€â”€ project_plan.md                   # ç®€å†ç‰ˆæœ¬é¡¹ç›®è®¡åˆ’
â”‚   â”œâ”€â”€ eval_protocol.md                  # è¯„æµ‹åè®®å®šä¹‰ï¼ˆEVAL@1/k/@budgetï¼‰
â”‚   â”œâ”€â”€ data_governance.md                # æ•°æ®æ²»ç†åŸåˆ™ï¼ˆå»é‡/æ³„æ¼æ£€æŸ¥ï¼‰
â”‚   â”œâ”€â”€ reward_design.md                  # å¥–åŠ±å‡½æ•°è®¾è®¡ï¼ˆDense vs Sparseï¼‰
â”‚   â”œâ”€â”€ guardrails.md                     # é˜² reward hacking çº¦æŸ
â”‚   â”œâ”€â”€ grpo_minimal_hparams.md           # GRPO è¶…å‚æœ€å°é›†
â”‚   â”œâ”€â”€ metric_templates.md               # æŒ‡æ ‡å®šä¹‰æ¨¡æ¿
â”‚   â””â”€â”€ resource_plan.md                  # GPU èµ„æºè§„åˆ’
â”‚
â”œâ”€â”€ phase_0_ Baseline/                    # ğŸ“Š Phase 0 åŸºçº¿è¯„æµ‹
â”‚   â”œâ”€â”€ PARAMETERS.md                     # Phase 0 å‚æ•°è¯´æ˜
â”‚   â”œâ”€â”€ phase0_implementation_plan.md     # è¯¦ç»†å®æ–½è®¡åˆ’ï¼ˆ1900+ è¡Œï¼‰
â”‚   â”œâ”€â”€ verl_standalone_rollout_guide.md  # verl æ¶æ„æ·±åº¦è®²è§£
â”‚   â”œâ”€â”€ data_governance_guide.md          # æ•°æ®æ²»ç†å®Œæ•´æŒ‡å—
â”‚   â”œâ”€â”€ metrics_collection_spec.md        # æŒ‡æ ‡æ”¶é›†è§„èŒƒ
â”‚   â””â”€â”€ implement_explain_doc/            # ä»£ç è®²è§£æ–‡æ¡£
â”‚       â”œâ”€â”€ 01_data_governance.md
â”‚       â”œâ”€â”€ 02_eval_script.md
â”‚       â”œâ”€â”€ 03_output_files_and_metrics.md
â”‚       â””â”€â”€ eval_scriptä»£ç è®²è§£.md
â”‚
â”œâ”€â”€ src/                                  # ğŸ’» æ ¸å¿ƒå®ç°ä»£ç 
â”‚   â”œâ”€â”€ phase0_eval.py                    # â˜… Phase 0 è¯„æµ‹ä¸»è„šæœ¬ï¼ˆ2000+ è¡Œï¼‰
â”‚   â”‚                                     # åŠŸèƒ½ï¼švLLM æœåŠ¡å™¨å¯åŠ¨ã€ä»£ç ç”Ÿæˆã€
â”‚   â”‚                                     # SandboxFusion è¯„æµ‹ã€æŒ‡æ ‡æ”¶é›†
â”‚   â”‚
â”‚   â”œâ”€â”€ eval_config.py                    # è¯„æµ‹å¸¸é‡ä¸é…ç½®ï¼ˆ280+ è¡Œï¼‰
â”‚   â”‚                                     # ç®¡ç†ï¼šEVAL_CONSTANTSã€DATASET_CONFIGSã€
â”‚   â”‚                                     # CONCURRENCY_CONFIGS
â”‚   â”‚
â”‚   â”œâ”€â”€ data_governance.py                # æ•°æ®æ²»ç†è„šæœ¬ï¼ˆæ•°æ®è·å–+å»é‡+éªŒè¯ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ metrics.py                    # æŒ‡æ ‡æ”¶é›†å™¨ï¼ˆMetricsCollectorï¼‰
â”‚   â”‚   â”‚                                 # åŒ…å«ï¼šEvalResultã€DatasetMetrics
â”‚   â”‚   â””â”€â”€ qa_logger.py                  # é—®ç­”æ—¥å¿—ï¼ˆåˆ†å±‚æŠ½æ ·ä¿å­˜ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ phase0_config.yaml            # YAML é…ç½®æ–‡ä»¶
â”‚   â”‚
â”‚   â””â”€â”€ temp/                             # ä¸´æ—¶è„šæœ¬ç›®å½•
â”‚       â”œâ”€â”€ test_sandbox_eval.py
â”‚       â”œâ”€â”€ verify_dedup.py
â”‚       â””â”€â”€ add_mbpp_entry_point.py
â”‚
â”œâ”€â”€ data/                                 # ğŸ“ æ•°æ®ç®¡ç†
â”‚   â”œâ”€â”€ manifests/                        # å»é‡åæ•°æ®çš„ç´¢å¼•æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ humaneval_manifest.jsonl
â”‚   â”‚   â”œâ”€â”€ mbpp_reg_manifest.jsonl
â”‚   â”‚   â”œâ”€â”€ codecontests_train_manifest.jsonl
â”‚   â”‚   â”œâ”€â”€ codecontests_valid_manifest.jsonl
â”‚   â”‚   â”œâ”€â”€ codecontests_test_manifest.jsonl
â”‚   â”‚   â””â”€â”€ codecontests_*_duplicates_intrasplit.jsonl
â”‚   â”‚
â”‚   â””â”€â”€ raw/                              # åŸå§‹æ•°æ®æ–‡ä»¶
â”‚       â”œâ”€â”€ humaneval_raw.jsonl
â”‚       â”œâ”€â”€ mbpp_reg_raw.jsonl
â”‚       â”œâ”€â”€ codecontests_train_raw.jsonl
â”‚       â”œâ”€â”€ codecontests_valid_raw.jsonl
â”‚       â”œâ”€â”€ codecontests_test_raw.jsonl
â”‚       â””â”€â”€ dataset_samples.jsonl
â”‚
â”œâ”€â”€ outputs/                              # ğŸ“ˆ è¯„æµ‹ç»“æœè¾“å‡º
â”‚   â””â”€â”€ phase0_YYYYMMDD_HHMMSS/           # æ—¶é—´æˆ³è¾“å‡ºç›®å½•
â”‚       â”œâ”€â”€ metrics.json                  # ä¸»è¦æŒ‡æ ‡ï¼ˆæŒ‰æ•°æ®é›†èšåˆï¼‰
â”‚       â”œâ”€â”€ summary.json                  # å®Œæ•´æ±‡æ€»
â”‚       â”œâ”€â”€ run_info.json                 # å¯å®¡è®¡çš„è¿è¡Œä¿¡æ¯
â”‚       â””â”€â”€ qa_logs/                      # é—®ç­”æ—¥å¿—ï¼ˆåˆ†å±‚æŠ½æ ·ï¼‰
â”‚           â”œâ”€â”€ humaneval_qa.jsonl
â”‚           â”œâ”€â”€ mbpp_reg_qa.jsonl
â”‚           â”œâ”€â”€ codecontests_valid_qa.jsonl
â”‚           â””â”€â”€ qa_summary.json
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_phase0.sh                     # Phase 0 å¯åŠ¨è„šæœ¬
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ data_audit_report.md              # æ•°æ®æ²»ç†å®¡è®¡æŠ¥å‘Š
â”‚
â”œâ”€â”€ phase_1_sft/                          # Phase 1 SFT ç›®å½•ï¼ˆå¾…å®ç°ï¼‰
â”‚
â”œâ”€â”€ verlåŸºç¡€è®²è§£/                         # ğŸ“š å­¦ä¹ èµ„æ–™
â”‚   â”œâ”€â”€ 01_ä»RayWorkerGroupåˆ°Replica.md
â”‚   â””â”€â”€ claude.md
â”‚
â”œâ”€â”€ agent.md                              # æœ¬æ–‡ä»¶ - é¡¹ç›®ä»‹ç»
â””â”€â”€ claude.md                             # AI åŠ©æ‰‹ä¸Šä¸‹æ–‡è¯´æ˜
```

---

## å››ã€æ ¸å¿ƒæ–‡ä»¶è¯¦è§£

### 4.1 è¯„æµ‹ä¸»è„šæœ¬ `src/phase0_eval.py`

**åŠŸèƒ½æ¶æ„**ï¼š

```python
# ç¬¬1å±‚ï¼šé…ç½®ä¸æ•°æ®
class EvalConfig           # è¯„æµ‹é…ç½®æ•°æ®ç±»
SYSTEM_PROMPT             # LLM ç³»ç»Ÿæç¤º
PROMPT_TEMPLATES          # ä¸åŒæ•°æ®é›†çš„ prompt æ¨¡æ¿

# ç¬¬2å±‚ï¼šæœåŠ¡å™¨ç®¡ç†
async start_rollout_servers()      # å¯åŠ¨ verl Standalone æœåŠ¡å™¨
async fetch_openai_models()        # æŸ¥è¯¢æœåŠ¡ç«¯çš„å®é™…æ¨¡å‹

# ç¬¬3å±‚ï¼šä»£ç ç”Ÿæˆ
async generate_code()              # å•ä¸ªä»£ç ç”Ÿæˆï¼ˆasyncï¼‰
async batch_generate()             # æ‰¹é‡å¹¶å‘ç”Ÿæˆï¼ˆè´Ÿè½½å‡è¡¡ï¼‰

# ç¬¬4å±‚ï¼šä»£ç è¯„æµ‹
evaluate_with_submit_api()         # SandboxFusion submit() è¯„æµ‹
evaluate_with_run_code()           # SandboxFusion run_code() è¯„æµ‹

# ç¬¬5å±‚ï¼šæ•°æ®åŠ è½½
load_prompts()                     # ä» manifest æˆ– SandboxFusion åŠ è½½

# ç¬¬6å±‚ï¼šä¸»è¯„æµ‹æµç¨‹
async evaluate_dataset()           # è¯„æµ‹å•ä¸ªæ•°æ®é›†
async run_evaluation()             # å®Œæ•´è¯„æµ‹é—­ç¯
```

**å…³é”®è®¾è®¡**ï¼š
- **Async å¹¶å‘ç”Ÿæˆ**ï¼šä½¿ç”¨ `asyncio.Semaphore` é™åˆ¶å¹¶å‘æ•°ï¼ˆé»˜è®¤64ï¼‰
- **Round-Robin è´Ÿè½½å‡è¡¡**ï¼šå°†è¯·æ±‚å‡åŒ€åˆ†å‘åˆ°å¤šä¸ª vLLM replica
- **å¤šç§è¯„æµ‹æ–¹å¼æ”¯æŒ**ï¼š`submit()` / `run_code()` / `compute_score()`
- **é”™è¯¯ç±»å‹è‡ªåŠ¨åˆ†ç±»**ï¼šsyntax/runtime/timeout/wrong_answer
- **å¯å®¡è®¡çš„è¿è¡Œä¿¡æ¯**ï¼šè®°å½•é…ç½®ã€æœåŠ¡å™¨åœ°å€ã€å®é™…åŠ è½½çš„æ¨¡å‹ ID

### 4.2 æŒ‡æ ‡æ”¶é›†å™¨ `src/utils/metrics.py`

```python
class EvalResult:              # å•ä¸ªé—®é¢˜çš„è¯„æµ‹ç»“æœ
  problem_id, accepted, pass_ratio, error_type, judge_time, gen_tokens

class DatasetMetrics:          # æ•°æ®é›†çº§èšåˆæŒ‡æ ‡
  # è´¨é‡ï¼šaccepted_at_1, pass_ratio_mean/p50/p90, exec_success_rate
  # é”™è¯¯åˆ†å¸ƒï¼šå„é”™è¯¯ç±»å‹çš„æ¯”ä¾‹
  # æˆæœ¬ï¼šavg_judge_time, p50/p95_judge_time, avg_gen_tokens, throughput
  # æˆæœ¬æ¯”ç‡ï¼šcost_per_solved_tokens/judge_time

class MetricsCollector:        # æŒ‡æ ‡æ”¶é›†å™¨
  def add_result()             # é€ä¸ªæ·»åŠ è¯„æµ‹ç»“æœ
  def get_dataset_metrics()    # è®¡ç®—èšåˆæŒ‡æ ‡ï¼ˆnumpy ç»Ÿè®¡ï¼‰
  def get_summary()            # å¤šæ•°æ®é›†æ±‡æ€»
  def get_wandb_metrics()      # è¿”å› WandB æ ¼å¼å­—å…¸
```

### 4.3 è¯„æµ‹é…ç½® `src/eval_config.py`

```python
EVAL_CONSTANTS = {
    "temperature": 0.0,         # EVAL@1 åè®®ï¼ˆgreedy decodingï¼‰
    "top_p": 1.0,
    "max_new_tokens": 2048,
    "run_timeout": 30,          # SandboxFusion è¶…æ—¶
    "memory_limit_mb": 1024,
}

DATASET_CONFIGS = {
    "humaneval": {...},
    "mbpp_reg": {...},
    "codecontests_valid": {...},
    ...
}
```

---

## äº”ã€Phase 0 å®é™…è¿è¡Œç»“æœ

| æ•°æ®é›† | accepted@1 | pass_ratio_mean | exec_success_rate | avg_gen_tokens | throughput |
|--------|------------|-----------------|-------------------|----------------|------------|
| HumanEval | 87.2% | 0.872 | 100% | 122.5 | 5.85 é—®é¢˜/ç§’ |
| MBPP_reg | 58.0% | 0.58 | 100% | 54.1 | 7.9 é—®é¢˜/ç§’ |
| CodeContests_valid | 3.4% | 0.120 | 88.9% | 267.4 | 0.17 é—®é¢˜/ç§’ |

**é”™è¯¯åˆ†å¸ƒï¼ˆCodeContests_validï¼‰**ï¼š
- Wrong Answer: ~70%
- Timeout: 10.3%
- Runtime Error: ~10%
- Syntax Error: <1%

---

## å…­ã€ç®€å†å†™æ³•å»ºè®®

### Bullet Points ç¤ºä¾‹

1. **ç«¯åˆ°ç«¯é—­ç¯**
   - Built an end-to-end LLM post-training pipeline with verifiable rewards from judging feedback
   - Implemented SFT â†’ (optional offline DPO) â†’ online GRPO with strict train/dev/test isolation

2. **è´¨é‡æ”¹è¿›æ•°æ®**
   - Improved CodeContests_test accepted@1 from X% â†’ Y% (2 seeds, meanÂ±std)
   - Reduced syntax/runtime errors by X% through SFT phase

3. **å…³é”®æ¶ˆèä¸ç¨³å®šæ€§**
   - Conducted critical ablations: dense (pass_ratio) vs sparse (accepted) reward signals
   - Demonstrated faster convergence and lower variance with dense rewards

4. **å·¥ç¨‹ç»†èŠ‚**
   - Implemented async code generation + load balancing, achieving 5.8x throughput improvement
   - Established auditable data isolation to prevent train/test leakage

---

## ä¸ƒã€å¿«é€Ÿå¼€å§‹

```bash
# 1. å¯åŠ¨ vLLM æœåŠ¡
docker run --gpus all -p 8000:8000 \
  vllm/vllm-openai:latest \
  --model Qwen/Qwen2.5-Coder-7B-Instruct

# 2. å¯åŠ¨ SandboxFusion æœåŠ¡
docker run -p 8080:8080 volcengine/sandbox-fusion:server-20250609

# 3. è¿è¡Œ Phase 0 è¯„æµ‹
python src/phase0_eval.py \
    --mode simple \
    --model Qwen/Qwen2.5-Coder-7B-Instruct \
    --vllm_url http://localhost:8000 \
    --sandbox_url http://localhost:8080 \
    --datasets humaneval mbpp_reg codecontests_valid \
    --output_dir outputs/phase0
```

---

## å…«ã€å…³é”®æ–‡æ¡£å¯¼èˆª

| æ–‡æ¡£ | ç”¨é€” | è·¯å¾„ |
|------|------|------|
| **final_experiment_design.md** | å®Œæ•´äº”é˜¶æ®µè®¾è®¡ï¼ˆæœ€æƒå¨ï¼‰ | experiment_design/ |
| **project_plan.md** | ç®€å†ç‰ˆæœ¬ï¼ˆHR çœ‹è¿™ä¸ªï¼‰ | experiment_design/ |
| **phase0_implementation_plan.md** | Phase 0 è¯¦ç»†æ‰§è¡Œ | phase_0_ Baseline/ |
| **verl_standalone_rollout_guide.md** | verl æ¶æ„æ·±åº¦è®²è§£ | phase_0_ Baseline/ |
| **eval_protocol.md** | EVAL@1/k/@budget å£å¾„å¯¹é½ | experiment_design/ |
| **data_governance.md** | æ•°æ®å»é‡+æ³„æ¼æ£€æŸ¥ | experiment_design/ |
