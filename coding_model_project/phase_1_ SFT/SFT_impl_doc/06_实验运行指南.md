# Phase 1 SFT 实验运行指南

本文档详细介绍如何在 vast.ai 云 GPU 平台上运行 Phase 1 SFT 训练和评测实验。

---

## 重要：评测参数一致性

**所有评测参数统一管理在 `src/eval_config.py` 中**，确保 Baseline → SFT → RL 各阶段评测一致性：

| 参数 | 值 | 说明 |
|------|-----|------|
| `temperature` | 0.0 | Greedy decoding (EVAL@1 协议) |
| `top_p` | 1.0 | Nucleus sampling 参数 |
| `max_new_tokens` | 2048 | 最大生成 token 数 |
| `run_timeout` | **30s** | 代码执行超时 |
| `memory_limit_mb` | 1024 | 内存限制 (1GB) |

---

## 目录

1. [环境要求](#1-环境要求)
2. [vast.ai 实例配置](#2-vastai-实例配置)
3. [环境配置](#3-环境配置)
4. [数据准备](#4-数据准备)
5. [烟雾测试](#5-烟雾测试)
6. [正式训练](#6-正式训练)
7. [检查点评测](#7-检查点评测)
8. [结果分析与下载](#8-结果分析与下载)
9. [常见问题排查](#9-常见问题排查)
10. [成本估算](#10-成本估算)

---

## 1. 环境要求

### 1.1 GPU 显存需求（训练）

**模型**: Qwen/Qwen2.5-Coder-7B-Instruct (7B 参数, FSDP2 全分片)

| 组件 | 大小 | 说明 |
|------|------|------|
| 模型参数 (bf16) | ~14 GB | 7B × 2 bytes |
| AdamW 优化器 (fp32 master + momentum + variance) | ~84 GB | 全部分片到各 GPU |
| **每 GPU 分片需求** | **~84 GB / N_gpu** | |

| 配置 | 每 GPU 分片 | 剩余显存 | micro_batch | 可行性 |
|------|------------|---------|-------------|--------|
| 4× 4090 (24GB) | 21 GB | 3 GB | 1 | 勉强，容易 OOM |
| **4× 5090 (32GB)** | **21 GB** | **11 GB** | **2** | **推荐** |
| 2× 6000 Pro (96GB) | 42 GB | 54 GB | 4-8 | 稳定但并行度低 |

### 1.2 推荐 GPU 配置

**首选：4× RTX 5090 (32GB)**

- 11GB headroom 足够 gradient checkpointing + micro_batch=2 + max_length=4096
- 4 路数据并行提供合理训练速度
- 预计训练时间：2-4 小时（3 epoch, ~378 steps）

**备选：2× RTX 6000 Pro (96GB)**

- 显存极充裕，micro_batch 可开到 4-8
- 但仅 2 路并行，训练速度较慢
- 需调整 `run_sft.sh` 参数：`bash run_sft.sh 2`

**不推荐：4× RTX 4090 (24GB)**

- 仅 3GB headroom，micro_batch 只能设 1
- 如必须使用，修改：`data.micro_batch_size_per_gpu=1`，`data.max_length=2048`

### 1.3 软件依赖

| 软件 | 版本 | 用途 |
|------|------|------|
| Python | >= 3.10 | 运行环境 |
| CUDA | >= 12.8 | GPU 计算 + FSDP2 |
| PyTorch | >= 2.8.0 | 训练框架 |
| vLLM | >= 0.11.0 | 评测阶段模型推理 |
| verl | 源码安装 | SFT 训练框架 |
| Docker | 20.10+ | SandboxFusion (可选) |

---

## 2. vast.ai 实例配置

### 2.1 实例搜索条件

```
GPU Model: RTX 5090 (首选) / RTX 6000 Pro
GPU Count: 4 (5090) 或 2 (6000 Pro)
GPU RAM: >= 32 GB per GPU
Disk Space: >= 200 GB (模型 + checkpoints + 数据)
CPU: >= 16 cores
RAM: >= 64 GB
CUDA Version: >= 12.8
```

> **磁盘空间说明**：每个 checkpoint（含 HF 模型导出）约 30GB，保存 4 个 checkpoint 需要 ~120GB；加上模型权重 (~14GB) 和工作空间，建议至少 200GB。

### 2.2 Docker 模板选择

#### 方式 A：使用 verl 官方镜像（强烈推荐）

verl 官方提供了预装 FSDP、vLLM、PyTorch 的 Docker 镜像，**最大程度节省配置时间**。

| 镜像 | CUDA | PyTorch | vLLM | 推荐度 |
|------|------|---------|------|--------|
| `verlai/verl:vllm012.latest` | 12.9.1 | 2.9.0 | 0.12.0 | 最新稳定 |
| `verlai/verl:vllm011.latest` | 12.8 | 2.8.0 | 0.11.0 | 稳定验证 |

vast.ai 上使用方式：在 "Template" 中选 "Edit Image & Config"，输入镜像名：

```
verlai/verl:vllm012.latest
```

**优势**：
- PyTorch + CUDA + vLLM + Flash Attention 全部预装且版本匹配
- 只需安装 verl 源码本身（`pip install --no-deps -e .`）
- 训练和评测可在同一环境完成
- 省去 1-2 小时的环境调试时间

#### 方式 B：使用 vast.ai vLLM 模板（可接受）

选择 vast.ai 官方 vLLM 模板：
- `vastai/vllm:v0.10.2-cuda-12.8-pytorch-2.8.0-py312`

**注意**：此模板预装了 vLLM 和 PyTorch，但缺少：
- Flash Attention（需手动 `pip install flash-attn`）
- verl 框架（需源码安装）
- 可能需要额外调试 FSDP2 兼容性

#### 不推荐：纯 PyTorch 模板

使用 `vastai/pytorch` 需要手动安装 vLLM、Flash Attention 等大量依赖，容易遇到版本冲突。

### 2.3 实例配置参数

| 参数 | 推荐值 | 说明 |
|------|--------|------|
| Disk Space | 200+ GB | checkpoints 需要大量空间 |
| Docker Image | `verlai/verl:vllm012.latest` | verl 官方镜像 |
| On-start Script | 留空 | 手动配置更灵活 |
| SSH | 启用 | 远程连接 |

---

## 3. 环境配置

### 3.1 连接实例并验证

```bash
# SSH 连接（替换为你的实例信息）
ssh -p <PORT> root@<HOST>

# 验证 GPU
nvidia-smi

# 验证 CUDA 和 PyTorch
python -c "import torch; print(f'PyTorch: {torch.__version__}, CUDA: {torch.version.cuda}, GPUs: {torch.cuda.device_count()}')"

# 验证 vLLM
python -c "import vllm; print(f'vLLM: {vllm.__version__}')"

# 验证磁盘空间
df -h
```

### 3.2 安装系统工具

```bash
apt update && apt install -y git tmux htop
```

### 3.3 克隆代码仓库

```bash
mkdir -p /workspace
cd /workspace

# 克隆你的仓库（包含 verl 源码 + coding_model_project）
git clone https://github.com/<your-username>/<your-repo>.git verl
cd verl

# 切换到 SFT 开发分支
git checkout feature/sft-development
```

### 3.4 安装 verl

```bash
cd /workspace/verl

# 使用 --no-deps 避免覆盖已安装的 PyTorch/vLLM
pip install --no-deps -e .

# 验证安装
python -c "import verl; print('verl installed successfully')"
```

### 3.5 安装额外 Python 依赖

```bash
# SFT 训练所需
pip install wandb hydra-core tensordict pyarrow pandas numpy

# 评测所需
pip install aiohttp sandbox-fusion

# 登录 WandB（可选，用于训练监控）
wandb login
```

### 3.6 安装 SandboxFusion（评测用）

评测阶段需要 SandboxFusion 来执行和测试模型生成的代码。两种方式任选：

#### 方式 A：源码安装（推荐，已修复已知 bug）

```bash
# 安装 Miniconda（如果 verl 镜像中没有 conda）
# verl 官方镜像通常自带 conda，先检查：
which conda || {
    cd /tmp
    wget -q https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
    bash miniconda.sh -b -p /opt/conda
    rm miniconda.sh
    export PATH="/opt/conda/bin:$PATH"
    conda init bash
    source ~/.bashrc
}

# 克隆修复过的 SandboxFusion
cd /workspace
git clone https://github.com/Rogerffff/sandbox.git
cd sandbox

# 创建服务端环境
conda create -n sandbox-server python=3.12 -y
source $(conda info --base)/etc/profile.d/conda.sh
conda activate sandbox-server
pip install poetry
poetry install
mkdir -p docs/build

# 创建运行时环境（必须）
conda create -n sandbox-runtime python=3.11 -y
```

> **重要**：使用 Rogerffff/sandbox 仓库，其中已修复 `find_conda_root()` 路径查找 bug 和 SDK `memory_limit_MB` 字段缺失 bug（详见 `intro_doc/07-部署问题修复记录.md`）。

#### 方式 B：Docker 部署（快速但需要 Docker-in-Docker）

```bash
docker run -d \
    --name sandbox_fusion \
    --rm \
    --privileged \
    -p 8080:8080 \
    volcengine/sandbox-fusion:server-20250609
```

> 注意：vast.ai 容器中运行 Docker-in-Docker 需要实例支持 `--privileged` 模式。

#### 验证 SandboxFusion

```bash
curl http://localhost:8080/health

curl -X POST http://localhost:8080/run_code \
    -H "Content-Type: application/json" \
    -d '{"code": "print(1 + 1)", "language": "python", "run_timeout": 10}'
# 预期: {"status": "Success", "run_result": {"stdout": "2\n", ...}}
```

### 3.7 验证完整环境

```bash
echo "=== Environment Check ==="

echo "1. PyTorch + GPU:"
python -c "import torch; print(f'  GPUs: {torch.cuda.device_count()}, CUDA: {torch.version.cuda}')"

echo "2. verl:"
python -c "import verl; print('  OK')"

echo "3. vLLM:"
python -c "import vllm; print(f'  {vllm.__version__}')"

echo "4. WandB:"
python -c "import wandb; print(f'  {wandb.__version__}')" 2>/dev/null || echo "  Not installed (optional)"

echo "5. SandboxFusion:"
curl -s http://localhost:8080/health && echo "  OK" || echo "  Not running (needed for eval only)"
```

---

## 4. 数据准备

### 4.1 预期数据结构

```
coding_model_project/
├── phase_1_ SFT/
│   ├── data/
│   │   ├── sft_train.parquet      # 2,020 条训练数据 (需 scp 上传)
│   │   ├── sft_val.parquet        # 200 条验证数据 (需 scp 上传)
│   │   └── split_mapping.json     # 数据分割映射 (已在 git 中)
│   ├── bee_hq_python_deduped_filtered/
│   │   └── sft_validated.jsonl    # 原始数据 (2,220 条)
│   ├── prepare_sft_data.py        # 数据预处理脚本
│   ├── run_sft.sh                 # 训练脚本
│   ├── run_smoke_test.sh          # 烟雾测试脚本
│   ├── phase1_eval.py             # 检查点评测脚本
│   └── run_eval_all.sh            # 批量评测脚本
├── data/
│   ├── manifests/                 # 评测数据 manifest (已在 git 中)
│   └── raw/                       # 评测原始数据 (不在 git 中, 需 scp 上传)
└── src/
    ├── phase0_eval.py             # 评测脚本 (Phase 0/1 共用)
    └── eval_config.py             # 评测配置
```

### 4.2 上传本地数据文件

以下数据文件不在 git 中（被 `.gitignore` 排除），需要通过 scp 从本地上传到服务器。

```bash
# 在服务器上创建目录
ssh -p <PORT> root@<HOST> "mkdir -p /workspace/verl/coding_model_project/phase_1_\ SFT/data /workspace/verl/coding_model_project/data/raw"

# 上传 SFT 训练数据 (parquet + split_mapping)
scp -P <PORT> \
    /path/to/local/verl/coding_model_project/phase_1_\ SFT/data/sft_train.parquet \
    /path/to/local/verl/coding_model_project/phase_1_\ SFT/data/sft_val.parquet \
    /path/to/local/verl/coding_model_project/phase_1_\ SFT/data/split_mapping.json \
    root@<HOST>:"/workspace/verl/coding_model_project/phase_1_ SFT/data/"

# 上传评测原始数据 (raw jsonl)
scp -P <PORT> \
    /path/to/local/verl/coding_model_project/data/raw/*_raw.jsonl \
    root@<HOST>:/workspace/verl/coding_model_project/data/raw/
```

验证上传完整性：

```bash
# 验证训练数据
ls -lh "/workspace/verl/coding_model_project/phase_1_ SFT/data/"
# 预期: sft_train.parquet (~2.1MB), sft_val.parquet (~243KB), split_mapping.json

# 验证评测数据
ls -lh /workspace/verl/coding_model_project/data/raw/*_raw.jsonl
# 预期: codecontests_valid_raw.jsonl, mbpp_reg_raw.jsonl, codecontests_valid_big_raw.jsonl,
#        codecontests_test_raw.jsonl, humaneval_raw.jsonl
```

---

## 5. 烟雾测试

在正式训练前，用 1 GPU + 10 步验证整个流水线。

### 5.1 运行烟雾测试

```bash
cd "/workspace/verl/coding_model_project/phase_1_ SFT"

bash run_smoke_test.sh
```

### 5.2 预期输出

```
============================================
Phase 1 SFT - Smoke Test
  1 GPU, 10 steps
============================================

... (训练日志) ...

  [PASS] HF model checkpoint found
Smoke test passed! Ready for full training.
```

### 5.3 烟雾测试验证清单

- [ ] Loss 在 10 步内下降（从 ~2-3 降至 ~1.5-2）
- [ ] checkpoint 在 step 5 和 10 处保存
- [ ] `huggingface/` 目录在 checkpoint 中存在（含 `config.json`）
- [ ] `grad_norm` 出现在训练日志中
- [ ] 无 OOM 或 NCCL 错误

### 5.4 烟雾测试失败排查

**OOM 错误**：降低 `data.micro_batch_size_per_gpu` 到 1，或 `data.max_length` 到 2048

**NCCL 错误**：检查多 GPU 通信
```bash
# 测试 NCCL
python -c "import torch.distributed as dist; dist.init_process_group('nccl'); print('NCCL OK')"
```

**tokenization 警告**：如果出现 `ignore_input_ids_mismatch` 相关警告，通常可以忽略，不影响训练。

---

## 6. 正式训练

### 6.1 启动训练

```bash
cd "/workspace/verl/coding_model_project/phase_1_ SFT"

# 使用 tmux 保持后台运行
tmux new -s sft_train

# 4x 5090 训练
bash run_sft.sh 4

# 或 2x 6000 Pro
# bash run_sft.sh 2

# 分离 tmux: Ctrl+B, D
```

### 6.2 训练参数概览

| 参数 | 值 | 说明 |
|------|-----|------|
| 模型 | Qwen/Qwen2.5-Coder-7B-Instruct | 基座模型 |
| 策略 | FSDP2 (bf16) | 全分片训练 |
| global_batch_size | 16 | |
| micro_batch_size_per_gpu | 2 | |
| gradient_accumulation | 2 | 16 / (4 GPU x 2 micro_batch) |
| max_length | 4096 | |
| 学习率 | 2e-5, cosine decay | |
| warmup | 5% of total steps | |
| clip_grad | 1.0 | |
| weight_decay | 0.01 | |
| 训练数据 | 2,020 条 | |
| 验证数据 | 200 条 | |
| epochs | 3 | |
| **total_steps** | **~378** | 2020 / 16 x 3 |
| save_freq | 100 步 | |
| test_freq | 100 步 | |

### 6.3 监控训练（WandB）

如果配置了 WandB，可在 wandb.ai 上实时查看：

- `train/loss` - 训练损失（应持续下降）
- `train/grad_norm` - 梯度范数（异常飙升说明有问题）
- `train/lr(1e-3)` - 学习率曲线（应为 cosine 形状）
- `val/loss` - 验证损失（200 条回归集）

```bash
# 如果没有配置 WandB，查看控制台日志
tmux attach -t sft_train
```

### 6.4 预期训练指标

| 阶段 | train/loss | val/loss | 说明 |
|------|-----------|---------|------|
| 开始 (step 0) | ~2.0-3.0 | ~2.0-3.0 | 未微调的模型 |
| step 100 | ~0.8-1.2 | ~1.0-1.5 | 快速下降 |
| step 200 | ~0.5-0.8 | ~0.8-1.2 | 收敛中 |
| step 300 | ~0.3-0.6 | ~0.8-1.5 | 可能开始过拟合 |
| step 378 (final) | ~0.2-0.5 | ~0.8-2.0 | val/loss 可能回升 |

> **过拟合预期**：2,020 条数据训练 3 epoch，train/loss 持续下降但 val/loss 可能回升是正常的。最优检查点依据评测指标（accepted@1）选择，不依据 val/loss。

### 6.5 检查点保存位置

```
phase_1_ SFT/checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/
├── global_step_100/
│   ├── huggingface/          # HF 格式模型（用于 vLLM 加载评测）
│   │   ├── config.json
│   │   ├── model-*.safetensors
│   │   └── tokenizer.json
│   ├── model/                # FSDP 分片模型
│   ├── optimizer/            # 优化器状态
│   └── extra/                # 训练元信息
├── global_step_200/
├── global_step_300/
└── global_step_378/          # 最终检查点
```

### 6.6 训练中断恢复

`run_sft.sh` 已配置 `trainer.resume_mode=auto`，如果训练中断：

```bash
# 直接重新运行相同命令，会自动从最近的 checkpoint 恢复
bash run_sft.sh 4
```

---

## 7. 检查点评测

训练完成后，评测所有检查点的代码生成质量。

### 7.1 启动评测服务

评测需要 vLLM（推理）+ SandboxFusion（代码执行）两个服务。

```bash
# --- 终端 1：启动 SandboxFusion ---
tmux new -s sandbox

# 源码方式
export PATH="/opt/conda/bin:$PATH"
source /opt/conda/bin/activate sandbox-server
cd /workspace/sandbox
uvicorn sandbox.server.server:app --host 0.0.0.0 --port 8080

# 分离: Ctrl+B, D
```

> **注意**：vLLM 由 `phase1_eval.py` 自动启动和关闭，无需手动管理。

### 7.2 评测方式

#### 方式 A：批量评测所有检查点（推荐）

```bash
cd "/workspace/verl/coding_model_project/phase_1_ SFT"

bash run_eval_all.sh \
    checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder \
    http://localhost:8080
```

脚本会：
1. 自动发现所有 `global_step_*` 检查点
2. 中间检查点使用 Tier 1 评测（codecontests_valid + mbpp_reg）
3. 最终检查点使用 Tier 3 评测（全量数据集）
4. 每个检查点依次启动 vLLM -> 评测 -> 关闭 vLLM
5. 输出比较表格

#### 方式 B：单个检查点评测

```bash
# Tier 1 评测（快速，~10 分钟）
python phase1_eval.py \
    --checkpoint_dir checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/global_step_100 \
    --tier 1 \
    --sandbox_url http://localhost:8080

# Tier 3 评测（完整，~35 分钟）
python phase1_eval.py \
    --checkpoint_dir checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/global_step_378 \
    --tier 3 \
    --sandbox_url http://localhost:8080
```

#### 方式 C：连接已有 vLLM 服务器（手动管理 vLLM）

如果想手动启动 vLLM（例如用更多 GPU 做 tensor parallel）：

```bash
# 手动启动 vLLM（在另一个 tmux 会话中）
python -m vllm.entrypoints.openai.api_server \
    --model checkpoints/.../global_step_378/huggingface \
    --port 8000 \
    --tensor-parallel-size 2 \
    --gpu-memory-utilization 0.85 \
    --max-model-len 6144 \
    --dtype bfloat16 \
    --trust-remote-code

# 连接已有服务器评测
python phase1_eval.py \
    --checkpoint_dir checkpoints/.../global_step_378 \
    --vllm_url http://localhost:8000 \
    --tier 3 \
    --sandbox_url http://localhost:8080
```

### 7.3 评测 Tier 说明

| Tier | 数据集 | 题目数 | 预估时间 | 使用场景 |
|------|--------|--------|---------|---------|
| **1** | codecontests_valid (117) + mbpp_reg (200) | 317 | ~10 min | 每个检查点 |
| **2** | + codecontests_valid_big (500) | 817 | ~20 min | 按需 |
| **3** | + codecontests_test (165) + humaneval (164) | 1,146 | ~35 min | 最终检查点 |

### 7.4 评测结果目录

```
phase_1_ SFT/outputs/phase1/
├── step_100/
│   ├── metrics.json         # 汇总指标
│   ├── summary.json         # 详细统计
│   ├── eval_info.json       # 评测配置
│   ├── run_info.json        # 运行信息
│   ├── qa_logs/             # 问答日志（采样）
│   └── per_problem/         # 每题详细结果
├── step_200/
├── step_300/
├── step_378/
├── eval_history.jsonl       # 所有评测历史（一行一条）
└── best_checkpoint.json     # 最优检查点信息
```

---

## 8. 结果分析与下载

### 8.1 查看评测历史对比

```bash
# 查看所有检查点的核心指标
cat "phase_1_ SFT/outputs/phase1/eval_history.jsonl" | python3 -m json.tool
```

### 8.2 提取关键指标对比表

```bash
python3 -c "
import json

print('Step | CC_valid accepted@1 | MBPP_reg accepted@1')
print('-----|--------------------|--------------------|')

with open('phase_1_ SFT/outputs/phase1/eval_history.jsonl') as f:
    for line in f:
        d = json.loads(line)
        step = d['step']
        cc = d['scores'].get('codecontests_valid', {}).get('accepted_at_1')
        mbpp = d['scores'].get('mbpp_reg', {}).get('accepted_at_1')
        cc_s = f'{cc:.2%}' if cc is not None else 'N/A'
        mbpp_s = f'{mbpp:.2%}' if mbpp is not None else 'N/A'
        print(f'{step:>5}| {cc_s:>19}| {mbpp_s:>19}|')
"
```

### 8.3 查看最优检查点

```bash
# 自动生成的最优检查点信息
cat "phase_1_ SFT/outputs/phase1/best_checkpoint.json" | python3 -m json.tool
```

选择最优检查点的主要指标：

- **首选**：`codecontests_valid/exec_success_rate` 最高的检查点
- **次选**：如果差异不大，参考 `codecontests_valid/accepted_at_1`
- **回归检查**：确保 `humaneval/accepted_at_1` 没有明显下降（与 Phase 0 baseline 对比）

### 8.4 下载最优检查点到本地

```bash
# 假设 global_step_200 是最优检查点

# 只下载 HF 模型（评测和后续使用只需要这个）
scp -P <PORT> -r \
    root@<HOST>:"/workspace/verl/coding_model_project/phase_1_ SFT/checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/global_step_200/huggingface" \
    ./best_sft_checkpoint/

# 下载评测结果
scp -P <PORT> -r \
    root@<HOST>:"/workspace/verl/coding_model_project/phase_1_ SFT/outputs/phase1" \
    ./phase1_eval_results/
```

### 8.5 Phase 0 vs Phase 1 对比

| 指标 | Phase 0 Baseline | Phase 1 SFT (预期) | 目标 |
|------|-----------------|-------------------|------|
| codecontests_valid accepted@1 | ~5-10% | ~15-30% | +10-20pp |
| mbpp_reg accepted@1 | ~40-50% | ~45-55% | 不回归 |
| humaneval accepted@1 | ~30-40% | ~35-45% | 不回归 |

---

## 9. 常见问题排查

### 9.1 训练相关

#### OOM (Out of Memory)

```bash
# 降低 micro_batch_size
# 编辑 run_sft.sh，修改:
data.micro_batch_size_per_gpu=1

# 或降低 max_length
data.max_length=2048
```

#### NCCL 超时

```bash
# 增加 NCCL 超时时间
export NCCL_TIMEOUT=1800

# 检查 GPU 互连
nvidia-smi topo -m
```

#### 训练 loss 不下降

- 检查学习率：初始 loss 应在 step 10 左右开始下降
- 检查数据：`python -c "import pandas as pd; df=pd.read_parquet('data/sft_train.parquet'); print(df.head())"`
- 确认 gradient checkpointing 启用：日志中应有相关信息

### 9.2 评测相关

#### vLLM 加载 checkpoint 失败

```bash
# 确认 HF 模型完整
ls checkpoints/.../global_step_*/huggingface/config.json

# 手动测试加载
python -c "
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained('checkpoints/.../global_step_100/huggingface', trust_remote_code=True)
print(f'Model loaded: {sum(p.numel() for p in model.parameters())/1e9:.1f}B params')
"
```

#### SandboxFusion 代码执行全部失败

- 检查 `sandbox-runtime` 环境是否存在：`conda env list | grep sandbox-runtime`
- 查看 SandboxFusion 日志：`tmux attach -t sandbox`
- 参考 `intro_doc/07-部署问题修复记录.md` 中的已知问题

#### 评测 model 名称不匹配警告

```
Warning: requested model 'xxx' not in /v1/models
```

`phase1_eval.py` 使用 `--model` 传入 HF 模型路径，vLLM 通常会以此路径作为 model ID。如果出现不匹配，评测仍会使用服务器上加载的模型，通常不影响结果。

### 9.3 vast.ai 相关

#### 实例意外断开

- 使用 `tmux` 保持训练进程存活
- `trainer.resume_mode=auto` 会自动从最近 checkpoint 恢复

#### 磁盘空间不足

```bash
# 检查磁盘使用
du -sh "phase_1_ SFT/checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/"/*

# 如需释放空间，可删除不需要的 checkpoint 中的 optimizer/ 目录
# （保留 huggingface/ 用于评测）
rm -rf checkpoints/.../global_step_100/optimizer/
```

---

## 10. 成本估算

### 10.1 时间估算

| 阶段 | 预估时间 | 说明 |
|------|---------|------|
| 环境配置 | 15-30 min | verl 镜像省时，主要是安装 SandboxFusion |
| 数据准备 | 5 min | 上传 parquet 或重新生成 |
| 烟雾测试 | 5-10 min | 1 GPU, 10 steps |
| 正式训练 | 2-4 hr | 4x 5090, 3 epochs |
| 检查点评测 | 1-2 hr | 4 个检查点，最终全量评测 |
| **总计** | **~4-7 hr** | |

### 10.2 费用估算（4x RTX 5090）

| 资源 | 规格 | 单价 | 时长 | 费用 |
|------|------|------|------|------|
| GPU | 4x RTX 5090 (32GB) | ~$1.60-2.40/hr | 6 hr | ~$10-15 |
| 存储 | 200 GB | ~$0.10/hr | 6 hr | ~$0.60 |
| **总计** | | | | **~$11-16** |

### 10.3 省钱建议

1. **使用 verl 官方镜像**：省 1-2 小时环境调试时间
2. **先本地生成 parquet 数据**：避免在 GPU 实例上花费 CPU 时间
3. **烟雾测试通过再开多卡**：可以先用 1 卡实例验证流水线
4. **训练和评测分开**：训练用 4x 5090，评测用 1x GPU 即可（更便宜）
5. **及时下载结果并释放实例**

---

## 附录：快速参考

### 服务端口

| 服务 | 端口 | 用途 |
|------|------|------|
| vLLM | 8000 | 模型推理 (评测阶段, phase1_eval.py 自动管理) |
| SandboxFusion | 8080 | 代码执行评测 |

### 关键文件

| 文件 | 用途 |
|------|------|
| `phase_1_ SFT/run_smoke_test.sh` | 烟雾测试 (1 GPU, 10 steps) |
| `phase_1_ SFT/run_sft.sh` | 正式训练 (多 GPU) |
| `phase_1_ SFT/phase1_eval.py` | 单检查点评测 |
| `phase_1_ SFT/run_eval_all.sh` | 批量评测所有检查点 |
| `phase_1_ SFT/prepare_sft_data.py` | 数据预处理 |
| `src/phase0_eval.py` | 评测核心脚本 (Phase 0/1 共用) |
| `src/eval_config.py` | 评测参数配置 |

### 常用命令速查

```bash
# tmux 会话管理
tmux ls                    # 列出会话
tmux attach -t sft_train   # 连接训练会话
tmux attach -t sandbox     # 连接 sandbox 会话

# GPU 监控
nvidia-smi                 # 一次性查看
watch -n 2 nvidia-smi      # 持续监控

# 训练日志
tail -f "phase_1_ SFT/checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/*.log"

# 磁盘使用
du -sh "phase_1_ SFT/checkpoints/rlvr_coding_model/phase1_sft_qwen7b_coder/"*/
```

### 推荐 Docker 镜像汇总

| 用途 | 镜像 | 说明 |
|------|------|------|
| **SFT 训练 + 评测** | `verlai/verl:vllm012.latest` | PyTorch 2.9 + CUDA 12.9 + vLLM 0.12 |
| SFT 训练 + 评测 | `verlai/verl:vllm011.latest` | PyTorch 2.8 + CUDA 12.8 + vLLM 0.11 |
| 仅评测 | `vastai/vllm:v0.10.2-cuda-12.8-pytorch-2.8.0-py312` | 纯推理 |
| SandboxFusion | `volcengine/sandbox-fusion:server-20250609` | 代码沙盒 |
