# RLVR Coding Model - 资源规划（GPU 为主）

假设目标模型规模为 **7B**（如 Qwen2.5-Coder-7B-Instruct），并且判题在 SandboxFusion 上完成（CPU 并发通常是瓶颈之一）。

你给出的可用资源：`4×H100` / `8×RTX4090(24GB)` / `4×RTX6000 Pro(高显存)`。

---

## 1) 每个 Phase 的“经济适用”推荐

| Phase | 主要工作 | 推荐 GPU（经济） | 推荐理由 |
|------|----------|------------------|----------|
| Phase 0（Baseline） | 批量生成 + 大量判题 | `1×4090`（或 `1×RTX6000`） | eval 主要吃判题并发与 I/O；单卡即可把生成端跑满 |
| Phase 1（SFT） | 监督微调 + 回归评测 | `8×4090` | 7B 全参 SFT 在 24GB 卡上更稳；吞吐/性价比最好 |
| Phase 2（DPO，可选） | 采样构造偏好对 + DPO 训练 | `1–2×4090`（数据构建）+ `8×4090`（训练） | 构建阶段偏 I/O；训练阶段类似 SFT |
| Phase 3（GRPO） | 在线 rollout + 判题 + PPO 更新 | `8×4090` | GRPO 端到端成本高，4090 的总算力性价比更高；同时保留更多并发空间 |
| Phase 4（多轮修复） | 多轮生成 + 判题（推理为主） | `1–2×4090` | 不涉及大规模反向更新时，推理卡数主要影响吞吐 |

> 注：如果你做 2 seeds，最省钱的方式是**同一套 GPU 资源串行跑两次**，避免为了并行而扩预算。

---

## 2) 何时用 H100 / 高显存卡更划算

### `4×H100`（最快交付、最省墙钟时间）

适合：

- Phase 3（GRPO）需要快速迭代、或你希望把“训练曲线 + 多次消融”在更短时间内跑完
- 你想把更多时间花在分析与报告，而不是等训练跑完

### `4×RTX6000 Pro（高显存）`（最省心、最稳）

适合：

- 你在 GRPO 里需要同时驻留更多模型/worker（actor/ref/rollout 等）导致显存压力大
- 你不想在 24GB 卡上花太多时间调 ZeRO/FSDP/分片策略

---

## 3) 非 GPU 但必须写进“资源假设”的两条

- **CPU 并发**：Sandbox 判题吞吐通常由 CPU 核心数/进程并发决定（建议在报告里写明并发数与限流策略）
- **磁盘与日志**：多轮日志与判题输出会很大；建议将 `qa_logs/` 与 `checkpoints/` 分盘或做归档策略

