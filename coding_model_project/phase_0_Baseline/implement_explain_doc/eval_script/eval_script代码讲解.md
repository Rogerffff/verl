# Phase 0 è¯„æµ‹è„šæœ¬è®²è§£ (Simple æ¨¡å¼)

æœ¬æ–‡æ¡£æŒ‰ç…§ **ä»£ç æ‰§è¡Œé¡ºåº** è®²è§£ `phase0_eval.py` çš„å®ç°ã€‚

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼šç¨‹åºå…¥å£ä¸é…ç½®åˆå§‹åŒ–

### 1.1 ç¨‹åºå…¥å£ç‚¹

```python
# ç¬¬ 1896-1897 è¡Œ
if __name__ == "__main__":
    main()
```

Python ç¨‹åºä»è¿™é‡Œå¼€å§‹æ‰§è¡Œã€‚`__name__ == "__main__"` ç¡®ä¿åªæœ‰ç›´æ¥è¿è¡Œè„šæœ¬æ—¶æ‰æ‰§è¡Œ `main()`ï¼Œè¢« import æ—¶ä¸æ‰§è¡Œã€‚

---

### 1.2 main() å‡½æ•°ï¼šå‘½ä»¤è¡Œè§£æä¸é…ç½®åˆ›å»º

```python
# ç¬¬ 1768-1892 è¡Œ
def main():
    """å‘½ä»¤è¡Œå…¥å£å‡½æ•°"""

    # ========== ç¬¬ä¸€æ­¥ï¼šå®šä¹‰å‘½ä»¤è¡Œå‚æ•° ==========
    parser = argparse.ArgumentParser(
        description="Phase 0 Baseline Evaluation (verl Standalone Rollout)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="..."  # ä½¿ç”¨ç¤ºä¾‹
    )

    # æ¨¡å¼é€‰æ‹©
    parser.add_argument("--mode", type=str, default="simple",
                        choices=["verl", "simple"],
                        help="è¿è¡Œæ¨¡å¼: verl (åˆ†å¸ƒå¼) æˆ– simple (ç®€åŒ–)")

    # æ¨¡å‹é…ç½®
    parser.add_argument("--model", type=str,
                        default="Qwen/Qwen2.5-Coder-7B-Instruct")

    # vLLM æœåŠ¡å™¨åœ°å€ï¼ˆsimple æ¨¡å¼ä½¿ç”¨ï¼‰
    parser.add_argument("--vllm_url", type=str,
                        default="http://localhost:8000")

    # SandboxFusion é…ç½®
    parser.add_argument("--sandbox_url", type=str,
                        default="http://localhost:8080")

    # æ•°æ®é›†åˆ—è¡¨
    parser.add_argument("--datasets", nargs="+", type=str,
                        default=["humaneval", "mbpp_reg"])

    # ... æ›´å¤šå‚æ•° ...

    args = parser.parse_args()
```

#### argparse å…³é”®è¯­æ³•

| å‚æ•° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|
| `type=str` | å‚æ•°ç±»å‹ | å­—ç¬¦ä¸² |
| `default="simple"` | é»˜è®¤å€¼ | ä¸ä¼ å‚æ•°æ—¶ä½¿ç”¨ |
| `choices=[...]` | é™åˆ¶å¯é€‰å€¼ | åªèƒ½æ˜¯åˆ—è¡¨ä¸­çš„å€¼ |
| `nargs="+"` | æ¥æ”¶å¤šä¸ªå€¼ | `--datasets humaneval mbpp_reg` |
| `action="store_true"` | å¸ƒå°”å¼€å…³ | å­˜åœ¨åˆ™ä¸º True |

---

### 1.3 åˆ›å»º EvalConfig é…ç½®å¯¹è±¡

```python
    # ç¬¬ 1862-1884 è¡Œ
    # ========== ç¬¬äºŒæ­¥ï¼šåˆ›å»ºé…ç½®å¯¹è±¡ ==========
    config = EvalConfig(
        mode=args.mode,                    # "simple"
        model_path=args.model,             # æ¨¡å‹è·¯å¾„
        vllm_url=args.vllm_url,            # vLLM æœåŠ¡å™¨åœ°å€
        sandbox_url=args.sandbox_url,      # SandboxFusion åœ°å€
        run_timeout=args.run_timeout,      # ä»£ç æ‰§è¡Œè¶…æ—¶
        temperature=args.temperature,      # é‡‡æ ·æ¸©åº¦ï¼ˆ0.0 = greedyï¼‰
        max_new_tokens=args.max_tokens,    # æœ€å¤§ç”Ÿæˆé•¿åº¦
        datasets=args.datasets,            # æ•°æ®é›†åˆ—è¡¨
        manifest_dir=args.manifest_dir,    # æ•°æ®ç›®å½•
        output_dir=args.output_dir,        # è¾“å‡ºç›®å½•
        max_concurrent_requests=args.max_concurrent,  # æœ€å¤§å¹¶å‘æ•°
        batch_size=args.batch_size,        # æ‰¹å¤„ç†å¤§å°
        # ... æ›´å¤šé…ç½® ...
    )
```

#### EvalConfig æ•°æ®ç±»ï¼ˆç¬¬ 130-189 è¡Œï¼‰

```python
@dataclass
class EvalConfig:
    """
    è¯„æµ‹é…ç½® - ä½¿ç”¨ Python dataclass è‡ªåŠ¨ç”Ÿæˆ __init__ ç­‰æ–¹æ³•
    """
    # === è¿è¡Œæ¨¡å¼ ===
    mode: str = "verl"  # "simple" æ—¶è¿æ¥å·²æœ‰ vLLM æœåŠ¡å™¨

    # === æ¨¡å‹é…ç½® ===
    model_path: str = "Qwen/Qwen2.5-Coder-7B-Instruct"

    # === ç®€åŒ–æ¨¡å¼é…ç½® ===
    vllm_url: str = "http://localhost:8000"  # vLLM æœåŠ¡å™¨åœ°å€

    # === è§£ç å‚æ•°ï¼ˆEVAL@1 åè®®ï¼šè´ªå©ªè§£ç ï¼‰ ===
    temperature: float = 0.0  # 0.0 = greedyï¼Œç¡®ä¿å¯å¤ç°
    top_p: float = 1.0
    max_new_tokens: int = 2048

    # === SandboxFusion é…ç½® ===
    sandbox_url: str = "http://localhost:8080"
    run_timeout: int = 30     # ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰
    memory_limit_mb: int = 1024

    # === å¹¶å‘æ§åˆ¶ ===
    max_concurrent_requests: int = 64  # æœ€å¤§å¹¶å‘è¯·æ±‚æ•°
    batch_size: int = 50               # æ‰¹å¤„ç†å¤§å°

    # === æ•°æ®é…ç½® ===
    datasets: List[str] = field(default_factory=lambda: ["humaneval", "mbpp_reg"])
```

#### @dataclass è£…é¥°å™¨

`@dataclass` æ˜¯ Python 3.7+ çš„ç‰¹æ€§ï¼Œè‡ªåŠ¨ç”Ÿæˆï¼š
- `__init__()` æ–¹æ³•
- `__repr__()` æ–¹æ³•
- `__eq__()` æ–¹æ³•

```python
# ç­‰ä»·äºæ‰‹å†™ï¼š
class EvalConfig:
    def __init__(self, mode="verl", model_path="...", ...):
        self.mode = mode
        self.model_path = model_path
        # ...
```

#### field(default_factory=...) çš„ä½œç”¨

```python
# é”™è¯¯å†™æ³•ï¼å¯å˜é»˜è®¤å€¼ä¼šè¢«æ‰€æœ‰å®ä¾‹å…±äº«
datasets: List[str] = ["humaneval"]  # å±é™©ï¼

# æ­£ç¡®å†™æ³•ï¼šæ¯æ¬¡åˆ›å»ºæ–°å®ä¾‹æ—¶è°ƒç”¨ lambda ç”Ÿæˆæ–°åˆ—è¡¨
datasets: List[str] = field(default_factory=lambda: ["humaneval", "mbpp_reg"])
```

---

### 1.4 å¯åŠ¨å¼‚æ­¥äº‹ä»¶å¾ªç¯

```python
    # ç¬¬ 1892 è¡Œ
    # ========== ç¬¬ä¸‰æ­¥ï¼šè¿è¡Œè¯„æµ‹ï¼ˆå¼‚æ­¥ï¼‰ ==========
    asyncio.run(run_evaluation(config))
```

**è¿™æ˜¯æ•´ä¸ªè„šæœ¬çš„å…³é”®è½¬æŠ˜ç‚¹ï¼**

#### asyncio.run() åšäº†ä»€ä¹ˆï¼Ÿ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  åŒæ­¥ä¸–ç•Œ (main å‡½æ•°)                                        â”‚
â”‚                                                              â”‚
â”‚  asyncio.run(run_evaluation(config))                        â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  å¼‚æ­¥ä¸–ç•Œ (äº‹ä»¶å¾ªç¯ Event Loop)                      â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  run_evaluation() åç¨‹å¼€å§‹æ‰§è¡Œ                       â”‚    â”‚
â”‚  â”‚       â”‚                                              â”‚    â”‚
â”‚  â”‚       â”œâ”€â”€ await evaluate_dataset() â”€â”€â”              â”‚    â”‚
â”‚  â”‚       â”‚                               â”‚              â”‚    â”‚
â”‚  â”‚       â”‚   await batch_generate() â”€â”€â”€â”€â”€â”¤              â”‚    â”‚
â”‚  â”‚       â”‚                               â”‚ å¹¶å‘æ‰§è¡Œ     â”‚    â”‚
â”‚  â”‚       â”‚   await generate_code() â”€â”€â”€â”€â”€â”€â”¤              â”‚    â”‚
â”‚  â”‚       â”‚   await generate_code() â”€â”€â”€â”€â”€â”€â”¤              â”‚    â”‚
â”‚  â”‚       â”‚   await generate_code() â”€â”€â”€â”€â”€â”€â”˜              â”‚    â”‚
â”‚  â”‚       â”‚                                              â”‚    â”‚
â”‚  â”‚       â–¼                                              â”‚    â”‚
â”‚  â”‚  åç¨‹å®Œæˆï¼Œè¿”å›ç»“æœ                                   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                                                      â”‚
â”‚       â–¼                                                      â”‚
â”‚  asyncio.run() è¿”å›ï¼Œå›åˆ°åŒæ­¥ä¸–ç•Œ                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

`asyncio.run()` çš„ä½œç”¨ï¼š
1. **åˆ›å»ºäº‹ä»¶å¾ªç¯** (Event Loop)
2. **è¿è¡Œåç¨‹** ç›´åˆ°å®Œæˆ
3. **å…³é—­äº‹ä»¶å¾ªç¯**
4. **è¿”å›åç¨‹çš„ç»“æœ**

---

### 1.5 æœ¬éƒ¨åˆ†çš„é…ç½®å‚æ•°æ€»ç»“ï¼ˆSimple æ¨¡å¼ï¼‰

è¿è¡Œå‘½ä»¤ç¤ºä¾‹ï¼š
```bash
python src/phase0_eval.py \
    --mode simple \
    --vllm_url http://localhost:8000 \
    --sandbox_url http://localhost:8080 \
    --datasets humaneval mbpp_reg \
    --temperature 0.0 \
    --max_tokens 2048 \
    --max_concurrent 64 \
    --batch_size 50 \
    --output_dir outputs/phase0
```

| å‚æ•° | å€¼ | è¯´æ˜ |
|------|----|----|
| `mode` | `simple` | è¿æ¥å·²æœ‰ vLLM æœåŠ¡å™¨ |
| `vllm_url` | `http://localhost:8000` | vLLM OpenAI-compatible API åœ°å€ |
| `sandbox_url` | `http://localhost:8080` | SandboxFusion åˆ¤é¢˜æœåŠ¡åœ°å€ |
| `datasets` | `["humaneval", "mbpp_reg"]` | è¦è¯„æµ‹çš„æ•°æ®é›† |
| `temperature` | `0.0` | è´ªå©ªè§£ç ï¼Œç¡®ä¿ç»“æœå¯å¤ç° |
| `max_concurrent` | `64` | æœ€å¤šåŒæ—¶ 64 ä¸ªå¹¶å‘è¯·æ±‚ |

---

## å°ç»“

**æ‰§è¡Œæµç¨‹åˆ°ç›®å‰ä¸ºæ­¢ï¼š**

```
1. if __name__ == "__main__": main()
       â”‚
       â–¼
2. main() å‡½æ•°
       â”‚
       â”œâ”€â”€ argparse è§£æå‘½ä»¤è¡Œå‚æ•°
       â”‚
       â”œâ”€â”€ åˆ›å»º EvalConfig é…ç½®å¯¹è±¡
       â”‚
       â””â”€â”€ asyncio.run(run_evaluation(config))
              â”‚
              â–¼
3. è¿›å…¥å¼‚æ­¥ä¸–ç•Œ... (ä¸‹ä¸€éƒ¨åˆ†è®²è§£)
```

**å…³é”®æ¦‚å¿µï¼š**
- `argparse`: å‘½ä»¤è¡Œå‚æ•°è§£æ
- `@dataclass`: è‡ªåŠ¨ç”Ÿæˆæ•°æ®ç±»æ–¹æ³•
- `field(default_factory=...)`: å¯å˜é»˜è®¤å€¼çš„æ­£ç¡®å†™æ³•
- `asyncio.run()`: åŒæ­¥ä¸–ç•Œåˆ°å¼‚æ­¥ä¸–ç•Œçš„å…¥å£

---

**è¯·ç¡®è®¤ä½ ç†è§£äº†ç¬¬ä¸€éƒ¨åˆ†åï¼Œæˆ‘å°†ç»§ç»­è®²è§£ç¬¬äºŒéƒ¨åˆ†ï¼šrun_evaluation() ä¸»æµç¨‹ã€‚**

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šrun_evaluation() ä¸»æµç¨‹

### 2.1 åç¨‹å‡½æ•°çš„å®šä¹‰

```python
# ç¬¬ 1616 è¡Œ
async def run_evaluation(config: EvalConfig):
    """
    è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹
    """
```

#### ä»€ä¹ˆæ˜¯åç¨‹ (Coroutine)ï¼Ÿ

**åç¨‹** æ˜¯å¯ä»¥æš‚åœå’Œæ¢å¤æ‰§è¡Œçš„å‡½æ•°ã€‚`async def` å®šä¹‰çš„å‡½æ•°å«åš **åç¨‹å‡½æ•°**ï¼Œè°ƒç”¨å®ƒä¸ä¼šç«‹å³æ‰§è¡Œï¼Œè€Œæ˜¯è¿”å›ä¸€ä¸ª **åç¨‹å¯¹è±¡**ã€‚

```python
# æ™®é€šå‡½æ•°ï¼šè°ƒç”¨ç«‹å³æ‰§è¡Œ
def normal_func():
    return 42
result = normal_func()  # ç«‹å³æ‰§è¡Œï¼Œresult = 42

# åç¨‹å‡½æ•°ï¼šè°ƒç”¨è¿”å›åç¨‹å¯¹è±¡ï¼Œä¸ä¼šç«‹å³æ‰§è¡Œ
async def async_func():
    return 42
coro = async_func()     # ä¸æ‰§è¡Œï¼è¿”å› <coroutine object>
result = await coro     # ç°åœ¨æ‰æ‰§è¡Œï¼Œresult = 42
```

#### åç¨‹ vs çº¿ç¨‹ vs è¿›ç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          å¯¹æ¯”è¡¨                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚             â”‚   è¿›ç¨‹        â”‚   çº¿ç¨‹        â”‚   åç¨‹                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å†…å­˜å ç”¨     â”‚   æœ€å¤§        â”‚   ä¸­ç­‰        â”‚   æœ€å°ï¼ˆå‡ KBï¼‰          â”‚
â”‚ åˆ‡æ¢å¼€é”€     â”‚   æœ€å¤§        â”‚   ä¸­ç­‰        â”‚   æœ€å°ï¼ˆç”¨æˆ·æ€ï¼‰        â”‚
â”‚ å¹¶è¡Œèƒ½åŠ›     â”‚   çœŸå¹¶è¡Œ      â”‚   å— GIL é™åˆ¶ â”‚   å•çº¿ç¨‹å¹¶å‘           â”‚
â”‚ é€‚ç”¨åœºæ™¯     â”‚   CPU å¯†é›†    â”‚   æ··åˆåœºæ™¯    â”‚   I/O å¯†é›†ï¼ˆç½‘ç»œï¼‰     â”‚
â”‚ æ•°é‡ä¸Šé™     â”‚   å‡ åä¸ª      â”‚   å‡ ç™¾ä¸ª      â”‚   å‡ ä¸‡ä¸ª               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æœ¬è„šæœ¬çš„åœºæ™¯**ï¼šè¯„æµ‹éœ€è¦å¤§é‡ç½‘ç»œ I/Oï¼ˆè°ƒç”¨ vLLM å’Œ SandboxFusionï¼‰ï¼Œåç¨‹æ˜¯æœ€ä½³é€‰æ‹©ã€‚

---

### 2.2 run_evaluation() æ‰§è¡Œæµç¨‹ï¼ˆSimple æ¨¡å¼ï¼‰

```python
async def run_evaluation(config: EvalConfig):
    # ========== æ­¥éª¤1ï¼šæ‰“å°é…ç½®ä¿¡æ¯ ==========
    print(f"Mode: {config.mode}")        # "simple"
    print(f"Model: {config.model_path}") # "Qwen/Qwen2.5-Coder-7B-Instruct"
    print(f"Datasets: {config.datasets}")# ["humaneval", "mbpp_reg"]

    # ========== æ­¥éª¤2ï¼šåˆ›å»ºè¾“å‡ºç›®å½• ==========
    output_dir = Path(config.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    # parents=True: è‡ªåŠ¨åˆ›å»ºçˆ¶ç›®å½•
    # exist_ok=True: ç›®å½•å·²å­˜åœ¨æ—¶ä¸æŠ¥é”™
```

#### pathlib.Path ç”¨æ³•

```python
from pathlib import Path

# åˆ›å»º Path å¯¹è±¡ï¼ˆæ¯”å­—ç¬¦ä¸²æ‹¼æ¥æ›´å®‰å…¨ï¼‰
output_dir = Path("outputs/phase0")

# åˆ›å»ºç›®å½•
output_dir.mkdir(parents=True, exist_ok=True)

# è·¯å¾„æ‹¼æ¥ï¼ˆç”¨ / è¿ç®—ç¬¦ï¼Œä¸ç”¨ os.path.joinï¼‰
metrics_file = output_dir / "metrics.json"  # "outputs/phase0/metrics.json"

# å¸¸ç”¨æ–¹æ³•
output_dir.exists()      # æ˜¯å¦å­˜åœ¨
output_dir.is_dir()      # æ˜¯å¦æ˜¯ç›®å½•
output_dir.iterdir()     # éå†ç›®å½•
```

---

### 2.3 è·å–æœåŠ¡å™¨åœ°å€ï¼ˆSimple æ¨¡å¼çš„å…³é”®åˆ†æ”¯ï¼‰

```python
    # ç¬¬ 1639-1649 è¡Œ
    # ========== æ­¥éª¤3ï¼šè·å–æœåŠ¡å™¨åœ°å€ ==========
    if config.mode == "verl":
        # verl åˆ†å¸ƒå¼æ¨¡å¼ï¼šå¯åŠ¨å¤šä¸ª vLLM replicaï¼ˆè·³è¿‡ï¼‰
        rollout_servers, server_addresses = await start_rollout_servers(config)
    else:
        # â˜… Simple æ¨¡å¼ï¼šè¿æ¥å·²æœ‰çš„ vLLM æœåŠ¡å™¨ â˜…
        print(f"\n[Simple Mode] Connecting to {config.vllm_url}")
        # å»æ‰ http:// å‰ç¼€ï¼Œå› ä¸ºåç»­ä»£ç ä¼šé‡æ–°æ·»åŠ 
        server_addresses = [config.vllm_url.replace("http://", "")]
        # ä¾‹å¦‚ï¼š["localhost:8000"]
        rollout_servers = None
```

**Simple æ¨¡å¼**çš„æ ¸å¿ƒï¼š
- ä¸å¯åŠ¨ä»»ä½•æœåŠ¡å™¨
- ç›´æ¥ä½¿ç”¨ç”¨æˆ·æä¾›çš„ `--vllm_url` åœ°å€
- `server_addresses` åˆ—è¡¨åªæœ‰ä¸€ä¸ªå…ƒç´ 

---

### 2.4 åˆå§‹åŒ–ç»„ä»¶

```python
    # ç¬¬ 1651-1664 è¡Œ
    # ========== æ­¥éª¤4ï¼šåˆå§‹åŒ–ç»„ä»¶ ==========

    # 1. æŒ‡æ ‡æ”¶é›†å™¨ï¼šæ”¶é›† accepted@1ã€pass_ratio ç­‰ç»Ÿè®¡ä¿¡æ¯
    metrics_collector = MetricsCollector()

    # 2. é—®ç­”æ—¥å¿—ï¼šä¿å­˜ç”Ÿæˆçš„ä»£ç å’Œè¯„æµ‹ç»“æœï¼ˆç”¨äºè°ƒè¯•ï¼‰
    qa_logger = QALogger(
        output_dir / "qa_logs",
        sample_size=config.qa_sample_size  # é»˜è®¤ 20
    )

    # 3. WandBï¼ˆå¯é€‰ï¼‰ï¼šå®éªŒè¿½è¸ªå¹³å°
    if config.use_wandb:
        import wandb
        wandb.init(project=config.wandb_project, name=run_name)
        wandb.config.update(asdict(config))  # è®°å½•é…ç½®

    all_metrics = {}  # å­˜å‚¨æ‰€æœ‰æ•°æ®é›†çš„è¯„æµ‹ç»“æœ
```

#### asdict() å‡½æ•°

```python
from dataclasses import asdict

# å°† dataclass è½¬æ¢ä¸ºå­—å…¸
config_dict = asdict(config)
# {
#     "mode": "simple",
#     "model_path": "Qwen/Qwen2.5-Coder-7B-Instruct",
#     "vllm_url": "http://localhost:8000",
#     ...
# }
```

---

### 2.5 ä¸»å¾ªç¯ï¼šè¯„æµ‹æ¯ä¸ªæ•°æ®é›†

```python
    # ç¬¬ 1668-1699 è¡Œ
    # ========== æ­¥éª¤5ï¼šè¯„æµ‹æ¯ä¸ªæ•°æ®é›† ==========
    try:
        for dataset_key in config.datasets:  # ["humaneval", "mbpp_reg"]
            print(f"\n[Loading {dataset_key}]")

            # 5.1 åŠ è½½é¢˜ç›®
            prompts = load_prompts(dataset_key, config)
            # prompts æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
            # {
            #     "problem_id": "HumanEval/0",
            #     "prompt": "def func():\n    ...",
            #     "sandbox_dataset": "HumanEval",
            #     "test_cases": {...}  # å¯é€‰
            # }

            if not prompts:
                print(f"  No prompts found, skipping...")
                continue

            print(f"  Loaded {len(prompts)} problems")

            # 5.2 è¯„æµ‹æ•°æ®é›†ï¼ˆæ ¸å¿ƒï¼ï¼‰
            dataset_metrics = await evaluate_dataset(
                dataset_key,       # "humaneval"
                prompts,           # é¢˜ç›®åˆ—è¡¨
                server_addresses,  # ["localhost:8000"]
                config,            # é…ç½®
                metrics_collector, # æŒ‡æ ‡æ”¶é›†å™¨
                qa_logger,         # æ—¥å¿—è®°å½•å™¨
            )
            # â†‘ await: ç­‰å¾…åç¨‹æ‰§è¡Œå®Œæˆ

            all_metrics[dataset_key] = dataset_metrics

# â˜… evaluate_dataset() è¿”å›å€¼ç¤ºä¾‹ â˜…
# dataset_metrics æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«è¯¥æ•°æ®é›†çš„ç»Ÿè®¡ä¿¡æ¯ï¼š
dataset_metrics = {
    # åŸºç¡€ç»Ÿè®¡
    "total_problems": 164,              # æ€»é¢˜æ•°

    # è´¨é‡æŒ‡æ ‡ï¼ˆæ ¸å¿ƒï¼‰
    "accepted_at_1": 0.7256,            # é€šè¿‡ç‡ = 119/164
    "pass_ratio_mean": 0.8234,          # å¹³å‡æµ‹è¯•ç”¨ä¾‹é€šè¿‡æ¯”ä¾‹
    "pass_ratio_p50": 1.0,              # ä¸­ä½æ•°ï¼ˆå¾ˆå¤šé¢˜å…¨éƒ¨é€šè¿‡ï¼‰
    "pass_ratio_p90": 1.0,              # 90% åˆ†ä½

    # Token ç»Ÿè®¡
    "total_gen_tokens": 40278,          # æ€»ç”Ÿæˆ token æ•°
    "avg_gen_tokens": 245.6,            # å¹³å‡æ¯é¢˜ç”Ÿæˆ token æ•°

    # æ—¶é—´ç»Ÿè®¡
    "total_gen_time": 387.5,            # æ€»ç”Ÿæˆæ—¶é—´ï¼ˆç§’ï¼‰
    "avg_gen_time": 2.36,               # å¹³å‡æ¯é¢˜ç”Ÿæˆæ—¶é—´
    "total_judge_time": 85.3,           # æ€»åˆ¤é¢˜æ—¶é—´
    "avg_judge_time": 0.52,             # å¹³å‡æ¯é¢˜åˆ¤é¢˜æ—¶é—´
    "wall_clock_time": 45.2,            # å®é™…è€—æ—¶ï¼ˆåŒ…å«å¹¶å‘ï¼‰

    # æ•ˆç‡æŒ‡æ ‡
    "throughput": 3.63,                 # ååé‡ = 164/45.2 é¢˜/ç§’
    "cost_per_solved_tokens": 338.5,    # æ¯è§£å†³ä¸€é¢˜æ¶ˆè€—çš„ token
    "cost_per_solved_judge_time": 0.72, # æ¯è§£å†³ä¸€é¢˜çš„åˆ¤é¢˜æ—¶é—´

    # å¼‚å¸¸ç»Ÿè®¡
    "truncation_count": 2,              # è¢«æˆªæ–­çš„é¢˜æ•°
    "truncation_rate": 0.012,           # æˆªæ–­ç‡ = 2/164
    "timeout_count": 1,                 # è¶…æ—¶é¢˜æ•°
    "timeout_rate": 0.006,              # è¶…æ—¶ç‡

    # é”™è¯¯åˆ†å¸ƒ
    "error_distribution": {
        "success": 119,
        "wrong_answer": 32,
        "runtime_error": 8,
        "syntax_error": 3,
        "timeout": 1,
        "empty_output": 1
    }
}

    finally:
        # æ¸…ç†èµ„æºï¼ˆverl æ¨¡å¼æ‰éœ€è¦ï¼‰
        if rollout_servers:
            print("\n[Shutting down Rollout Servers]")
```

#### await å…³é”®å­—è¯¦è§£

```python
# await åªèƒ½åœ¨ async å‡½æ•°å†…ä½¿ç”¨
async def run_evaluation(config):
    # await åšäº†ä¸¤ä»¶äº‹ï¼š
    # 1. ç­‰å¾…åç¨‹æ‰§è¡Œå®Œæˆ
    # 2. è·å–åç¨‹çš„è¿”å›å€¼
    dataset_metrics = await evaluate_dataset(...)
    #                 â†‘
    #     æš‚åœå½“å‰åç¨‹ï¼Œè®©å‡ºæ§åˆ¶æƒç»™äº‹ä»¶å¾ªç¯
    #     äº‹ä»¶å¾ªç¯å¯ä»¥å»æ‰§è¡Œå…¶ä»–åç¨‹
    #     å½“ evaluate_dataset å®Œæˆåï¼Œæ¢å¤æ‰§è¡Œ
```

**æ‰§è¡Œæµç¨‹å›¾**ï¼š

```
run_evaluation()                    äº‹ä»¶å¾ªç¯
      â”‚                                 â”‚
      â”œâ”€â”€ await evaluate_dataset() â”€â”€â”€â”€â–ºâ”‚ æš‚åœ run_evaluation
      â”‚                                 â”‚ æ‰§è¡Œ evaluate_dataset
      â”‚                                 â”‚   â”œâ”€â”€ await batch_generate()
      â”‚                                 â”‚   â”‚      â””â”€â”€ æ‰§è¡Œ HTTP è¯·æ±‚
      â”‚                                 â”‚   â””â”€â”€ è¿”å›ç»“æœ
      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ æ¢å¤ run_evaluation
      â”‚                                 â”‚
      â”œâ”€â”€ å¤„ç†ç»“æœ                       â”‚
      â””â”€â”€ ç»§ç»­ä¸‹ä¸€ä¸ªæ•°æ®é›†               â”‚
```

#### try...finally ä¿è¯æ¸…ç†

```python
try:
    # å¯èƒ½æŠ›å‡ºå¼‚å¸¸çš„ä»£ç 
    for dataset_key in config.datasets:
        dataset_metrics = await evaluate_dataset(...)
finally:
    # æ— è®ºæ˜¯å¦å¼‚å¸¸ï¼Œéƒ½ä¼šæ‰§è¡Œæ¸…ç†
    if rollout_servers:
        print("[Shutting down]")
```

---

### 2.6 ä¿å­˜ç»“æœ

```python
    # ç¬¬ 1705-1735 è¡Œ
    # ========== æ­¥éª¤6ï¼šä¿å­˜ç»“æœ ==========

    # 6.1 å¤„ç† JSON ä¸æ”¯æŒçš„å€¼ï¼ˆinf â†’ nullï¼‰
    def handle_inf(obj):
        """é€’å½’å¤„ç† inf å€¼"""
        if isinstance(obj, dict):
            return {k: handle_inf(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [handle_inf(v) for v in obj]
        elif isinstance(obj, float) and obj == float('inf'):
            return None  # JSON ä¸æ”¯æŒ inf
        return obj

    # 6.2 ä¿å­˜æŒ‡æ ‡
    with open(output_dir / "metrics.json", 'w', encoding='utf-8') as f:
        json.dump(handle_inf(all_metrics), f, indent=2, ensure_ascii=False)

    # 6.3 ä¿å­˜é—®ç­”æ—¥å¿—
    qa_logger.save()

    # 6.4 ä¿å­˜è¯¦ç»†ç»Ÿè®¡
    summary = metrics_collector.get_summary()
    with open(output_dir / "summary.json", 'w') as f:
        json.dump(handle_inf(summary), f, indent=2)
```

#### â˜… all_metrics æœ€ç»ˆç»“æ„ç¤ºä¾‹ â˜…

```python
# è¯„æµ‹å®Œæ‰€æœ‰æ•°æ®é›†åï¼Œall_metrics çš„ç»“æ„ï¼š
all_metrics = {
    "humaneval": {
        "total_problems": 164,
        "accepted_at_1": 0.7256,        # 72.56% é€šè¿‡ç‡
        "pass_ratio_mean": 0.8234,
        "pass_ratio_p50": 1.0,
        "pass_ratio_p90": 1.0,
        "total_gen_tokens": 40278,
        "avg_gen_tokens": 245.6,
        "avg_gen_time": 2.36,
        "avg_judge_time": 0.52,
        "throughput": 3.63,
        "cost_per_solved_tokens": 338.5,
        "truncation_rate": 0.012,
        "timeout_rate": 0.006,
        "error_distribution": {
            "success": 119,
            "wrong_answer": 32,
            "runtime_error": 8,
            "syntax_error": 3,
            "timeout": 1,
            "empty_output": 1
        }
    },
    "mbpp_reg": {
        "total_problems": 200,
        "accepted_at_1": 0.685,          # 68.5% é€šè¿‡ç‡
        "pass_ratio_mean": 0.7856,
        "pass_ratio_p50": 1.0,
        "pass_ratio_p90": 1.0,
        "total_gen_tokens": 52340,
        "avg_gen_tokens": 261.7,
        "avg_gen_time": 2.89,
        "avg_judge_time": 0.48,
        "throughput": 3.21,
        "cost_per_solved_tokens": 382.0,
        "truncation_rate": 0.015,
        "timeout_rate": 0.005,
        "error_distribution": {
            "success": 137,
            "wrong_answer": 45,
            "runtime_error": 12,
            "syntax_error": 4,
            "timeout": 1,
            "empty_output": 1
        }
    }
}

# è¿™ä¸ªç»“æ„ä¼šè¢«ä¿å­˜åˆ° metrics.json æ–‡ä»¶
```

#### handle_inf() é€’å½’å‡½æ•°

```python
# é—®é¢˜ï¼šJSON ä¸æ”¯æŒ Python çš„ float('inf')
json.dumps({"value": float('inf')})  # æŠ¥é”™ï¼

# è§£å†³ï¼šé€’å½’æ›¿æ¢ä¸º None
data = {
    "dataset1": {
        "pass_rate": 0.85,
        "cost_per_solved": float('inf')  # æ²¡æœ‰é€šè¿‡çš„é¢˜ç›®æ—¶
    }
}
handle_inf(data)
# â†’ {"dataset1": {"pass_rate": 0.85, "cost_per_solved": None}}
```

---

### 2.7 æœ¬éƒ¨åˆ†å°ç»“

**æ‰§è¡Œæµç¨‹åˆ°ç›®å‰ä¸ºæ­¢**ï¼š

```
asyncio.run(run_evaluation(config))
       â”‚
       â–¼
run_evaluation() åç¨‹å¼€å§‹æ‰§è¡Œ
       â”‚
       â”œâ”€â”€ 1. æ‰“å°é…ç½®ä¿¡æ¯
       â”‚
       â”œâ”€â”€ 2. åˆ›å»ºè¾“å‡ºç›®å½• (Path.mkdir)
       â”‚
       â”œâ”€â”€ 3. è·å–æœåŠ¡å™¨åœ°å€ (Simple: ç›´æ¥ç”¨ vllm_url)
       â”‚
       â”œâ”€â”€ 4. åˆå§‹åŒ–ç»„ä»¶ (MetricsCollector, QALogger)
       â”‚
       â”œâ”€â”€ 5. ä¸»å¾ªç¯
       â”‚      â”œâ”€â”€ load_prompts() â”€â”€â”€â”€â”€â”€â–º åŠ è½½é¢˜ç›®ï¼ˆä¸‹ä¸€éƒ¨åˆ†è®²è§£ï¼‰
       â”‚      â”‚
       â”‚      â””â”€â”€ await evaluate_dataset() â”€â”€â–º è¯„æµ‹ï¼ˆç¬¬å››éƒ¨åˆ†è®²è§£ï¼‰
       â”‚
       â””â”€â”€ 6. ä¿å­˜ç»“æœ (metrics.json, qa_logs, summary.json)
```

**å…³é”®æ¦‚å¿µ**ï¼š
- `async def`: å®šä¹‰åç¨‹å‡½æ•°
- **åç¨‹å¯¹è±¡**: è°ƒç”¨åç¨‹å‡½æ•°è¿”å›çš„å¯¹è±¡ï¼Œéœ€è¦ `await` æ‰§è¡Œ
- `await`: æš‚åœå½“å‰åç¨‹ï¼Œç­‰å¾…å¦ä¸€ä¸ªåç¨‹å®Œæˆ
- `pathlib.Path`: ç°ä»£åŒ–çš„è·¯å¾„å¤„ç†
- `try...finally`: ä¿è¯æ¸…ç†ä»£ç æ‰§è¡Œ
- `asdict()`: dataclass è½¬å­—å…¸

---

**è¯·ç¡®è®¤ä½ ç†è§£äº†ç¬¬äºŒéƒ¨åˆ†åï¼Œæˆ‘å°†ç»§ç»­è®²è§£ç¬¬ä¸‰éƒ¨åˆ†ï¼šload_prompts() æ•°æ®åŠ è½½ã€‚**

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šload_prompts() æ•°æ®åŠ è½½ä¸ Prompt æ¨¡æ¿

### 3.1 æ‰§è¡Œæµç¨‹å›é¡¾

```
run_evaluation()
    â”‚
    for dataset_key in config.datasets:  # ["humaneval", "mbpp_reg"]
        â”‚
        â–¼
        prompts = load_prompts(dataset_key, config)  â† æˆ‘ä»¬ç°åœ¨åœ¨è¿™é‡Œ
        â”‚
        â–¼
        await evaluate_dataset(prompts, ...)
```

---

### 3.2 load_prompts() å…¥å£å‡½æ•°

```python
# ç¬¬ 1262-1276 è¡Œ
def load_prompts(dataset_key: str, config: EvalConfig) -> List[Dict[str, Any]]:
    """
    åŠ è½½è¯„æµ‹æ•°æ®

    ä¸¤ç§æ•°æ®æºï¼š
    1. manifest_dir: ä»æœ¬åœ° manifest + raw æ–‡ä»¶åŠ è½½ï¼ˆåŒ…å«æµ‹è¯•ç”¨ä¾‹ï¼‰
    2. SandboxFusion: ä»åœ¨çº¿æœåŠ¡åŠ è½½ï¼ˆä»… promptï¼‰
    """
    if config.manifest_dir:
        # ä¼˜å…ˆä½¿ç”¨æœ¬åœ°æ•°æ®ï¼ˆåŒ…å«æµ‹è¯•ç”¨ä¾‹ï¼‰
        return _load_from_manifest(dataset_key, config.manifest_dir)
    else:
        # ä» SandboxFusion åœ¨çº¿æœåŠ¡åŠ è½½
        return _load_from_sandbox(dataset_key, config.sandbox_url)
```

**æ³¨æ„**ï¼šè¿™æ˜¯æ™®é€šå‡½æ•°ï¼ˆä¸æ˜¯ `async def`ï¼‰ï¼Œå› ä¸ºæ–‡ä»¶è¯»å–æ˜¯ CPU æ“ä½œï¼Œä¸éœ€è¦å¼‚æ­¥ã€‚

#### ä¸¤ç§æ•°æ®æºå¯¹æ¯”

| æ•°æ®æº | åŒ…å«æµ‹è¯•ç”¨ä¾‹ | é€‚ç”¨åœºæ™¯ |
|--------|-------------|----------|
| æœ¬åœ° manifest | âœ… æ˜¯ | ç”Ÿäº§è¯„æµ‹ï¼Œéœ€è¦å¤–éƒ¨æµ‹è¯•ç”¨ä¾‹ |
| SandboxFusion | âŒ å¦ | å¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨å†…ç½®æµ‹è¯•ç”¨ä¾‹ |

---

### 3.3 æ•°æ®é›†é…ç½®æ˜ å°„

```python
# ç¬¬ 341-362 è¡Œ
DATASET_SANDBOX_CONFIG = {
    "humaneval": {
        "sandbox_dataset": "humaneval_python",  # SandboxFusion ä¸­çš„åç§°
        "language": "python",
    },
    "mbpp_reg": {
        "sandbox_dataset": "mbpp",
        "language": "python",
        "id_range": (11, 210),  # MBPP Regular å­é›†ï¼ˆ200é¢˜ï¼‰
    },
    "codecontests_train": {
        "sandbox_dataset": "code_contests",
        "language": "python",
    },
    # ...
}
```

**ä½œç”¨**ï¼šå°†è„šæœ¬å†…éƒ¨çš„ `dataset_key` æ˜ å°„åˆ° SandboxFusion çš„æ•°æ®é›†åç§°ã€‚

| è„šæœ¬ dataset_key | SandboxFusion åç§° | è¯´æ˜ |
|------------------|-------------------|------|
| `humaneval` | `humaneval_python` | HumanEval 164 é¢˜ |
| `mbpp_reg` | `mbpp` | MBPP Regular 200 é¢˜ (ID 11-210) |

---

### 3.4 ä»æœ¬åœ°åŠ è½½ï¼š_load_from_manifest()

```python
# ç¬¬ 1327-1395 è¡Œ
def _load_from_manifest(dataset_key: str, manifest_dir: str) -> List[Dict[str, Any]]:
    """
    æ–‡ä»¶ç»“æ„ï¼š
    manifest_dir/
        humaneval_manifest.jsonl   # å»é‡åçš„ problem_id åˆ—è¡¨
    manifest_dir/../raw/
        humaneval_raw.jsonl        # å®Œæ•´æ•°æ®ï¼ˆå«æµ‹è¯•ç”¨ä¾‹ï¼‰
    """
    manifest_path = Path(manifest_dir) / f"{dataset_key}_manifest.jsonl"
    raw_path = Path(manifest_dir).parent / "raw" / f"{dataset_key}_raw.jsonl"
```

#### æ–‡ä»¶ç»“æ„ç¤ºæ„

```
data/
â”œâ”€â”€ manifest/
â”‚   â”œâ”€â”€ humaneval_manifest.jsonl   # {"problem_id": "HumanEval/0"}
â”‚   â””â”€â”€ mbpp_reg_manifest.jsonl    # {"problem_id": "11"}
â””â”€â”€ raw/
    â”œâ”€â”€ humaneval_raw.jsonl        # å®Œæ•´æ•°æ® + æµ‹è¯•ç”¨ä¾‹
    â””â”€â”€ mbpp_reg_raw.jsonl
```

#### åŠ è½½æµç¨‹

```python
    # æ­¥éª¤1ï¼šä» manifest è¯»å–è¦è¯„æµ‹çš„é¢˜ç›® IDï¼ˆå»é‡åçš„ï¼‰
    problem_ids = set()
    with open(manifest_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue
            entry = json.loads(line)
            problem_ids.add(entry["problem_id"])
    # problem_ids = {"HumanEval/0", "HumanEval/1", ...}

    # æ­¥éª¤2ï¼šä» raw æ–‡ä»¶åŠ è½½å®Œæ•´æ•°æ®ï¼Œåªä¿ç•™ manifest ä¸­çš„é¢˜ç›®
    result = []
    with open(raw_path, 'r', encoding='utf-8') as f:
        for line in f:
            record = json.loads(line)
            if record["problem_id"] in problem_ids:  # è¿‡æ»¤
                item = {
                    "problem_id": record["problem_id"],
                    "prompt": record["prompt"],
                    "sandbox_dataset": sandbox_dataset,
                }
                # åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if "test_cases" in record:
                    item["test_cases"] = record["test_cases"]
                result.append(item)

    return result
```

#### JSONL æ ¼å¼

JSONL (JSON Lines)ï¼šæ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¾¿äºæµå¼å¤„ç†å¤§æ–‡ä»¶ã€‚

```jsonl
{"problem_id": "HumanEval/0", "prompt": "def func():\n    ..."}
{"problem_id": "HumanEval/1", "prompt": "def another():\n    ..."}
```

**ä¼˜ç‚¹**ï¼š
- å¯ä»¥é€è¡Œè¯»å–ï¼Œä¸éœ€è¦ä¸€æ¬¡åŠ è½½æ•´ä¸ªæ–‡ä»¶åˆ°å†…å­˜
- ä¾¿äºè¿½åŠ å†™å…¥
- ä¾¿äºç”¨ `grep`/`wc -l` ç­‰å·¥å…·å¤„ç†

---

### 3.5 ä» SandboxFusion åŠ è½½ï¼š_load_from_sandbox()

```python
# ç¬¬ 1279-1324 è¡Œ
def _load_from_sandbox(dataset_key: str, sandbox_url: str) -> List[Dict[str, Any]]:
    """ä» SandboxFusion åœ¨çº¿æœåŠ¡åŠ è½½æ•°æ®"""

    # æ£€æŸ¥ SDK æ˜¯å¦å¯ç”¨
    if not SANDBOX_AVAILABLE:
        print(f"  Warning: SandboxFusion SDK not available")
        return []

    cfg = DATASET_SANDBOX_CONFIG.get(dataset_key, {})
    sandbox_dataset = cfg.get("sandbox_dataset", dataset_key)
    id_range = cfg.get("id_range")  # MBPP Regular çš„ ID èŒƒå›´

    # è®¾ç½® SandboxFusion æœåŠ¡å™¨åœ°å€
    set_sandbox_endpoint(sandbox_url)

    # è°ƒç”¨ SDK è·å–é¢˜ç›®åˆ—è¡¨
    prompts = get_prompts(GetPromptsRequest(
        dataset=sandbox_dataset,
        config={"language": cfg.get("language", "python")}
    ))

    result = []
    for p in prompts:
        pid = str(p.id)

        # ID èŒƒå›´è¿‡æ»¤ï¼ˆç”¨äº MBPP Regular å­é›†ï¼‰
        if id_range:
            id_num = int(pid)
            if id_num < id_range[0] or id_num > id_range[1]:
                continue  # è·³è¿‡ä¸åœ¨èŒƒå›´å†…çš„é¢˜ç›®

        result.append({
            "problem_id": pid,
            "prompt": p.prompt,
            "sandbox_dataset": sandbox_dataset,
            # æ³¨æ„ï¼šæ²¡æœ‰ test_casesï¼
        })

    return result
```

**æ³¨æ„**ï¼šä» SandboxFusion åŠ è½½çš„æ•°æ® **ä¸åŒ…å«æµ‹è¯•ç”¨ä¾‹**ï¼Œè¯„æµ‹æ—¶å¿…é¡»ä½¿ç”¨ SandboxFusion çš„å†…ç½®æµ‹è¯•ã€‚

---

### 3.6 è¿”å›æ•°æ®çš„ç»“æ„

#### â˜… load_prompts() è¿”å›å€¼ç¤ºä¾‹ â˜…

```python
prompts = load_prompts("humaneval", config)
# prompts æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä»£è¡¨ä¸€é“é¢˜ç›®ï¼š

# ç¤ºä¾‹ï¼šHumanEval æ•°æ®é›†çš„å‰ 3 é¢˜
prompts = [
    # ç¬¬ 1 é¢˜ï¼šæ£€æŸ¥åˆ—è¡¨ä¸­æ˜¯å¦æœ‰ä¸¤ä¸ªæ•°å­—è¶³å¤Ÿæ¥è¿‘
    {
        "problem_id": "HumanEval/0",
        "prompt": '''from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer to each other than
    given threshold.
    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)
    False
    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)
    True
    """
''',
        "sandbox_dataset": "humaneval_python",
        "test_cases": {  # åªæœ‰ä» manifest åŠ è½½æ—¶æ‰æœ‰
            "entry_point": "has_close_elements",
            "test_code": "assert has_close_elements([1.0, 2.0, 3.0], 0.5) == False\nassert has_close_elements([1.0, 2.8, 3.0], 0.3) == True\n..."
        }
    },

    # ç¬¬ 2 é¢˜ï¼šåˆ†ç¦»æ‹¬å·ç»„
    {
        "problem_id": "HumanEval/1",
        "prompt": '''from typing import List

def separate_paren_groups(paren_string: str) -> List[str]:
    """ Input to this function is a string containing multiple groups of nested parentheses.
    Your goal is to separate those group into separate strings and return the list of those.
    >>> separate_paren_groups('( ) (( )) (( )( ))')
    ['()', '(())', '(()())']
    """
''',
        "sandbox_dataset": "humaneval_python",
        "test_cases": {
            "entry_point": "separate_paren_groups",
            "test_code": "..."
        }
    },

    # ç¬¬ 3 é¢˜ï¼šæˆªæ–­æ•°å­—çš„å°æ•°éƒ¨åˆ†
    {
        "problem_id": "HumanEval/2",
        "prompt": '''def truncate_number(number: float) -> float:
    """ Given a positive floating point number, it can be decomposed into
    and integer part (largest integer smaller than given number) and decimals
    (leftover part always smaller than 1).
    >>> truncate_number(3.5)
    0.5
    """
''',
        "sandbox_dataset": "humaneval_python",
        "test_cases": {
            "entry_point": "truncate_number",
            "test_code": "..."
        }
    },

    # ... å…± 164 é¢˜
]
```

**ä» SandboxFusion åŠ è½½æ—¶ï¼ˆæ—  test_casesï¼‰**ï¼š
```python
prompts = load_prompts("humaneval", config)  # config.manifest_dir ä¸º None
# è¿”å›ç»“æ„ç›¸åŒï¼Œä½†æ²¡æœ‰ test_cases å­—æ®µï¼š
[
    {
        "problem_id": "HumanEval/0",
        "prompt": "from typing import List\n\ndef has_close_elements...",
        "sandbox_dataset": "humaneval_python"
        # æ³¨æ„ï¼šæ²¡æœ‰ test_casesï¼è¯„æµ‹æ—¶å¿…é¡»ç”¨ SandboxFusion å†…ç½®æµ‹è¯•
    },
    # ...
]
```

---

### 3.7 Prompt æ¨¡æ¿ï¼ˆformat_promptï¼‰

åŠ è½½å®Œ prompts åï¼Œåœ¨ `evaluate_dataset()` ä¸­ä¼šç”¨æ¨¡æ¿æ ¼å¼åŒ–ï¼š

```python
# ç¬¬ 1459-1470 è¡Œï¼ˆevaluate_dataset å†…éƒ¨ï¼‰
if dataset_key == "mbpp_reg":
    prompt_texts = [
        format_prompt(
            p["prompt"],
            dataset_key,
            p.get("test_cases", {}).get("entry_point", ""),
            p.get("test_cases", {}).get("example_call", "")
        )
        for p in batch
    ]
else:
    prompt_texts = [format_prompt(p["prompt"], dataset_key) for p in batch]
```

#### format_prompt() å‡½æ•°

```python
# ç¬¬ 282-310 è¡Œ
def format_prompt(
    prompt: str,
    dataset_key: str,
    entry_point: str = "",
    example_call: str = ""
) -> str:
    """
    æ ¹æ®æ•°æ®é›†ç±»å‹æ ¼å¼åŒ– prompt

    Args:
        prompt: åŸå§‹é¢˜ç›®æè¿°
        dataset_key: æ•°æ®é›†åç§°
        entry_point: å‡½æ•°åï¼ˆMBPP éœ€è¦ï¼‰
        example_call: è°ƒç”¨ç¤ºä¾‹ï¼ˆMBPP éœ€è¦ï¼‰
    """
    template = PROMPT_TEMPLATES.get(dataset_key, PROMPT_TEMPLATES["humaneval"])

    # æ›¿æ¢å ä½ç¬¦
    formatted = template.format(
        prompt=prompt,
        entry_point=entry_point,
        example_call=example_call
    )
    return formatted
```

#### HumanEval æ¨¡æ¿

```python
# ç¬¬ 214-229 è¡Œ
PROMPT_TEMPLATES = {
    "humaneval": """Complete the following Python function.

Rules:
- Keep the function name, parameters, and docstring unchanged.
- Output a complete, executable Python code snippet that defines the function.
- Use only Python standard library (no pip packages).
- Do NOT read from stdin and do NOT print anything.
- Do NOT include "if __name__ == '__main__':" or any top-level execution.
- Do NOT define a function named "check" (it is reserved for tests).

{prompt}

Output ONLY:
<code>
# python code
</code>""",
```

#### MBPP æ¨¡æ¿

```python
    "mbpp_reg": """Implement a Python function for the following task.

Task:
{prompt}

Rules:
- The function name MUST be: {entry_point}
- Your function will be called like: {example_call}
- Use only Python standard library (no pip packages).
- Do NOT read from stdin and do NOT print anything.
- Do NOT include "if __name__ == '__main__':" or any top-level execution.

Output ONLY:
<code>
# python code
</code>""",
```

#### â˜… format_prompt() è¿”å›å€¼ç¤ºä¾‹ â˜…

**è¾“å…¥ï¼ˆHumanEval åŸå§‹ promptï¼‰**ï¼š
```python
prompt = '''from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer...
    """
'''

formatted = format_prompt(prompt, "humaneval")
```

**è¾“å‡ºï¼ˆæ ¼å¼åŒ–åå‘ç»™æ¨¡å‹çš„å®Œæ•´ promptï¼‰**ï¼š
```
Complete the following Python function.

Rules:
- Keep the function name, parameters, and docstring unchanged.
- Output a complete, executable Python code snippet that defines the function.
- Use only Python standard library (no pip packages).
- Do NOT read from stdin and do NOT print anything.
- Do NOT include "if __name__ == '__main__':" or any top-level execution.
- Do NOT define a function named "check" (it is reserved for tests).

from typing import List

def has_close_elements(numbers: List[float], threshold: float) -> bool:
    """ Check if in given list of numbers, are any two numbers closer...
    """

Output ONLY:
<code>
# python code
</code>
```

**MBPP æ•°æ®é›†çš„æ ¼å¼åŒ–ç¤ºä¾‹**ï¼š
```python
# MBPP éœ€è¦é¢å¤–çš„ entry_point å’Œ example_call
prompt = "Write a function to find the similar elements from the given two tuple lists."
formatted = format_prompt(
    prompt,
    "mbpp_reg",
    entry_point="similar_elements",
    example_call="similar_elements((3, 4, 5, 6), (5, 7, 4, 10))"
)
```

**è¾“å‡º**ï¼š
```
Implement a Python function for the following task.

Task:
Write a function to find the similar elements from the given two tuple lists.

Rules:
- The function name MUST be: similar_elements
- Your function will be called like: similar_elements((3, 4, 5, 6), (5, 7, 4, 10))
- Use only Python standard library (no pip packages).
- Do NOT read from stdin and do NOT print anything.
- Do NOT include "if __name__ == '__main__':" or any top-level execution.

Output ONLY:
<code>
# python code
</code>
```

---

### 3.8 System Prompt

```python
# ç¬¬ 198-205 è¡Œ
SYSTEM_PROMPT = """You are an expert Python programmer.

Output rules:
1. Output Python code only.
2. Include necessary imports only if needed.
3. Wrap the entire code in <code> and </code>.
4. Do not write anything outside the <code> tags.
5. Follow dataset-specific constraints given by the user prompt (function-only vs full program)."""
```

**ä½œç”¨**ï¼šä½œä¸ºæ¨¡å‹çš„"è§’è‰²è®¾å®š"ï¼Œåœ¨æ¯æ¬¡ API è°ƒç”¨æ—¶ä¼ å…¥ã€‚

---

### 3.9 æœ¬éƒ¨åˆ†å°ç»“

**æ•°æ®åŠ è½½æµç¨‹**ï¼š

```
load_prompts(dataset_key, config)
        â”‚
        â”œâ”€â”€ config.manifest_dir å­˜åœ¨?
        â”‚       â”‚
        â”‚       â”œâ”€â”€ æ˜¯ â†’ _load_from_manifest()
        â”‚       â”‚         â”œâ”€â”€ è¯»å– manifest.jsonl (problem_id åˆ—è¡¨)
        â”‚       â”‚         â”œâ”€â”€ è¯»å– raw.jsonl (å®Œæ•´æ•°æ®)
        â”‚       â”‚         â””â”€â”€ è¿”å› [{problem_id, prompt, test_cases}, ...]
        â”‚       â”‚
        â”‚       â””â”€â”€ å¦ â†’ _load_from_sandbox()
        â”‚                 â”œâ”€â”€ è°ƒç”¨ SandboxFusion SDK
        â”‚                 â””â”€â”€ è¿”å› [{problem_id, prompt}, ...]  (æ—  test_cases)
        â”‚
        â–¼
prompts åˆ—è¡¨ä¼ ç»™ evaluate_dataset()
        â”‚
        â–¼
format_prompt() æ ¼å¼åŒ–æ¯ä¸ª prompt
        â”‚
        â–¼
å‘é€ç»™ vLLM ç”Ÿæˆä»£ç 
```

**å…³é”®æ¦‚å¿µ**ï¼š
- **JSONL æ ¼å¼**ï¼šæ¯è¡Œä¸€ä¸ª JSONï¼Œä¾¿äºæµå¼å¤„ç†
- **Manifest + Raw åˆ†ç¦»**ï¼šmanifest æ§åˆ¶è¯„æµ‹å“ªäº›é¢˜ç›®ï¼Œraw å­˜å‚¨å®Œæ•´æ•°æ®
- **Prompt æ¨¡æ¿**ï¼š`{prompt}` å ä½ç¬¦ï¼Œç»Ÿä¸€æ ¼å¼åŒ–æŒ‡ä»¤
- **æ•°æ®é›†é…ç½®æ˜ å°„**ï¼š`DATASET_SANDBOX_CONFIG` å¤„ç†å‘½åå·®å¼‚

---

**è¯·ç¡®è®¤ä½ ç†è§£äº†ç¬¬ä¸‰éƒ¨åˆ†åï¼Œæˆ‘å°†ç»§ç»­è®²è§£ç¬¬å››éƒ¨åˆ†ï¼ševaluate_dataset() è¯„æµ‹æµç¨‹ä¸ asyncio.gather å¹¶å‘ã€‚**

---

## ç¬¬å››éƒ¨åˆ†ï¼ševaluate_dataset() è¯„æµ‹æµç¨‹ä¸å¼‚æ­¥å¹¶å‘

è¿™æ˜¯æ•´ä¸ªè„šæœ¬ä¸­ **å¼‚æ­¥ç¼–ç¨‹æœ€æ ¸å¿ƒ** çš„éƒ¨åˆ†ï¼Œä¼šè¯¦ç»†è®²è§£ `asyncio.gather`ã€`Semaphore`ã€`async with` ç­‰æ¦‚å¿µã€‚

### 4.1 æ‰§è¡Œæµç¨‹å›é¡¾

```
run_evaluation()
    â”‚
    for dataset_key in config.datasets:
        â”‚
        prompts = load_prompts(...)     â† ç¬¬ä¸‰éƒ¨åˆ†å·²è®²
        â”‚
        â–¼
        await evaluate_dataset(...)     â† æˆ‘ä»¬ç°åœ¨åœ¨è¿™é‡Œ
              â”‚
              â”œâ”€â”€ åˆ†æ‰¹å¤„ç† (batch)
              â”‚     â”‚
              â”‚     â”œâ”€â”€ await batch_generate()   â† å¹¶å‘ç”Ÿæˆä»£ç 
              â”‚     â”‚         â”‚
              â”‚     â”‚         â””â”€â”€ asyncio.gather(*tasks)
              â”‚     â”‚               â”œâ”€â”€ generate_code() Ã— N
              â”‚     â”‚               â”œâ”€â”€ generate_code() Ã— N
              â”‚     â”‚               â””â”€â”€ ...
              â”‚     â”‚
              â”‚     â””â”€â”€ é€ä¸ªåˆ¤é¢˜ evaluate_with_*()
              â”‚
              â””â”€â”€ è¿”å› dataset_metrics
```

---

### 4.2 evaluate_dataset() æ•´ä½“ç»“æ„

```python
# ç¬¬ 1399-1427 è¡Œ
async def evaluate_dataset(
    dataset_key: str,
    prompts: List[Dict[str, Any]],
    server_addresses: List[str],
    config: EvalConfig,
    metrics_collector: MetricsCollector,
    qa_logger: QALogger,
) -> Dict[str, Any]:
    """
    è¯„æµ‹å•ä¸ªæ•°æ®é›†

    æµç¨‹ï¼š
    1. åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
    2. æ‰¹é‡ç”Ÿæˆä»£ç ï¼ˆå¹¶å‘è¯·æ±‚ï¼‰   â† å¼‚æ­¥
    3. é€ä¸ªåˆ¤é¢˜                  â† åŒæ­¥
    4. æ”¶é›†æŒ‡æ ‡å’Œæ—¥å¿—
    5. è¿”å›ç»Ÿè®¡ä¿¡æ¯
    """
```

**æ³¨æ„**ï¼šå‡½æ•°ç­¾åæ˜¯ `async def`ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåç¨‹å‡½æ•°ã€‚

---

### 4.3 åˆ†æ‰¹å¤„ç† (Batching)

```python
    # ç¬¬ 1449-1454 è¡Œ
    # åˆ†æ‰¹å¤„ç†ï¼šæ¯æ‰¹ batch_size ä¸ªé¢˜ç›®
    for batch_start in range(0, len(prompts), config.batch_size):
        batch_end = min(batch_start + config.batch_size, len(prompts))
        batch = prompts[batch_start:batch_end]
        # batch_size = 50ï¼Œåˆ™ï¼š
        # ç¬¬1æ‰¹: prompts[0:50]
        # ç¬¬2æ‰¹: prompts[50:100]
        # ...
```

**ä¸ºä»€ä¹ˆåˆ†æ‰¹ï¼Ÿ**
- é¿å…ä¸€æ¬¡æ€§å‘é€å¤ªå¤šè¯·æ±‚
- å†…å­˜å ç”¨å¯æ§
- æ–¹ä¾¿æ˜¾ç¤ºè¿›åº¦

```python
        print(f"  Processing batch {batch_start//config.batch_size + 1}/...")
```

---

### 4.4 æ‰¹é‡ç”Ÿæˆä»£ç ï¼šbatch_generate() â˜…æ ¸å¿ƒâ˜…

```python
        # ç¬¬ 1471-1478 è¡Œ
        gen_results = await batch_generate(
            server_addresses,           # ["localhost:8000"]
            config.model_path,          # "Qwen/Qwen2.5-Coder-7B-Instruct"
            prompt_texts,               # æ ¼å¼åŒ–åçš„ prompts (50ä¸ª)
            sampling_params,            # {temperature: 0.0, max_tokens: 2048}
            config.max_concurrent_requests,  # 64
            system_prompt=SYSTEM_PROMPT,
        )
```

#### â˜… batch_generate() è¿”å›å€¼ç¤ºä¾‹ â˜…

`gen_results` æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ `(completion, metadata)` å…ƒç»„ï¼š

```python
gen_results = [
    # ç¬¬ 1 é¢˜ï¼šæˆåŠŸç”Ÿæˆ
    (
        "<code>\nfrom typing import List\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    for i in range(len(numbers)):\n        for j in range(i + 1, len(numbers)):\n            if abs(numbers[i] - numbers[j]) < threshold:\n                return True\n    return False\n</code>",
        {
            "gen_time": 2.34,              # ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰
            "prompt_tokens": 156,          # è¾“å…¥ token æ•°
            "completion_tokens": 89,       # è¾“å‡º token æ•°
            "total_tokens": 245,           # æ€» token æ•°
            "finish_reason": "stop"        # ç»“æŸåŸå› ï¼šstop=æ­£å¸¸ç»“æŸ
        }
    ),
    # ç¬¬ 2 é¢˜ï¼šæˆåŠŸç”Ÿæˆï¼ˆè¾ƒé•¿ï¼‰
    (
        "<code>\ndef separate_paren_groups(paren_string: str) -> List[str]:\n    result = []\n    current = []\n    depth = 0\n    for char in paren_string:\n        if char == '(':\n            depth += 1\n            current.append(char)\n        elif char == ')':\n            depth -= 1\n            current.append(char)\n            if depth == 0:\n                result.append(''.join(current))\n                current = []\n    return result\n</code>",
        {
            "gen_time": 3.12,
            "prompt_tokens": 178,
            "completion_tokens": 124,
            "total_tokens": 302,
            "finish_reason": "stop"
        }
    ),
    # ç¬¬ 3 é¢˜ï¼šç”Ÿæˆè¢«æˆªæ–­ï¼ˆè¾¾åˆ° max_tokens é™åˆ¶ï¼‰
    (
        "<code>\ndef truncate_number(number: float) -> float:\n    return number - int(number)\n# This function extracts the decimal part of a floating point number...",
        {
            "gen_time": 5.67,
            "prompt_tokens": 145,
            "completion_tokens": 2048,     # è¾¾åˆ°ä¸Šé™
            "total_tokens": 2193,
            "finish_reason": "length"      # å› é•¿åº¦é™åˆ¶æˆªæ–­
        }
    ),
    # ç¬¬ 4 é¢˜ï¼šè¯·æ±‚å¤±è´¥
    (
        "",                                # ç©ºå­—ç¬¦ä¸²è¡¨ç¤ºå¤±è´¥
        {
            "error": "timeout",            # é”™è¯¯ç±»å‹
            "gen_time": 300.0              # è¶…æ—¶æ—¶é—´
        }
    ),
    # ... å…± 50 ä¸ªå…ƒç´ ï¼Œä¸ prompt_texts ä¸€ä¸€å¯¹åº”
]
```

**finish_reason å¯èƒ½çš„å€¼**ï¼š

| finish_reason | å«ä¹‰ |
|---------------|------|
| `stop` | æ­£å¸¸ç»“æŸï¼ˆé‡åˆ° stop token æˆ–ç”Ÿæˆå®Œæ•´ï¼‰ |
| `length` | è¾¾åˆ° `max_tokens` é™åˆ¶è¢«æˆªæ–­ |
| `content_filter` | è¢«å†…å®¹è¿‡æ»¤å™¨æ‹¦æˆª |

ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ `batch_generate()` å†…éƒ¨ï¼š

```python
# ç¬¬ 554-597 è¡Œ
async def batch_generate(
    server_addresses: List[str],
    model_path: str,
    prompts: List[str],
    sampling_params: dict,
    max_concurrent: int = 64,
    system_prompt: Optional[str] = None,
) -> List[Tuple[str, Dict[str, Any]]]:
    """
    æ‰¹é‡ç”Ÿæˆä»£ç ï¼Œè´Ÿè½½å‡è¡¡åˆ°å¤šä¸ª replica
    """

    # â˜… æ­¥éª¤1ï¼šåˆ›å»ºä¿¡å·é‡ï¼ˆé™åˆ¶å¹¶å‘æ•°ï¼‰â˜…
    semaphore = asyncio.Semaphore(max_concurrent)  # æœ€å¤š 64 ä¸ªå¹¶å‘

    # â˜… æ­¥éª¤2ï¼šåˆ›å»º HTTP ä¼šè¯ï¼ˆè¿æ¥æ± ï¼‰â˜…
    async with aiohttp.ClientSession() as session:

        # â˜… æ­¥éª¤3ï¼šåˆ›å»ºåç¨‹ä»»åŠ¡åˆ—è¡¨ â˜…
        tasks = []
        for i, prompt in enumerate(prompts):
            # Round-Robin è´Ÿè½½å‡è¡¡
            server_idx = i % len(server_addresses)
            server_address = server_addresses[server_idx]

            # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯åˆ›å»ºåç¨‹å¯¹è±¡ï¼Œè¿˜æ²¡æœ‰æ‰§è¡Œï¼
            task = generate_code(
                session, server_address, model_path, prompt,
                sampling_params, semaphore, system_prompt
            )
            tasks.append(task)
        # tasks = [coroutine1, coroutine2, ..., coroutine50]

        # â˜… æ­¥éª¤4ï¼šå¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡ â˜…
        results = await asyncio.gather(*tasks)
        # gather ä¼šï¼š
        # 1. æŠŠæ‰€æœ‰åç¨‹æ³¨å†Œåˆ°äº‹ä»¶å¾ªç¯
        # 2. å¹¶å‘æ‰§è¡Œå®ƒä»¬
        # 3. ç­‰å¾…å…¨éƒ¨å®Œæˆ
        # 4. æŒ‰åŸé¡ºåºè¿”å›ç»“æœ

    return results
```

---

### 4.5 asyncio.gather() è¯¦è§£

```python
results = await asyncio.gather(*tasks)
```

**`*tasks` æ˜¯ä»€ä¹ˆï¼Ÿ**

```python
tasks = [coro1, coro2, coro3]
asyncio.gather(*tasks)  # ç­‰ä»·äº asyncio.gather(coro1, coro2, coro3)
# * æ˜¯è§£åŒ…è¿ç®—ç¬¦ï¼ŒæŠŠåˆ—è¡¨å±•å¼€ä¸ºå¤šä¸ªå‚æ•°
```

**asyncio.gather() çš„è¡Œä¸º**ï¼š

```
         asyncio.gather(coro1, coro2, coro3)
                        â”‚
                        â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           äº‹ä»¶å¾ªç¯ (Event Loop)            â”‚
    â”‚                                           â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚  coro1  â”‚ â”‚  coro2  â”‚ â”‚  coro3  â”‚    â”‚
    â”‚  â”‚         â”‚ â”‚         â”‚ â”‚         â”‚    â”‚
    â”‚  â”‚ HTTP â”€â”€â”€â”¼â”€â”¼â”€ HTTP â”€â”€â”¼â”€â”¼â”€ HTTP â”€â”€â”‚    â”‚  â† åŒæ—¶å‘å‡ºè¯·æ±‚
    â”‚  â”‚ ç­‰å¾…... â”‚ â”‚ ç­‰å¾…... â”‚ â”‚ ç­‰å¾…... â”‚    â”‚
    â”‚  â”‚         â”‚ â”‚         â”‚ â”‚         â”‚    â”‚
    â”‚  â”‚ å“åº” â—„â”€â”€â”¼â”€â”¼â”€ å“åº” â—„â”€â”¼â”€â”¼â”€ å“åº” â—„â”€â”‚    â”‚  â† é™†ç»­æ”¶åˆ°å“åº”
    â”‚  â”‚ å®Œæˆ!   â”‚ â”‚ å®Œæˆ!   â”‚ â”‚ å®Œæˆ!   â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                                           â”‚
    â”‚        å…¨éƒ¨å®Œæˆï¼Œè¿”å› [r1, r2, r3]         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®ç‰¹æ€§**ï¼š
1. **å¹¶å‘æ‰§è¡Œ**ï¼šæ‰€æœ‰åç¨‹"åŒæ—¶"å¼€å§‹
2. **ä¿æŒé¡ºåº**ï¼šç»“æœæŒ‰ tasks çš„é¡ºåºè¿”å›ï¼Œä¸æ˜¯æŒ‰å®Œæˆé¡ºåº
3. **ç­‰å¾…å…¨éƒ¨**ï¼šæ‰€æœ‰åç¨‹å®Œæˆåæ‰è¿”å›

```python
# ç¤ºä¾‹
tasks = [
    fetch_data("url1"),  # è€—æ—¶ 3 ç§’
    fetch_data("url2"),  # è€—æ—¶ 1 ç§’
    fetch_data("url3"),  # è€—æ—¶ 2 ç§’
]
results = await asyncio.gather(*tasks)
# æ€»è€—æ—¶çº¦ 3 ç§’ï¼ˆæœ€æ…¢çš„é‚£ä¸ªï¼‰ï¼Œä¸æ˜¯ 6 ç§’
# results = [result1, result2, result3]  â† æŒ‰åŸé¡ºåº
```

---

### 4.6 asyncio.Semaphore å¹¶å‘æ§åˆ¶

**é—®é¢˜**ï¼šå¦‚æœåŒæ—¶å‘èµ· 1000 ä¸ªè¯·æ±‚ä¼šæ€æ ·ï¼Ÿ
- vLLM æœåŠ¡å™¨å¯èƒ½è¿‡è½½
- ç½‘ç»œè¿æ¥å¯èƒ½è€—å°½
- å†…å­˜å¯èƒ½ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**ï¼šSemaphoreï¼ˆä¿¡å·é‡ï¼‰

```python
# ç¬¬ 578-579 è¡Œ
semaphore = asyncio.Semaphore(max_concurrent)  # max_concurrent = 64
```

**Semaphore æ˜¯ä»€ä¹ˆï¼Ÿ**

æƒ³è±¡ä¸€ä¸ªåœè½¦åœºåªæœ‰ 64 ä¸ªè½¦ä½ï¼š

```
åœè½¦åœºï¼ˆSemaphore = 64ï¼‰
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—ğŸš—...ï¼ˆ64ä¸ªè½¦ä½ï¼‰              â”‚
â”‚                                         â”‚
â”‚ è¿›å…¥ï¼šsemaphore.acquire() â†’ è½¦ä½ -1     â”‚
â”‚ ç¦»å¼€ï¼šsemaphore.release() â†’ è½¦ä½ +1     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

å½“è½¦ä½ = 0 æ—¶ï¼š
  æ–°è½¦å¿…é¡»ç­‰å¾…ï¼Œç›´åˆ°æœ‰è½¦ç¦»å¼€
```

**åœ¨ generate_code() ä¸­ä½¿ç”¨**ï¼š

```python
# ç¬¬ 506 è¡Œ
async with semaphore:  # è·å–ä¸€ä¸ª"è½¦ä½"
    # ... å‘é€ HTTP è¯·æ±‚ ...
    # ... ç­‰å¾…å“åº” ...
# ç¦»å¼€ async with æ—¶è‡ªåŠ¨é‡Šæ”¾"è½¦ä½"
```

**æ‰§è¡Œè¿‡ç¨‹**ï¼š

```
åç¨‹ 1-64:  async with semaphore â†’ æˆåŠŸï¼Œè¿›å…¥æ‰§è¡Œ
åç¨‹ 65:    async with semaphore â†’ ç­‰å¾…...ï¼ˆè½¦ä½æ»¡äº†ï¼‰
åç¨‹ 1 å®Œæˆ: é‡Šæ”¾ semaphore
åç¨‹ 65:    è·å¾— semaphore â†’ å¼€å§‹æ‰§è¡Œ
...
```

---

### 4.7 async with è¯¦è§£

`async with` æ˜¯ **å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨**ï¼Œç”¨äºç®¡ç†éœ€è¦å¼‚æ­¥åˆå§‹åŒ–/æ¸…ç†çš„èµ„æºã€‚

#### æ™®é€š with vs async with

```python
# æ™®é€š withï¼ˆåŒæ­¥ï¼‰
with open("file.txt") as f:
    data = f.read()
# ç¦»å¼€ with æ—¶è‡ªåŠ¨è°ƒç”¨ f.close()

# async withï¼ˆå¼‚æ­¥ï¼‰
async with aiohttp.ClientSession() as session:
    async with session.get(url) as response:
        data = await response.text()
# ç¦»å¼€æ—¶è‡ªåŠ¨æ¸…ç†è¿æ¥
```

#### ä»£ç ä¸­çš„ä¸‰å±‚ async with

```python
# ç¬¬ 1 å±‚ï¼šHTTP ä¼šè¯ç®¡ç†
async with aiohttp.ClientSession() as session:
    #       â†‘ åˆ›å»ºè¿æ¥æ± 
    #       ç¦»å¼€æ—¶è‡ªåŠ¨å…³é—­æ‰€æœ‰è¿æ¥

    # ç¬¬ 2 å±‚ï¼šå¹¶å‘æ§åˆ¶ï¼ˆåœ¨ generate_code ä¸­ï¼‰
    async with semaphore:
        #       â†‘ è·å–è®¸å¯ï¼ˆå¦‚æœæ²¡æœ‰è®¸å¯åˆ™ç­‰å¾…ï¼‰
        #       ç¦»å¼€æ—¶è‡ªåŠ¨é‡Šæ”¾è®¸å¯

        # ç¬¬ 3 å±‚ï¼šå•ä¸ª HTTP è¯·æ±‚
        async with session.post(url, json=data) as resp:
            #       â†‘ å‘é€è¯·æ±‚ï¼Œç­‰å¾…å“åº”
            #       ç¦»å¼€æ—¶è‡ªåŠ¨å…³é—­å“åº”ä½“
            result = await resp.json()
```

#### async with çš„æœ¬è´¨

```python
async with something as x:
    # ä½¿ç”¨ x
```

ç­‰ä»·äºï¼š

```python
x = await something.__aenter__()  # å¼‚æ­¥è¿›å…¥
try:
    # ä½¿ç”¨ x
finally:
    await something.__aexit__()   # å¼‚æ­¥é€€å‡ºï¼ˆæ¸…ç†ï¼‰
```

---

### 4.8 generate_code() å•ä¸ªè¯·æ±‚çš„å®ç°

```python
# ç¬¬ 468-551 è¡Œ
async def generate_code(
    session: aiohttp.ClientSession,
    server_address: str,
    model_path: str,
    prompt: str,
    sampling_params: dict,
    semaphore: asyncio.Semaphore,
    system_prompt: Optional[str] = None,
) -> Tuple[str, Dict[str, Any]]:
    """
    é€šè¿‡ OpenAI-compatible API è°ƒç”¨ vLLM ç”Ÿæˆä»£ç 
    """

    # â˜… å¹¶å‘æ§åˆ¶ï¼šæœ€å¤š 64 ä¸ªåç¨‹èƒ½åŒæ—¶è¿›å…¥è¿™é‡Œ â˜…
    async with semaphore:
        start_time = time.time()

        # æ„å»º messagesï¼ˆOpenAI æ ¼å¼ï¼‰
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})

        try:
            # â˜… å‘é€ HTTP POST è¯·æ±‚ â˜…
            async with session.post(
                url=f"http://{server_address}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json={
                    "model": model_path,
                    "messages": messages,
                    **sampling_params  # temperature, max_tokens
                },
                timeout=aiohttp.ClientTimeout(total=300),  # 5åˆ†é’Ÿè¶…æ—¶
            ) as resp:
                # â˜… è¿™é‡Œå‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿâ˜…
                # 1. å‘é€è¯·æ±‚ï¼ˆå‡ ä¹ç¬é—´å®Œæˆï¼‰
                # 2. ç­‰å¾…æœåŠ¡å™¨å¤„ç†ï¼ˆåç¨‹åœ¨è¿™é‡Œæš‚åœï¼‰
                # 3. äº‹ä»¶å¾ªç¯å»æ‰§è¡Œå…¶ä»–åç¨‹
                # 4. å“åº”åˆ°è¾¾åï¼Œæ¢å¤è¿™ä¸ªåç¨‹

                if resp.status != 200:
                    error_text = await resp.text()
                    return "", {"error": f"API error {resp.status}"}

                # â˜… è¯»å–å“åº”ä½“ï¼ˆåˆä¸€æ¬¡ I/O ç­‰å¾…ï¼‰â˜…
                data = await resp.json()

                # â˜… vLLM API å“åº”ç¤ºä¾‹ â˜…
                # data çš„ç»“æ„ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰ï¼š
                # {
                #     "id": "cmpl-abc123",
                #     "object": "chat.completion",
                #     "created": 1699000000,
                #     "model": "Qwen/Qwen2.5-Coder-7B-Instruct",
                #     "choices": [
                #         {
                #             "index": 0,
                #             "message": {
                #                 "role": "assistant",
                #                 "content": "<code>\nfrom typing import List\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    for i in range(len(numbers)):\n        for j in range(i + 1, len(numbers)):\n            if abs(numbers[i] - numbers[j]) < threshold:\n                return True\n    return False\n</code>"
                #             },
                #             "finish_reason": "stop"
                #         }
                #     ],
                #     "usage": {
                #         "prompt_tokens": 156,
                #         "completion_tokens": 89,
                #         "total_tokens": 245
                #     }
                # }

                # æå–ç»“æœ
                completion = data["choices"][0]["message"]["content"]
                usage = data.get("usage", {})

                return completion, {
                    "gen_time": time.time() - start_time,
                    "completion_tokens": usage.get("completion_tokens", 0),
                    "finish_reason": data["choices"][0].get("finish_reason"),
                }

        except asyncio.TimeoutError:
            return "", {"error": "timeout"}
        except Exception as e:
            return "", {"error": str(e)}
```

#### â˜… generate_code() è¿”å›å€¼ç¤ºä¾‹ â˜…

å•ä¸ª `generate_code()` è°ƒç”¨è¿”å›ä¸€ä¸ªå…ƒç»„ `(completion, metadata)`ï¼š

**æˆåŠŸæ¡ˆä¾‹**ï¼š
```python
(
    # completion: vLLM è¿”å›çš„åŸå§‹å†…å®¹ï¼ˆåŒ…å« <code> æ ‡ç­¾ï¼‰
    "<code>\nfrom typing import List\n\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\n    \"\"\" Check if in given list of numbers, are any two numbers closer\n    to each other than given threshold.\n    \"\"\"\n    for i in range(len(numbers)):\n        for j in range(i + 1, len(numbers)):\n            if abs(numbers[i] - numbers[j]) < threshold:\n                return True\n    return False\n</code>",

    # metadata: ç”Ÿæˆçš„å…ƒä¿¡æ¯
    {
        "gen_time": 2.34,           # ä»å‘é€è¯·æ±‚åˆ°æ”¶åˆ°å“åº”çš„è€—æ—¶
        "prompt_tokens": 156,       # è¾“å…¥ prompt çš„ token æ•°
        "completion_tokens": 89,    # ç”Ÿæˆå†…å®¹çš„ token æ•°
        "total_tokens": 245,        # æ€» token æ•°
        "finish_reason": "stop"     # æ­£å¸¸ç»“æŸ
    }
)
```

**å¤±è´¥æ¡ˆä¾‹**ï¼š
```python
# è¶…æ—¶
("", {"error": "timeout", "gen_time": 300.0})

# API é”™è¯¯
("", {"error": "API error 503: Service Unavailable", "gen_time": 0.5})

# ç½‘ç»œé”™è¯¯
("", {"error": "Cannot connect to host localhost:8000", "gen_time": 0.1})
```

---

### 4.9 å®Œæ•´çš„å¹¶å‘æ‰§è¡Œæµç¨‹å›¾

```
batch_generate(prompts=[p1, p2, ..., p50], max_concurrent=64)
        â”‚
        â–¼
åˆ›å»º Semaphore(64)
        â”‚
        â–¼
async with aiohttp.ClientSession() as session:
        â”‚
        â–¼
åˆ›å»º 50 ä¸ªåç¨‹å¯¹è±¡ï¼ˆè¿˜æ²¡æ‰§è¡Œï¼‰
tasks = [generate_code(p1), generate_code(p2), ..., generate_code(p50)]
        â”‚
        â–¼
await asyncio.gather(*tasks)
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    äº‹ä»¶å¾ªç¯å¼€å§‹è°ƒåº¦                                â”‚
â”‚                                                                   â”‚
â”‚  æ—¶åˆ» T0:                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ coro1: async with semaphore â†’ è·å–(å‰©ä½™63)                   â”‚ â”‚
â”‚  â”‚ coro2: async with semaphore â†’ è·å–(å‰©ä½™62)                   â”‚ â”‚
â”‚  â”‚ ...                                                          â”‚ â”‚
â”‚  â”‚ coro50: async with semaphore â†’ è·å–(å‰©ä½™14)                  â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚ å…¨éƒ¨ 50 ä¸ªåç¨‹éƒ½è·å¾—äº† semaphoreï¼ˆå› ä¸º 50 < 64ï¼‰              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  æ—¶åˆ» T1:                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ coro1: session.post() â†’ å‘é€è¯·æ±‚ â†’ ç­‰å¾…å“åº”...              â”‚ â”‚
â”‚  â”‚ coro2: session.post() â†’ å‘é€è¯·æ±‚ â†’ ç­‰å¾…å“åº”...              â”‚ â”‚
â”‚  â”‚ ...                                                          â”‚ â”‚
â”‚  â”‚ coro50: session.post() â†’ å‘é€è¯·æ±‚ â†’ ç­‰å¾…å“åº”...             â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚ 50 ä¸ª HTTP è¯·æ±‚å‡ ä¹åŒæ—¶å‘å‡ºï¼                                 â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  æ—¶åˆ» T2~T10:                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ coro5:  å“åº”åˆ°è¾¾ â†’ await resp.json() â†’ å®Œæˆ âœ“               â”‚ â”‚
â”‚  â”‚ coro12: å“åº”åˆ°è¾¾ â†’ await resp.json() â†’ å®Œæˆ âœ“               â”‚ â”‚
â”‚  â”‚ coro1:  å“åº”åˆ°è¾¾ â†’ await resp.json() â†’ å®Œæˆ âœ“               â”‚ â”‚
â”‚  â”‚ ...                                                          â”‚ â”‚
â”‚  â”‚ (å“åº”é™†ç»­åˆ°è¾¾ï¼Œå®Œæˆé¡ºåºä¸ç¡®å®š)                                â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  æ—¶åˆ» T_end:                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ æ‰€æœ‰ 50 ä¸ªåç¨‹éƒ½å®Œæˆ                                         â”‚ â”‚
â”‚  â”‚ gather è¿”å› [result1, result2, ..., result50]               â”‚ â”‚
â”‚  â”‚ (æŒ‰åŸé¡ºåºï¼Œä¸æ˜¯å®Œæˆé¡ºåº)                                     â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 4.10 aiohttp.ClientSession çš„ä½œç”¨

```python
async with aiohttp.ClientSession() as session:
```

**ClientSession çš„ä¼˜åŠ¿**ï¼š

1. **è¿æ¥æ± å¤ç”¨**ï¼šä¸éœ€è¦æ¯ä¸ªè¯·æ±‚éƒ½å»ºç«‹æ–°è¿æ¥
2. **Cookie ç®¡ç†**ï¼šè‡ªåŠ¨å¤„ç† Cookie
3. **é…ç½®å…±äº«**ï¼šheadersã€timeout ç­‰å¯ä»¥ç»Ÿä¸€é…ç½®

```
ä¸ç”¨ Sessionï¼ˆæ…¢ï¼‰:
è¯·æ±‚1: TCPæ¡æ‰‹ â†’ å‘é€ â†’ å“åº” â†’ å…³é—­
è¯·æ±‚2: TCPæ¡æ‰‹ â†’ å‘é€ â†’ å“åº” â†’ å…³é—­  ï¼ˆåˆè¦æ¡æ‰‹ï¼‰
è¯·æ±‚3: TCPæ¡æ‰‹ â†’ å‘é€ â†’ å“åº” â†’ å…³é—­

ç”¨ Sessionï¼ˆå¿«ï¼‰:
è¯·æ±‚1: TCPæ¡æ‰‹ â†’ å‘é€ â†’ å“åº” â†’ (ä¿æŒè¿æ¥)
è¯·æ±‚2: (å¤ç”¨è¿æ¥) â†’ å‘é€ â†’ å“åº” â†’ (ä¿æŒè¿æ¥)
è¯·æ±‚3: (å¤ç”¨è¿æ¥) â†’ å‘é€ â†’ å“åº” â†’ å…³é—­
```

---

### 4.11 åˆ¤é¢˜éƒ¨åˆ†ï¼ˆåŒæ­¥ï¼‰

ç”Ÿæˆä»£ç åï¼Œé€ä¸ªåˆ¤é¢˜ï¼š

```python
        # ç¬¬ 1480-1513 è¡Œ
        # 2. é€ä¸ªåˆ¤é¢˜
        for i, (generated_code, gen_meta) in enumerate(gen_results):
            problem_id = batch[i]["problem_id"]
            test_cases = batch[i].get("test_cases")

            # æ ¹æ®é…ç½®é€‰æ‹©è¯„æµ‹æ–¹å¼
            if test_cases and config.use_external_tests:
                # æ–¹å¼1ï¼šä½¿ç”¨å¤–éƒ¨æµ‹è¯•ç”¨ä¾‹ + run_code API
                eval_result = evaluate_with_run_code(
                    generated_code, test_cases, problem_id, config
                )
            else:
                # æ–¹å¼2ï¼šä½¿ç”¨ submit APIï¼ˆSandboxFusion å†…ç½®æµ‹è¯•ï¼‰
                eval_result = evaluate_with_submit_api(
                    generated_code, sandbox_dataset, problem_id, config
                )
```

**ä¸ºä»€ä¹ˆåˆ¤é¢˜æ˜¯åŒæ­¥çš„ï¼Ÿ**

è¯„æµ‹å‡½æ•° `evaluate_with_submit_api()` å’Œ `evaluate_with_run_code()` éƒ½æ˜¯æ™®é€šå‡½æ•°ï¼ˆä¸æ˜¯ `async def`ï¼‰ï¼Œå› ä¸ºï¼š
1. SandboxFusion SDK çš„ `submit_safe()` æ˜¯åŒæ­¥ API
2. åˆ¤é¢˜é€šå¸¸æ¯”ç”Ÿæˆå¿«å¾—å¤šï¼Œä¼˜åŒ–æ”¶ç›Šå°
3. ä¿æŒä»£ç ç®€å•

---

### 4.12 æœ¬éƒ¨åˆ†å°ç»“

**å¼‚æ­¥ç¼–ç¨‹æ ¸å¿ƒæ¦‚å¿µ**ï¼š

| æ¦‚å¿µ | ä½œç”¨ | ä»£ç ä½ç½® |
|------|------|----------|
| `async def` | å®šä¹‰åç¨‹å‡½æ•° | ç¬¬ 468, 554, 1399 è¡Œ |
| `await` | ç­‰å¾…åç¨‹å®Œæˆ | ç¬¬ 595, 1471 è¡Œ |
| `asyncio.gather()` | å¹¶å‘æ‰§è¡Œå¤šä¸ªåç¨‹ | ç¬¬ 595 è¡Œ |
| `asyncio.Semaphore` | é™åˆ¶å¹¶å‘æ•°é‡ | ç¬¬ 579 è¡Œ |
| `async with` | å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨ | ç¬¬ 506, 517, 581 è¡Œ |
| `aiohttp.ClientSession` | HTTP è¿æ¥æ±  | ç¬¬ 581 è¡Œ |

**æ‰§è¡Œæµç¨‹**ï¼š

```
evaluate_dataset()
    â”‚
    for batch in batches:
        â”‚
        â”œâ”€â”€ batch_generate()
        â”‚       â”‚
        â”‚       â”œâ”€â”€ Semaphore(64) â† å¹¶å‘æ§åˆ¶
        â”‚       â”‚
        â”‚       â”œâ”€â”€ ClientSession() â† è¿æ¥æ± 
        â”‚       â”‚
        â”‚       â”œâ”€â”€ åˆ›å»º N ä¸ªåç¨‹
        â”‚       â”‚
        â”‚       â””â”€â”€ asyncio.gather() â† å¹¶å‘æ‰§è¡Œï¼
        â”‚               â”‚
        â”‚               â””â”€â”€ 50 ä¸ª HTTP è¯·æ±‚åŒæ—¶å‘å‡º
        â”‚                   å“åº”é™†ç»­è¿”å›
        â”‚                   å…¨éƒ¨å®Œæˆåè¿”å›ç»“æœ
        â”‚
        â””â”€â”€ é€ä¸ªåˆ¤é¢˜ï¼ˆåŒæ­¥ï¼‰
```

**å…³é”®ç†è§£**ï¼š
- `asyncio.gather()` æ˜¯å®ç°å¹¶å‘çš„æ ¸å¿ƒ
- `Semaphore` é˜²æ­¢å¹¶å‘è¿‡å¤šå¯¼è‡´èµ„æºè€—å°½
- `async with` ç®¡ç†å¼‚æ­¥èµ„æºçš„ç”Ÿå‘½å‘¨æœŸ
- åç¨‹åœ¨ I/O ç­‰å¾…æ—¶è®©å‡ºæ§åˆ¶æƒï¼Œå®ç°"å¹¶å‘"

---

**è¯·ç¡®è®¤ä½ ç†è§£äº†ç¬¬å››éƒ¨åˆ†åï¼Œæˆ‘å°†ç»§ç»­è®²è§£ç¬¬äº”éƒ¨åˆ†ï¼šä»£ç è¯„æµ‹ï¼ˆSandboxFusionï¼‰ä¸ç»“æœç»Ÿè®¡ã€‚**

---

## ç¬¬äº”éƒ¨åˆ†ï¼šä»£ç è¯„æµ‹ï¼ˆSandboxFusionï¼‰ä¸ç»“æœç»Ÿè®¡

### 5.1 è¯„æµ‹æµç¨‹æ¦‚è§ˆ

```
batch_generate() è¿”å›ç”Ÿæˆçš„ä»£ç 
        â”‚
        â–¼
for i, (generated_code, gen_meta) in enumerate(gen_results):
        â”‚
        â”œâ”€â”€ é€‰æ‹©è¯„æµ‹æ–¹å¼
        â”‚       â”‚
        â”‚       â”œâ”€â”€ æ–¹å¼1: evaluate_with_submit_api()
        â”‚       â”‚          â””â”€â”€ ä½¿ç”¨ SandboxFusion å†…ç½®æµ‹è¯•
        â”‚       â”‚
        â”‚       â””â”€â”€ æ–¹å¼2: evaluate_with_run_code()
        â”‚                  â””â”€â”€ ä½¿ç”¨å¤–éƒ¨æµ‹è¯•ç”¨ä¾‹
        â”‚
        â”œâ”€â”€ è·å– EvalResult
        â”‚
        â”œâ”€â”€ æ”¶é›†æŒ‡æ ‡ (metrics_collector)
        â”‚
        â””â”€â”€ è®°å½•æ—¥å¿— (qa_logger)
```

---

### 5.2 EvalResult æ•°æ®ç»“æ„

```python
# utils/metrics.py ç¬¬ 28-38 è¡Œ
@dataclass
class EvalResult:
    """å•ä¸ªé—®é¢˜çš„è¯„æµ‹ç»“æœ"""
    problem_id: str          # é—®é¢˜ ID
    accepted: bool           # æ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹
    pass_ratio: float        # é€šè¿‡çš„æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹ [0, 1]
    error_type: str          # é”™è¯¯ç±»å‹
    judge_time: float        # åˆ¤é¢˜è€—æ—¶ï¼ˆç§’ï¼‰
    gen_tokens: int = 0      # ç”Ÿæˆçš„ token æ•°
    gen_time: float = 0.0    # ç”Ÿæˆè€—æ—¶ï¼ˆç§’ï¼‰
    details: Dict[str, Any] = field(default_factory=dict)  # é¢å¤–ä¿¡æ¯
```

**error_type å¯èƒ½çš„å€¼**ï¼š

| error_type | è¯´æ˜ |
|------------|------|
| `success` | é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ |
| `syntax_error` | è¯­æ³•é”™è¯¯ï¼ˆä»£ç æ— æ³•è§£æï¼‰ |
| `runtime_error` | è¿è¡Œæ—¶é”™è¯¯ï¼ˆå¼‚å¸¸ã€æ®µé”™è¯¯ç­‰ï¼‰ |
| `timeout` | æ‰§è¡Œè¶…æ—¶ |
| `wrong_answer` | ç»“æœé”™è¯¯ï¼ˆä»£ç èƒ½è¿è¡Œä½†è¾“å‡ºä¸å¯¹ï¼‰ |
| `api_error` | API è°ƒç”¨é”™è¯¯ |
| `empty_output` | æ¨¡å‹è¾“å‡ºä¸ºç©º |

#### â˜… EvalResult è¿”å›å€¼ç¤ºä¾‹ â˜…

**æˆåŠŸé€šè¿‡æ‰€æœ‰æµ‹è¯•**ï¼š
```python
EvalResult(
    problem_id="HumanEval/0",
    accepted=True,                    # å…¨éƒ¨é€šè¿‡
    pass_ratio=1.0,                   # 10/10 æµ‹è¯•ç”¨ä¾‹é€šè¿‡
    error_type="success",
    judge_time=0.45,                  # åˆ¤é¢˜è€—æ—¶ 0.45 ç§’
    gen_tokens=89,                    # ç”Ÿæˆäº† 89 ä¸ª token
    gen_time=2.34,                    # ç”Ÿæˆè€—æ—¶ 2.34 ç§’
    details={
        "extracted_code": "def has_close_elements(numbers, threshold):\n    ...",
        "test_count": 10
    }
)
```

**éƒ¨åˆ†é€šè¿‡**ï¼š
```python
EvalResult(
    problem_id="HumanEval/15",
    accepted=False,                   # æœªå…¨éƒ¨é€šè¿‡
    pass_ratio=0.7,                   # 7/10 æµ‹è¯•ç”¨ä¾‹é€šè¿‡
    error_type="wrong_answer",        # ç­”æ¡ˆé”™è¯¯
    judge_time=0.38,
    gen_tokens=156,
    gen_time=3.21,
    details={
        "extracted_code": "def split_words(txt):\n    ...",
        "test_count": 10,
        "failed_tests": [3, 5, 8]     # å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ç¼–å·
    }
)
```

**è¿è¡Œæ—¶é”™è¯¯**ï¼š
```python
EvalResult(
    problem_id="HumanEval/42",
    accepted=False,
    pass_ratio=0.0,
    error_type="runtime_error",
    judge_time=0.12,
    gen_tokens=78,
    gen_time=1.89,
    details={
        "extracted_code": "def incr_list(l):\n    return [x + 1 for x in l]",
        "test_count": 5,
        "error_message": "TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'"
    }
)
```

**ç©ºè¾“å‡º**ï¼š
```python
EvalResult(
    problem_id="HumanEval/99",
    accepted=False,
    pass_ratio=0.0,
    error_type="empty_output",
    judge_time=0.01,
    gen_tokens=0,
    gen_time=0.5,
    details={}
)
```

---

### 5.3 evaluate_with_submit_api() è¯¦è§£

```python
# ç¬¬ 604-706 è¡Œ
def evaluate_with_submit_api(
    completion: str,        # æ¨¡å‹ç”Ÿæˆçš„ä»£ç ï¼ˆåŒ…å« <code> æ ‡ç­¾ï¼‰
    sandbox_dataset: str,   # "humaneval_python"
    sandbox_id: str,        # "HumanEval/0"
    config: EvalConfig,
) -> EvalResult:
    """
    ä½¿ç”¨ SandboxFusion submit() API è¯„æµ‹ä»£ç 

    submit() API ç‰¹ç‚¹ï¼š
    - ä¾èµ– SandboxFusion å†…ç½®çš„æµ‹è¯•ç”¨ä¾‹æ•°æ®
    - è‡ªåŠ¨å¤„ç†ä»£ç æå–ã€ç¼–è¯‘ã€æ‰§è¡Œ
    - è¿”å›è¯¦ç»†çš„æµ‹è¯•ç»“æœ
    """
```

**æ‰§è¡Œæµç¨‹**ï¼š

```python
    # 1. æ£€æŸ¥ SDK å¯ç”¨æ€§
    if not SANDBOX_AVAILABLE:
        return EvalResult(..., error_type="sdk_unavailable")

    start_time = time.time()

    # 2. ç©ºè¾“å‡ºæ£€æŸ¥
    if not completion or not completion.strip():
        return EvalResult(..., error_type="empty_output")

    try:
        # 3. è®¾ç½®æœåŠ¡åœ°å€
        set_sandbox_endpoint(config.sandbox_url)  # "http://localhost:8080"

        # 4. è°ƒç”¨ submit API
        result = submit_safe(SubmitRequest(
            dataset=sandbox_dataset,   # "humaneval_python"
            id=sandbox_id,             # "HumanEval/0"
            completion=completion,     # æ¨¡å‹ç”Ÿæˆçš„ä»£ç 
            config=TestConfig(
                language='python',
                run_timeout=config.run_timeout,  # 30 ç§’
            )
        ))

        # 5. è§£æç»“æœ
        accepted = result.accepted        # æ˜¯å¦å…¨éƒ¨é€šè¿‡
        tests = result.tests or []        # æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœ

        # 6. è®¡ç®— pass_ratio
        if tests:
            passed = sum(1 for t in tests if t.status == "success")
            pass_ratio = passed / len(tests)
        else:
            pass_ratio = 1.0 if accepted else 0.0

        # 7. ç¡®å®šé”™è¯¯ç±»å‹
        error_type = "success" if accepted else _determine_error_type(tests)

        return EvalResult(
            problem_id=sandbox_id,
            accepted=accepted,
            pass_ratio=pass_ratio,
            error_type=error_type,
            judge_time=time.time() - start_time,
            details={
                "extracted_code": result.extracted_code,  # æå–çš„ä»£ç 
                "test_count": len(tests),
            },
        )

    except Exception as e:
        return EvalResult(..., error_type="api_error")
```

---

### 5.4 SandboxFusion æœåŠ¡æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SandboxFusion æœåŠ¡å™¨                          â”‚
â”‚                    (localhost:8080)                              â”‚
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    submit() API                           â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  è¾“å…¥:                                                    â”‚  â”‚
â”‚  â”‚    - dataset: "humaneval_python"                         â”‚  â”‚
â”‚  â”‚    - id: "HumanEval/0"                                   â”‚  â”‚
â”‚  â”‚    - completion: "<code>def func():...</code>"           â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  å¤„ç†æ­¥éª¤:                                                â”‚  â”‚
â”‚  â”‚    1. ä»£ç æå–ï¼šä» <code>...</code> ä¸­æå–ä»£ç             â”‚  â”‚
â”‚  â”‚    2. åŠ è½½æµ‹è¯•ç”¨ä¾‹ï¼šä»å†…ç½®æ•°æ®é›†è·å–æµ‹è¯•ç”¨ä¾‹              â”‚  â”‚
â”‚  â”‚    3. æ²™ç®±æ‰§è¡Œï¼šåœ¨éš”ç¦»ç¯å¢ƒä¸­è¿è¡Œä»£ç                       â”‚  â”‚
â”‚  â”‚    4. ç»“æœæ¯”å¯¹ï¼šæ¯”è¾ƒè¾“å‡ºä¸é¢„æœŸç»“æœ                        â”‚  â”‚
â”‚  â”‚                                                           â”‚  â”‚
â”‚  â”‚  è¾“å‡º:                                                    â”‚  â”‚
â”‚  â”‚    - accepted: True/False                                â”‚  â”‚
â”‚  â”‚    - tests: [{status: "success"}, {status: "failed"}]    â”‚  â”‚
â”‚  â”‚    - extracted_code: "def func():..."                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                  â”‚
â”‚  å†…ç½®æ•°æ®é›†:                                                     â”‚
â”‚    - humaneval_python (164 é¢˜ + æµ‹è¯•ç”¨ä¾‹)                       â”‚
â”‚    - mbpp (974 é¢˜ + æµ‹è¯•ç”¨ä¾‹)                                   â”‚
â”‚    - code_contests (...)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### 5.5 ä¸¤ç§è¯„æµ‹æ–¹å¼å¯¹æ¯”

```python
# ç¬¬ 1498-1506 è¡Œ
if test_cases and config.use_external_tests:
    # æ–¹å¼1ï¼šä½¿ç”¨å¤–éƒ¨æµ‹è¯•ç”¨ä¾‹ + run_code API
    eval_result = evaluate_with_run_code(
        generated_code, test_cases, problem_id, config
    )
else:
    # æ–¹å¼2ï¼šä½¿ç”¨ submit APIï¼ˆSandboxFusion å†…ç½®æµ‹è¯•ï¼‰
    eval_result = evaluate_with_submit_api(
        generated_code, sandbox_dataset, problem_id, config
    )
```

| æ–¹å¼ | æµ‹è¯•ç”¨ä¾‹æ¥æº | é€‚ç”¨åœºæ™¯ |
|------|-------------|----------|
| `submit_api` | SandboxFusion å†…ç½® | å¿«é€Ÿæµ‹è¯•ï¼Œæ ‡å‡†æ•°æ®é›† |
| `run_code` | å¤–éƒ¨ manifest/raw æ–‡ä»¶ | è‡ªå®šä¹‰æµ‹è¯•ç”¨ä¾‹ï¼Œç¦»çº¿è¯„æµ‹ |

---

### 5.6 MetricsCollector æŒ‡æ ‡æ”¶é›†

```python
# utils/metrics.py ç¬¬ 78-112 è¡Œ
class MetricsCollector:
    """æŒ‡æ ‡æ”¶é›†å™¨ï¼šæ”¶é›†è¯„æµ‹ç»“æœå¹¶è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""

    def __init__(self):
        # æŒ‰æ•°æ®é›†å­˜å‚¨ç»“æœ
        self._results: Dict[str, List[EvalResult]] = defaultdict(list)

        # é”™è¯¯ç±»å‹è®¡æ•°
        self._error_counts: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))

        # æ•°æ®é›†çš„ wall_clock_time
        self._wall_clock_time: Dict[str, float] = {}

    def add_result(self, dataset: str, result: EvalResult):
        """æ·»åŠ å•ä¸ªè¯„æµ‹ç»“æœ"""
        self._results[dataset].append(result)
        self._error_counts[dataset][result.error_type] += 1
```

**åœ¨ evaluate_dataset() ä¸­ä½¿ç”¨**ï¼š

```python
# ç¬¬ 1515-1530 è¡Œï¼ˆç®€åŒ–ï¼‰
for i, (generated_code, gen_meta) in enumerate(gen_results):
    # ... è¯„æµ‹ ...
    eval_result = evaluate_with_submit_api(...)

    # æ”¶é›†æŒ‡æ ‡
    metrics_collector.add_result(dataset_key, eval_result)

    # è®°å½•æ—¥å¿—ï¼ˆé‡‡æ ·ï¼‰
    qa_logger.log(dataset_key, {
        "problem_id": problem_id,
        "prompt": prompt,
        "generated_code": generated_code,
        "accepted": eval_result.accepted,
        "pass_ratio": eval_result.pass_ratio,
        "error_type": eval_result.error_type,
    })

    results.append({
        "problem_id": problem_id,
        "accepted": eval_result.accepted,
        "pass_ratio": eval_result.pass_ratio,
        ...
    })
```

---

### 5.7 æŒ‡æ ‡è®¡ç®—ï¼ˆevaluate_dataset ç»“å°¾ï¼‰

```python
# ç¬¬ 1550-1590 è¡Œ
# è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
accepted_count = sum(1 for r in results if r["accepted"])
pass_ratios = np.array([r["pass_ratio"] for r in results])

# è®¡ç®— throughputï¼ˆååé‡ï¼‰
wall_clock_time = time.time() - dataset_start_time
throughput = len(results) / wall_clock_time  # é—®é¢˜æ•°/ç§’

# è®¡ç®— cost_per_solvedï¼ˆæ¯è§£å†³ä¸€é¢˜çš„æˆæœ¬ï¼‰
if accepted_count > 0:
    cost_per_solved_tokens = total_gen_tokens / accepted_count
    cost_per_solved_judge_time = total_judge_time / accepted_count
else:
    cost_per_solved_tokens = float('inf')      # æ²¡æœ‰é€šè¿‡çš„é¢˜ç›®
    cost_per_solved_judge_time = float('inf')

# è¿”å›æ•°æ®é›†çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯
dataset_metrics = {
    "total_problems": len(results),

    # è´¨é‡æŒ‡æ ‡
    "accepted_at_1": accepted_count / len(results),  # ä¸»æŒ‡æ ‡ï¼
    "pass_ratio_mean": float(np.mean(pass_ratios)),
    "pass_ratio_p50": float(np.median(pass_ratios)),
    "pass_ratio_p90": float(np.percentile(pass_ratios, 90)),

    # æˆæœ¬æŒ‡æ ‡
    "total_gen_tokens": total_gen_tokens,
    "avg_gen_tokens": total_gen_tokens / len(results),
    "total_judge_time": total_judge_time,
    "avg_judge_time": total_judge_time / len(results),
    "throughput": throughput,
    "cost_per_solved_tokens": cost_per_solved_tokens,
    "cost_per_solved_judge_time": cost_per_solved_judge_time,

    # å¼‚å¸¸æŒ‡æ ‡
    "truncation_rate": truncation_count / len(results),
    "timeout_rate": timeout_count / len(results),
}
```

---

### 5.8 æŒ‡æ ‡è§£é‡Š

#### è´¨é‡æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | è®¡ç®—æ–¹å¼ |
|------|------|----------|
| `accepted_at_1` | **ä¸»æŒ‡æ ‡**ï¼šé€šè¿‡ç‡ | é€šè¿‡é¢˜æ•° / æ€»é¢˜æ•° |
| `pass_ratio_mean` | å¹³å‡é€šè¿‡æµ‹è¯•ç”¨ä¾‹æ¯”ä¾‹ | mean(æ¯é¢˜çš„ pass_ratio) |
| `pass_ratio_p50` | ä¸­ä½æ•°é€šè¿‡æ¯”ä¾‹ | median(pass_ratios) |
| `pass_ratio_p90` | 90% åˆ†ä½é€šè¿‡æ¯”ä¾‹ | percentile(pass_ratios, 90) |

**ä¸ºä»€ä¹ˆéœ€è¦ pass_ratioï¼Ÿ**

```
é¢˜ç›® A: 10 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé€šè¿‡ 10 ä¸ª â†’ accepted=True,  pass_ratio=1.0
é¢˜ç›® B: 10 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé€šè¿‡ 9 ä¸ª  â†’ accepted=False, pass_ratio=0.9
é¢˜ç›® C: 10 ä¸ªæµ‹è¯•ç”¨ä¾‹ï¼Œé€šè¿‡ 0 ä¸ª  â†’ accepted=False, pass_ratio=0.0

accepted_at_1 = 1/3 = 33.3%
pass_ratio_mean = (1.0 + 0.9 + 0.0) / 3 = 63.3%

pass_ratio æ¯” accepted æ›´èƒ½åæ˜ ä»£ç è´¨é‡çš„"æ¥è¿‘ç¨‹åº¦"
```

#### æˆæœ¬æŒ‡æ ‡

| æŒ‡æ ‡ | å«ä¹‰ | ç”¨é€” |
|------|------|------|
| `avg_gen_tokens` | å¹³å‡ç”Ÿæˆ token æ•° | ä¼°ç®— API æˆæœ¬ |
| `avg_judge_time` | å¹³å‡åˆ¤é¢˜æ—¶é—´ | è¯„ä¼°åˆ¤é¢˜æ•ˆç‡ |
| `throughput` | ååé‡ï¼ˆé—®é¢˜/ç§’ï¼‰ | è¯„ä¼°æ•´ä½“æ•ˆç‡ |
| `cost_per_solved_tokens` | æ¯è§£å†³ä¸€é¢˜çš„ token æˆæœ¬ | æ€§ä»·æ¯”è¯„ä¼° |

---

### 5.9 QALogger é—®ç­”æ—¥å¿—

```python
# ç”¨äºè°ƒè¯•ï¼šä¿å­˜ç”Ÿæˆçš„ä»£ç å’Œè¯„æµ‹ç»“æœ
qa_logger = QALogger(
    output_dir / "qa_logs",
    sample_size=config.qa_sample_size  # é»˜è®¤ 20
)
```

**è¾“å‡ºæ–‡ä»¶ç»“æ„**ï¼š

```
outputs/phase0/qa_logs/
â”œâ”€â”€ humaneval_samples.jsonl    # é‡‡æ ·çš„é—®ç­”è®°å½•
â””â”€â”€ mbpp_reg_samples.jsonl
```

**æ¯æ¡è®°å½•åŒ…å«**ï¼š

```json
{
    "problem_id": "HumanEval/0",
    "prompt": "from typing import List\ndef has_close_elements...",
    "generated_code": "def has_close_elements(numbers, threshold):\n    for i in range(len(numbers))...",
    "accepted": true,
    "pass_ratio": 1.0,
    "error_type": "success",
    "gen_tokens": 156,
    "gen_time": 2.34,
    "judge_time": 0.45
}
```

**ç”¨é€”**ï¼š
- è°ƒè¯•æ¨¡å‹è¾“å‡º
- åˆ†æé”™è¯¯ç±»å‹
- äººå·¥æ£€æŸ¥ä»£ç è´¨é‡

---

### 5.10 æœ€ç»ˆè¾“å‡ºæ–‡ä»¶

```
outputs/phase0/
â”œâ”€â”€ metrics.json       # æ¯ä¸ªæ•°æ®é›†çš„ç»Ÿè®¡æŒ‡æ ‡
â”œâ”€â”€ summary.json       # è¯¦ç»†ç»Ÿè®¡ï¼ˆé”™è¯¯åˆ†å¸ƒç­‰ï¼‰
â””â”€â”€ qa_logs/
    â”œâ”€â”€ humaneval_samples.jsonl
    â””â”€â”€ mbpp_reg_samples.jsonl
```

**metrics.json ç¤ºä¾‹**ï¼š

```json
{
    "humaneval": {
        "total_problems": 164,
        "accepted_at_1": 0.7256,
        "pass_ratio_mean": 0.8234,
        "pass_ratio_p50": 1.0,
        "pass_ratio_p90": 1.0,
        "avg_gen_tokens": 245.6,
        "avg_judge_time": 0.52,
        "throughput": 12.34,
        "cost_per_solved_tokens": 338.5,
        "truncation_rate": 0.012,
        "timeout_rate": 0.006
    },
    "mbpp_reg": {
        "total_problems": 200,
        "accepted_at_1": 0.685,
        ...
    }
}
```

---

### 5.11 æœ¬éƒ¨åˆ†å°ç»“

**è¯„æµ‹æµç¨‹**ï¼š

```
æ¨¡å‹ç”Ÿæˆä»£ç 
    â”‚
    â–¼
evaluate_with_submit_api() æˆ– evaluate_with_run_code()
    â”‚
    â”œâ”€â”€ SandboxFusion æ‰§è¡Œä»£ç 
    â”‚
    â”œâ”€â”€ æ¯”å¯¹æµ‹è¯•ç»“æœ
    â”‚
    â””â”€â”€ è¿”å› EvalResult
            â”‚
            â”œâ”€â”€ accepted: æ˜¯å¦å…¨éƒ¨é€šè¿‡
            â”œâ”€â”€ pass_ratio: é€šè¿‡æ¯”ä¾‹
            â””â”€â”€ error_type: é”™è¯¯ç±»å‹
    â”‚
    â–¼
MetricsCollector æ”¶é›†æŒ‡æ ‡
    â”‚
    â–¼
è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    â”‚
    â”œâ”€â”€ accepted_at_1ï¼ˆä¸»æŒ‡æ ‡ï¼‰
    â”œâ”€â”€ pass_ratio_mean/p50/p90
    â”œâ”€â”€ avg_gen_tokens, avg_judge_time
    â””â”€â”€ cost_per_solved, throughput
    â”‚
    â–¼
ä¿å­˜ç»“æœ
    â”‚
    â”œâ”€â”€ metrics.json
    â”œâ”€â”€ summary.json
    â””â”€â”€ qa_logs/*.jsonl
```

**å…³é”®æ¦‚å¿µ**ï¼š
- **EvalResult**: å•ä¸ªé—®é¢˜çš„è¯„æµ‹ç»“æœæ•°æ®ç»“æ„
- **SandboxFusion**: ä»£ç æ²™ç®±æœåŠ¡ï¼Œæä¾›å®‰å…¨çš„ä»£ç æ‰§è¡Œç¯å¢ƒ
- **accepted_at_1**: ä¸»è¦è¯„æµ‹æŒ‡æ ‡ï¼ˆé€šè¿‡ç‡ï¼‰
- **pass_ratio**: æ¯” accepted æ›´ç»†ç²’åº¦çš„è´¨é‡è¯„ä¼°
- **cost_per_solved**: æ€§ä»·æ¯”æŒ‡æ ‡

---

## æ€»ç»“ï¼šå®Œæ•´æ‰§è¡Œæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  python src/phase0_eval.py --mode simple --vllm_url http://localhost:8000 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  main()                                                               â”‚
â”‚    â”œâ”€â”€ argparse è§£æå‘½ä»¤è¡Œå‚æ•°                                        â”‚
â”‚    â”œâ”€â”€ åˆ›å»º EvalConfig                                               â”‚
â”‚    â””â”€â”€ asyncio.run(run_evaluation(config))                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  run_evaluation() [async]                                             â”‚
â”‚    â”œâ”€â”€ è·å–æœåŠ¡å™¨åœ°å€ (Simple: ç›´æ¥ç”¨ vllm_url)                       â”‚
â”‚    â”œâ”€â”€ åˆå§‹åŒ–ç»„ä»¶ (MetricsCollector, QALogger)                       â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â”œâ”€â”€ for dataset_key in ["humaneval", "mbpp_reg"]:                 â”‚
â”‚    â”‚     â”‚                                                            â”‚
â”‚    â”‚     â”œâ”€â”€ prompts = load_prompts()                                â”‚
â”‚    â”‚     â”‚     â””â”€â”€ ä» manifest/raw æˆ– SandboxFusion åŠ è½½             â”‚
â”‚    â”‚     â”‚                                                            â”‚
â”‚    â”‚     â””â”€â”€ await evaluate_dataset()                                â”‚
â”‚    â”‚           â”‚                                                      â”‚
â”‚    â”‚           â”œâ”€â”€ for batch in batches:                             â”‚
â”‚    â”‚           â”‚     â”‚                                                â”‚
â”‚    â”‚           â”‚     â”œâ”€â”€ format_prompt() æ ¼å¼åŒ–                      â”‚
â”‚    â”‚           â”‚     â”‚                                                â”‚
â”‚    â”‚           â”‚     â”œâ”€â”€ await batch_generate()                      â”‚
â”‚    â”‚           â”‚     â”‚     â”‚                                          â”‚
â”‚    â”‚           â”‚     â”‚     â”œâ”€â”€ Semaphore(64)                         â”‚
â”‚    â”‚           â”‚     â”‚     â”œâ”€â”€ ClientSession()                       â”‚
â”‚    â”‚           â”‚     â”‚     â””â”€â”€ asyncio.gather(*tasks)               â”‚
â”‚    â”‚           â”‚     â”‚           â””â”€â”€ å¹¶å‘è°ƒç”¨ vLLM API               â”‚
â”‚    â”‚           â”‚     â”‚                                                â”‚
â”‚    â”‚           â”‚     â””â”€â”€ for æ¯ä¸ªç”Ÿæˆç»“æœ:                           â”‚
â”‚    â”‚           â”‚           â””â”€â”€ evaluate_with_submit_api()            â”‚
â”‚    â”‚           â”‚                 â””â”€â”€ è°ƒç”¨ SandboxFusion åˆ¤é¢˜         â”‚
â”‚    â”‚           â”‚                                                      â”‚
â”‚    â”‚           â””â”€â”€ è¿”å› dataset_metrics                              â”‚
â”‚    â”‚                                                                  â”‚
â”‚    â””â”€â”€ ä¿å­˜ç»“æœ                                                       â”‚
â”‚          â”œâ”€â”€ metrics.json                                            â”‚
â”‚          â”œâ”€â”€ summary.json                                            â”‚
â”‚          â””â”€â”€ qa_logs/*.jsonl                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒæŠ€æœ¯ç‚¹**ï¼š
1. **asyncio** - å¼‚æ­¥ç¼–ç¨‹æ¡†æ¶
2. **asyncio.gather** - å¹¶å‘æ‰§è¡Œå¤šä¸ªåç¨‹
3. **Semaphore** - å¹¶å‘æ•°é‡æ§åˆ¶
4. **aiohttp** - å¼‚æ­¥ HTTP å®¢æˆ·ç«¯
5. **SandboxFusion** - ä»£ç æ²™ç®±æœåŠ¡

---

**è‡³æ­¤ï¼ŒSimple æ¨¡å¼è¯„æµ‹ä»£ç çš„è®²è§£å…¨éƒ¨å®Œæˆï¼**

å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿ç»§ç»­æé—®ã€‚
